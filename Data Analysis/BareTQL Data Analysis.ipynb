{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from itertools import permutations, accumulate\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import operator\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# https://medium.com/deep-learning-hk/compute-document-similarity-using-autoencoder-with-triplet-loss-eb7eb132eb38\n",
    "# https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73\n",
    "# FaceNet: https://arxiv.org/pdf/1503.03832.pdf\n",
    "# https://www.pytables.org/usersguide/tutorials.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epochs, losses):\n",
    "    timestr = time.strftime(\"%m%d-%H%M%S\")\n",
    "    prefix = \"../models/\"\n",
    "    torch.save(model, prefix + \"%s_%sE.pt\" % (timestr, epochs))\n",
    "    plt.plot(losses)\n",
    "    plt.savefig(prefix + \"%s_plot.jpg\" % (timestr,), bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfCheck(df):\n",
    "    \n",
    "    # Any column is > 20% null values\n",
    "    if any(cnt / len(df) > 0.2 for cnt in df.isna().sum()):\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 41575 tables\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17948/3725580513.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mtable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"List of\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtableTitle\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Ignore first table, doesn't exist yet\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\pahas\\anaconda3\\envs\\ml\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tables = defaultdict(list)\n",
    "path = \"../Data/tables_025.csv\"\n",
    "maxRowLen = 0\n",
    "maxRowLenR = \"\"\n",
    "maxRowLenT = \"\"\n",
    "allRows = []\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    tableCount = 0\n",
    "    for line in f:\n",
    "        if line.startswith(\"List of\"):\n",
    "            tableCount += 1\n",
    "            \n",
    "tableFrac = 0.25\n",
    "tablesLim = int(tableCount * tableFrac)\n",
    "tablesRead = 0\n",
    "\n",
    "print(\"Reading \" + str(tablesLim) + \" tables\")\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    tableTitle = \"\"\n",
    "    table = []\n",
    "    \n",
    "    for row in reader:\n",
    "        if len(row) == 1 and row[0].startswith(\"List of\"):\n",
    "            if tableTitle: # Ignore first table, doesn't exist yet\n",
    "                if dfCheck(pd.DataFrame(table)):\n",
    "                    joinedRows = ['|||'.join(row) for row in table]\n",
    "                    tables[tableTitle].append(joinedRows)\n",
    "                    allRows.extend(joinedRows)\n",
    "                table = []\n",
    "            tableTitle = row[0]\n",
    "            \n",
    "            tablesRead += 1\n",
    "            if tablesRead > tablesLim:\n",
    "                break\n",
    "            \n",
    "        elif len(row) != 0:\n",
    "            table.append(row)\n",
    "            \n",
    "            \n",
    "print(\"Total tables analyzed: \" + str(sum(len(val) for val in tables.values())))\n",
    "maxRowLen = max(len(row) for row in allRows)\n",
    "print(\"Maximum row length: \" + str(maxRowLen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(allRows)\n",
    "id_count = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of tables in dataset: 15762\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Convert each string into a list of its characters, converted to ints\n",
    "Filter out rows which are too long (i.e. not many other rows at that length)\n",
    "\"\"\"\n",
    "import json\n",
    "\n",
    "table_chars = []\n",
    "sizes = [len(seq) for seq in allRows]\n",
    "mu, sigma = np.mean(sizes), np.std(sizes)\n",
    "maxRowLen = int(mu)\n",
    "\n",
    "for i, category in enumerate(tables):\n",
    "    for i, table in enumerate(tables[category]):\n",
    "        tokens = tokenizer.texts_to_sequences(table)\n",
    "        char_table = np.array([np.pad(seq, (0, maxRowLen - len(seq))) for seq in tokens if len(seq) <= maxRowLen])\n",
    "\n",
    "        if len(char_table) >= 10:  # Remove tables that no longer have sufficient rows. We use 10 to allow for training & testing purposes   \n",
    "            table_chars.append(char_table)\n",
    "\n",
    "table_tensors = [torch.tensor(table) for table in table_chars]\n",
    "print(\"Total # of tables in dataset: \" + str(len(table_tensors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../Data/tables.json\", \"w\") as f:\n",
    "#     f.write(json.dumps([l.tolist() for l in table_chars]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model based on https://towardsdatascience.com/a-friendly-introduction-to-siamese-networks-85ab17522942\n",
    "\"\"\"\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken, d_model, nhead, d_hid, nlayers, dropout = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = src.T\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout = 0.1, max_len = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, emb_dim, h_activ=nn.LeakyReLU()):\n",
    "        super().__init__()\n",
    "        ntokens = id_count + 1  # size of vocabulary\n",
    "        emsize = 32  # embedding dimension\n",
    "        d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "        nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "        nhead = 2  # number of heads in nn.MultiheadAttention\n",
    "        dropout = 0.2  # dropout probability\n",
    "\n",
    "        # self.tf = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout)\n",
    "\n",
    "\n",
    "        # Defining the fully connected layers\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 24, kernel_size=11,stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.LocalResponseNorm(5,alpha=0.0001,beta=0.75,k=2),\n",
    "            # nn.MaxPool1d(3, stride=2),\n",
    "            \n",
    "            # nn.Conv1d(24, 96, kernel_size=5,stride=1,padding=2),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.LocalResponseNorm(5,alpha=0.0001,beta=0.75,k=2),\n",
    "            # nn.MaxPool1d(3, stride=2),\n",
    "            # nn.Dropout(p=0.3),\n",
    "\n",
    "            # nn.Conv1d(96,128 , kernel_size=3,stride=1,padding=1),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.LocalResponseNorm(5,alpha=0.0001,beta=0.75,k=2),\n",
    "            # nn.MaxPool1d(3, stride=2),\n",
    "            # nn.Dropout(p=0.3),\n",
    "\n",
    "            # nn.Conv1d(128, 196, kernel_size=1,stride=1,padding=0),\n",
    "            # nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Defining the fully connected layers\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(1440, 1024),\n",
    "            h_activ,\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            # h_activ(inplace=True),\n",
    "            # nn.Linear(256, 128),\n",
    "            # h_activ(inplace=True),\n",
    "            # nn.Linear(128, 128),\n",
    "            # h_activ(inplace=True),\n",
    "            # nn.Linear(128, 128),\n",
    "            # h_activ(inplace=True),\n",
    "            # nn.Linear(128, 128),\n",
    "            # h_activ(inplace=True),\n",
    "            # nn.Linear(128, 128),\n",
    "            # h_activ(inplace=True),\n",
    "            # nn.Linear(128, 256),\n",
    "            # h_activ(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass \n",
    "        x = x.unsqueeze(1)\n",
    "        # transformed = self.tf(x)\n",
    "        # seq_len, bsz, emb_dim = transformed.shape\n",
    "        # transformed = transformed.view(bsz, seq_len, emb_dim)\n",
    "        output = self.cnn1(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc1(output)\n",
    "        return output\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "class TableDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tables):\n",
    "        self.sizes = torch.tensor([table.shape[0] for table in tables])\n",
    "        self.table_positions = self.get_table_positions()\n",
    "        self.rows = torch.cat(tables, 0)\n",
    "        self.max = torch.max(self.rows)\n",
    "        self.rows = self.rows / self.max  # Normalization\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sizes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx == 0:\n",
    "            return self.rows[:self.sizes[0]]\n",
    "            \n",
    "        return self.rows[self.sizes[idx-1]:self.sizes[idx]]\n",
    "\n",
    "    def get_table_positions(self):\n",
    "        return np.array([0] + list(accumulate(self.sizes, operator.add)), dtype=np.int32)\n",
    "\n",
    "    def shuffle(self):\n",
    "        idxs = np.random.permutation(len(self.sizes))\n",
    "        tables = torch.split(self.rows, list(self.sizes))  # split_size_or_sections needs a list for some reason\n",
    "        tables = [tables[idx] for idx in idxs]\n",
    "        self.rows = torch.cat(tables, 0)\n",
    "        self.sizes = self.sizes[idxs]\n",
    "        self.table_positions = self.get_table_positions()\n",
    "        return\n",
    "\n",
    "    def get_triplets(self, model, start_idx, batch_size, rows_per_table=20):\n",
    "        \"\"\"\n",
    "        Using an online novel triplet mining strategy described in https://arxiv.org/pdf/1503.03832.pdf\n",
    "        \n",
    "        We perform all computations in numpy since it is much faster than with pytorch\n",
    "        \"\"\"\n",
    "        # t = time.time()\n",
    "        table_positions = self.table_positions[start_idx: start_idx+batch_size+1]\n",
    "\n",
    "        # Get 5 rows per table for a subset of the tables\n",
    "        size = (rows_per_table, min(batch_size+1, table_positions.shape[0]) - 1)\n",
    "        chosen_row_idxs = np.random.randint(table_positions[:-1], table_positions[1:], size=size).T.flatten()\n",
    "        chosen_rows = self.rows[chosen_row_idxs].float()\n",
    "\n",
    "        anchor_idxs = np.arange(chosen_rows.shape[0])\n",
    "        positive_idxs = np.array([])\n",
    "        negative_idxs = np.array([])\n",
    "\n",
    "        mask = np.ones(chosen_rows.shape[0], dtype=bool)\n",
    "\n",
    "        # turn off autograd to speed up computation\n",
    "        with torch.no_grad():\n",
    "            embeddings = np.array(model(chosen_rows.to(device)).cpu())\n",
    "    \n",
    "        dists = np.matmul(embeddings, embeddings.T)\n",
    "\n",
    "        pos_mask = np.zeros(dists.shape, dtype=bool)\n",
    "        for i in range(0, embeddings.shape[0], rows_per_table):\n",
    "            pos_mask[i:i+rows_per_table,i:i+rows_per_table] = 1\n",
    "\n",
    "        neg_mask = ~pos_mask\n",
    "\n",
    "        # Ensure the same rows are not comapred against each other\n",
    "        # np.fill_diagonal(pos_mask,0)\n",
    "        positive_dists = np.where(pos_mask, dists, 0)\n",
    "        negative_dists = np.where(neg_mask, dists, 0)\n",
    "\n",
    "        positive_idxs = np.argmax(positive_dists, axis=1)\n",
    "        negative_idxs = np.argmin(negative_dists, axis=1)\n",
    "        \n",
    "        # print(\"Elapsed: \" + str(time.time() - t), len(chosen_rows), len(anchor_idxs))\n",
    "        return (\n",
    "            chosen_rows.to(device), \n",
    "            torch.tensor(anchor_idxs).long().to(device), \n",
    "            torch.tensor(positive_idxs).long().to(device), \n",
    "            torch.tensor(negative_idxs).long().to(device)\n",
    "        )\n",
    "\n",
    "    # def get_average_precision(self, model, first_tables=20):\n",
    "    #     with torch.no_grad():\n",
    "    #         embs = np.array(model(self.rows.long().to(device)).cpu())  # Our model uses small => better, avg_precision uses opposite\n",
    "    #     precs = []\n",
    "\n",
    "    #     dists = np.matmul(embs, embs.T)\n",
    "    #     pos_mask = np.zeros(dists.shape, dtype=bool)\n",
    "        \n",
    "    #     for table_start, table_end in zip(self.table_positions[:first_tables], self.table_positions[1:first_tables]):\n",
    "    #         pos_mask[table_start:table_end,table_start:table_end] = 1\n",
    "\n",
    "    #     for mask, scores in zip(pos_mask, dists):\n",
    "    #         # print(mask, scores)\n",
    "    #         avg_prec_score = average_precision_score(mask, scores)\n",
    "    #         precs.append(avg_prec_score)\n",
    "\n",
    "    #     return precs\n",
    "\n",
    "table_tensors_train = []\n",
    "table_tensors_val = []\n",
    "table_tensors_test = []\n",
    "take_tables = 1000\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Generate training, validation, and testing datasets\n",
    "for table in table_tensors[:take_tables]:\n",
    "    # Select 5 random rows to be validation / test rows\n",
    "    non_train_idxs = torch.randint(0, table.shape[0], size=(1, 5)).flatten()\n",
    "    val_idxs, test_idxs = non_train_idxs[:3], non_train_idxs[3:]\n",
    "\n",
    "    # All other rows are training rows\n",
    "    train_idxs = torch.ones(table.shape[0], dtype=bool)\n",
    "    train_idxs[non_train_idxs] = False\n",
    "\n",
    "    # append the data to appropriate list\n",
    "    table_tensors_train.append(table[train_idxs])\n",
    "    table_tensors_val.append(table[val_idxs])\n",
    "    table_tensors_test.append(table[test_idxs])\n",
    "\n",
    "dataset_train = TableDataset(table_tensors_train)\n",
    "dataset_val = TableDataset(table_tensors_val)\n",
    "dataset_test = TableDataset(table_tensors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 0: 100%|██████████| 5/5 [00:02<00:00,  2.27it/s]\n",
      "EPOCH 1: 100%|██████████| 5/5 [00:02<00:00,  2.30it/s]\n",
      "EPOCH 2: 100%|██████████| 5/5 [00:02<00:00,  2.35it/s]\n",
      "EPOCH 3: 100%|██████████| 5/5 [00:02<00:00,  2.37it/s]\n",
      "EPOCH 4: 100%|██████████| 5/5 [00:02<00:00,  2.32it/s]\n",
      "EPOCH 5: 100%|██████████| 5/5 [00:02<00:00,  2.40it/s]\n",
      "EPOCH 6: 100%|██████████| 5/5 [00:02<00:00,  2.38it/s]\n",
      "EPOCH 7: 100%|██████████| 5/5 [00:02<00:00,  2.39it/s]\n",
      "EPOCH 8: 100%|██████████| 5/5 [00:02<00:00,  2.33it/s]\n",
      "EPOCH 9: 100%|██████████| 5/5 [00:02<00:00,  2.35it/s]\n",
      "EPOCH 10: 100%|██████████| 5/5 [00:02<00:00,  2.37it/s]\n",
      "EPOCH 11: 100%|██████████| 5/5 [00:02<00:00,  2.36it/s]\n",
      "EPOCH 12: 100%|██████████| 5/5 [00:02<00:00,  2.37it/s]\n",
      "EPOCH 13: 100%|██████████| 5/5 [00:02<00:00,  2.40it/s]\n",
      "EPOCH 14: 100%|██████████| 5/5 [00:02<00:00,  2.35it/s]\n",
      "EPOCH 15: 100%|██████████| 5/5 [00:02<00:00,  2.34it/s]\n",
      "EPOCH 16: 100%|██████████| 5/5 [00:02<00:00,  2.35it/s]\n",
      "EPOCH 17: 100%|██████████| 5/5 [00:02<00:00,  2.45it/s]\n",
      "EPOCH 18: 100%|██████████| 5/5 [00:01<00:00,  2.51it/s]\n",
      "EPOCH 19: 100%|██████████| 5/5 [00:02<00:00,  2.46it/s]\n",
      "EPOCH 20: 100%|██████████| 5/5 [00:02<00:00,  2.49it/s]\n",
      "EPOCH 21: 100%|██████████| 5/5 [00:01<00:00,  2.51it/s]\n",
      "EPOCH 22: 100%|██████████| 5/5 [00:01<00:00,  2.54it/s]\n",
      "EPOCH 23: 100%|██████████| 5/5 [00:02<00:00,  2.49it/s]\n",
      "EPOCH 24: 100%|██████████| 5/5 [00:01<00:00,  2.51it/s]\n",
      "EPOCH 25: 100%|██████████| 5/5 [00:02<00:00,  2.50it/s]\n",
      "EPOCH 26: 100%|██████████| 5/5 [00:02<00:00,  2.46it/s]\n",
      "EPOCH 27: 100%|██████████| 5/5 [00:01<00:00,  2.54it/s]\n",
      "EPOCH 28: 100%|██████████| 5/5 [00:02<00:00,  2.49it/s]\n",
      "EPOCH 29: 100%|██████████| 5/5 [00:02<00:00,  2.48it/s]\n",
      "EPOCH 30: 100%|██████████| 5/5 [00:01<00:00,  2.52it/s]\n",
      "EPOCH 31: 100%|██████████| 5/5 [00:01<00:00,  2.52it/s]\n",
      "EPOCH 32: 100%|██████████| 5/5 [00:01<00:00,  2.53it/s]\n",
      "EPOCH 33: 100%|██████████| 5/5 [00:01<00:00,  2.52it/s]\n",
      "EPOCH 34: 100%|██████████| 5/5 [00:01<00:00,  2.51it/s]\n",
      "EPOCH 35: 100%|██████████| 5/5 [00:02<00:00,  2.45it/s]\n",
      "EPOCH 36: 100%|██████████| 5/5 [00:02<00:00,  2.48it/s]\n",
      "EPOCH 37: 100%|██████████| 5/5 [00:02<00:00,  2.50it/s]\n",
      "EPOCH 38: 100%|██████████| 5/5 [00:02<00:00,  2.45it/s]\n",
      "EPOCH 39: 100%|██████████| 5/5 [00:01<00:00,  2.51it/s]\n",
      "EPOCH 40: 100%|██████████| 5/5 [00:01<00:00,  2.50it/s]\n",
      "EPOCH 41: 100%|██████████| 5/5 [00:01<00:00,  2.53it/s]\n",
      "EPOCH 42: 100%|██████████| 5/5 [00:01<00:00,  2.53it/s]\n",
      "EPOCH 43: 100%|██████████| 5/5 [00:01<00:00,  2.52it/s]\n",
      "EPOCH 44: 100%|██████████| 5/5 [00:02<00:00,  2.48it/s]\n",
      "EPOCH 45: 100%|██████████| 5/5 [00:02<00:00,  2.46it/s]\n",
      "EPOCH 46: 100%|██████████| 5/5 [00:01<00:00,  2.54it/s]\n",
      "EPOCH 47: 100%|██████████| 5/5 [00:02<00:00,  2.49it/s]\n",
      "EPOCH 48: 100%|██████████| 5/5 [00:02<00:00,  2.48it/s]\n",
      "EPOCH 49: 100%|██████████| 5/5 [00:01<00:00,  2.52it/s]\n",
      "EPOCH 50: 100%|██████████| 5/5 [00:01<00:00,  2.56it/s]\n",
      "EPOCH 51: 100%|██████████| 5/5 [00:01<00:00,  2.51it/s]\n",
      "EPOCH 52: 100%|██████████| 5/5 [00:01<00:00,  2.54it/s]\n",
      "EPOCH 53: 100%|██████████| 5/5 [00:01<00:00,  2.50it/s]\n",
      "EPOCH 54: 100%|██████████| 5/5 [00:01<00:00,  2.56it/s]\n",
      "EPOCH 55: 100%|██████████| 5/5 [00:01<00:00,  2.55it/s]\n",
      "EPOCH 56: 100%|██████████| 5/5 [00:01<00:00,  2.51it/s]\n",
      "EPOCH 57: 100%|██████████| 5/5 [00:01<00:00,  2.51it/s]\n",
      "EPOCH 58: 100%|██████████| 5/5 [00:01<00:00,  2.58it/s]\n",
      "EPOCH 59: 100%|██████████| 5/5 [00:01<00:00,  2.57it/s]\n",
      "EPOCH 60: 100%|██████████| 5/5 [00:01<00:00,  2.52it/s]\n",
      "EPOCH 61: 100%|██████████| 5/5 [00:01<00:00,  2.50it/s]\n",
      "EPOCH 62: 100%|██████████| 5/5 [00:01<00:00,  2.57it/s]\n",
      "EPOCH 63: 100%|██████████| 5/5 [00:01<00:00,  2.55it/s]\n",
      "EPOCH 64: 100%|██████████| 5/5 [00:01<00:00,  2.53it/s]\n",
      "EPOCH 65: 100%|██████████| 5/5 [00:01<00:00,  2.61it/s]\n",
      "EPOCH 66: 100%|██████████| 5/5 [00:01<00:00,  2.61it/s]\n",
      "EPOCH 67: 100%|██████████| 5/5 [00:01<00:00,  2.55it/s]\n",
      "EPOCH 68: 100%|██████████| 5/5 [00:01<00:00,  2.56it/s]\n",
      "EPOCH 69: 100%|██████████| 5/5 [00:01<00:00,  2.51it/s]\n",
      "EPOCH 70: 100%|██████████| 5/5 [00:01<00:00,  2.54it/s]\n",
      "EPOCH 71: 100%|██████████| 5/5 [00:01<00:00,  2.51it/s]\n",
      "EPOCH 72: 100%|██████████| 5/5 [00:02<00:00,  2.50it/s]\n",
      "EPOCH 73: 100%|██████████| 5/5 [00:01<00:00,  2.56it/s]\n",
      "EPOCH 74: 100%|██████████| 5/5 [00:01<00:00,  2.56it/s]\n",
      "EPOCH 75: 100%|██████████| 5/5 [00:01<00:00,  2.51it/s]\n",
      "EPOCH 76: 100%|██████████| 5/5 [00:02<00:00,  2.50it/s]\n",
      "EPOCH 77: 100%|██████████| 5/5 [00:01<00:00,  2.53it/s]\n",
      "EPOCH 78: 100%|██████████| 5/5 [00:01<00:00,  2.55it/s]\n",
      "EPOCH 79: 100%|██████████| 5/5 [00:01<00:00,  2.52it/s]\n",
      "EPOCH 80: 100%|██████████| 5/5 [00:01<00:00,  2.54it/s]\n",
      "EPOCH 81: 100%|██████████| 5/5 [00:01<00:00,  2.54it/s]\n",
      "EPOCH 82: 100%|██████████| 5/5 [00:02<00:00,  2.50it/s]\n",
      "EPOCH 83: 100%|██████████| 5/5 [00:01<00:00,  2.56it/s]\n",
      "EPOCH 84: 100%|██████████| 5/5 [00:01<00:00,  2.60it/s]\n",
      "EPOCH 85: 100%|██████████| 5/5 [00:01<00:00,  2.53it/s]\n",
      "EPOCH 86: 100%|██████████| 5/5 [00:01<00:00,  2.52it/s]\n",
      "EPOCH 87: 100%|██████████| 5/5 [00:01<00:00,  2.52it/s]\n",
      "EPOCH 88: 100%|██████████| 5/5 [00:02<00:00,  2.35it/s]\n",
      "EPOCH 89: 100%|██████████| 5/5 [00:02<00:00,  2.40it/s]\n",
      "EPOCH 90: 100%|██████████| 5/5 [00:02<00:00,  2.41it/s]\n",
      "EPOCH 91: 100%|██████████| 5/5 [00:02<00:00,  2.38it/s]\n",
      "EPOCH 92: 100%|██████████| 5/5 [00:02<00:00,  2.29it/s]\n",
      "EPOCH 93: 100%|██████████| 5/5 [00:02<00:00,  2.41it/s]\n",
      "EPOCH 94: 100%|██████████| 5/5 [00:02<00:00,  2.37it/s]\n",
      "EPOCH 95: 100%|██████████| 5/5 [00:02<00:00,  2.32it/s]\n",
      "EPOCH 96: 100%|██████████| 5/5 [00:02<00:00,  2.39it/s]\n",
      "EPOCH 97: 100%|██████████| 5/5 [00:02<00:00,  2.32it/s]\n",
      "EPOCH 98: 100%|██████████| 5/5 [00:02<00:00,  2.31it/s]\n",
      "EPOCH 99: 100%|██████████| 5/5 [00:02<00:00,  2.25it/s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train(model, optimizer, loss_fn, data, epoch, BATCH_SIZE=200):\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for i in tqdm(range(0, len(data), BATCH_SIZE), desc=\"EPOCH %s\" % (epoch,)):\n",
    "        batch, anchors, positives, negatives = data.get_triplets(model, i, BATCH_SIZE)\n",
    "        optimizer.zero_grad()\n",
    "        anchor_embs = model(batch[anchors])\n",
    "        positive_embs = model(batch[positives])\n",
    "        negative_embs = model(batch[negatives])\n",
    "            \n",
    "        loss = loss_fn(anchor_embs, positive_embs, negative_embs)\n",
    "        \n",
    "        loss.backward() # back-propagation, could do manually\n",
    "        optimizer.step() # Adjusts the weights for us\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return losses\n",
    "\n",
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        precision_scores = data.get_average_precision(model)\n",
    "\n",
    "    return precision_scores\n",
    "\n",
    "\n",
    "EPOCHS = 100\n",
    "model = LinearModel(256).to(device)\n",
    "loss_fn = nn.TripletMarginLoss(1, 2).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1, weight_decay=0.0005)\n",
    "train_losses = []\n",
    "SAVE_MODEL = False\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    dataset_train.shuffle()\n",
    "    train_losses += train(model, optimizer, loss_fn, dataset_train, epoch)\n",
    "\n",
    "    # if epoch % 100 == 0:\n",
    "    #     # dataset_val.shuffle()\n",
    "    #     prec = evaluate(model, dataset_val)\n",
    "    #     precision_scores.extend(prec)\n",
    "    #     print(\"Average Precision at epoch %s: %s\" % (epoch, np.mean(prec)))\n",
    "\n",
    "if SAVE_MODEL:\n",
    "    save_model(model, EPOCHS, train_losses)\n",
    "else:\n",
    "    plt.plot(train_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1656, -0.0166, -0.0865,  ..., -0.1364,  0.1463,  0.1841],\n",
      "        [ 0.0078, -0.0214, -0.0544,  ...,  0.1531, -0.0905,  0.0096],\n",
      "        [-0.0327,  0.0015,  0.1119,  ..., -0.1300,  0.0889,  0.0409],\n",
      "        ...,\n",
      "        [ 0.1253,  0.0712, -0.2599,  ...,  0.3194, -0.3076,  0.0459],\n",
      "        [-0.0111, -0.0274,  0.1611,  ..., -0.0232,  0.0142, -0.0125],\n",
      "        [ 0.0628, -0.0100, -0.2823,  ...,  0.4352, -0.2469,  0.0413]],\n",
      "       device='cuda:0')\n",
      "tensor([[-0.2037, -0.0187, -0.0568,  ..., -0.1363,  0.1518,  0.2618],\n",
      "        [ 0.0357, -0.0389, -0.1386,  ...,  0.1779, -0.1386,  0.0488],\n",
      "        [-0.0777, -0.0182,  0.1165,  ..., -0.0833,  0.0939,  0.0253],\n",
      "        ...,\n",
      "        [ 0.0659, -0.0840, -0.3212,  ...,  0.4645, -0.2961,  0.1272],\n",
      "        [ 0.1084,  0.0521,  0.3357,  ..., -0.0904,  0.0075, -0.2478],\n",
      "        [ 0.0390, -0.0857, -0.1665,  ...,  0.3746, -0.2821, -0.0112]],\n",
      "       device='cuda:0')\n",
      "tensor([[ 0.1024, -0.0340, -0.0152,  ...,  0.2944, -0.2234, -0.1586],\n",
      "        [ 0.0303,  0.1519,  0.4788,  ...,  0.0120,  0.0521, -0.1374],\n",
      "        [ 0.1795, -0.0894, -0.2287,  ...,  0.2707, -0.2835,  0.0762],\n",
      "        ...,\n",
      "        [-0.1620, -0.0288, -0.0617,  ..., -0.2295,  0.1064,  0.1795],\n",
      "        [-0.1272, -0.0162, -0.1024,  ..., -0.2057,  0.0599,  0.1340],\n",
      "        [-0.2004,  0.0222, -0.0868,  ..., -0.0925,  0.1536,  0.1935]],\n",
      "       device='cuda:0')\n",
      "tensor(0.1004, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    batch, anchors, positives, negatives = dataset_train.get_triplets(model, 0, 2)\n",
    "    positive_embs = model(batch[positives]) \n",
    "    negative_embs = model(batch[negatives]) \n",
    "    anchor_embs = model(batch[anchors])\n",
    "\n",
    "    print(anchor_embs)\n",
    "    print(positive_embs)\n",
    "    print(negative_embs)\n",
    "\n",
    "    print(loss_fn(anchor_embs, positive_embs, negative_embs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2078663557767868]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch, anchors, positives, negatives = dataset_val.get_triplets(model, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn1.0.weight Parameter containing:\n",
      "tensor([[[ 0.1513,  0.0083,  0.1648,  0.0176,  0.0790, -0.0941,  0.1755,\n",
      "          -0.0085, -0.1264,  0.0326, -0.0629]],\n",
      "\n",
      "        [[-0.0178, -0.0728,  0.1265, -0.1430, -0.0798, -0.0510, -0.1074,\n",
      "           0.0194, -0.1789,  0.1782, -0.1518]],\n",
      "\n",
      "        [[ 0.1382,  0.0262, -0.0604,  0.1089,  0.0274,  0.1364,  0.0173,\n",
      "          -0.0579,  0.0468, -0.0545,  0.0728]],\n",
      "\n",
      "        [[ 0.2492,  0.1517, -0.0561,  0.1442,  0.0407,  0.1085, -0.1678,\n",
      "          -0.2223, -0.1020, -0.1176,  0.1888]],\n",
      "\n",
      "        [[ 0.1115,  0.1336,  0.0614,  0.0503,  0.1376, -0.1739, -0.0637,\n",
      "          -0.1629,  0.0910, -0.0679,  0.0976]],\n",
      "\n",
      "        [[-0.0378,  0.1481, -0.1068, -0.1095, -0.1093,  0.1641,  0.0596,\n",
      "           0.1760, -0.1520, -0.1827, -0.1430]],\n",
      "\n",
      "        [[-0.0905,  0.0847,  0.1245,  0.1919, -0.1325, -0.1430,  0.1279,\n",
      "          -0.0488,  0.1135, -0.0307,  0.1819]],\n",
      "\n",
      "        [[-0.1405, -0.0912,  0.0540,  0.0395, -0.0492,  0.1137,  0.1232,\n",
      "          -0.1315, -0.0981,  0.1717, -0.0590]],\n",
      "\n",
      "        [[-0.0648, -0.1769, -0.1047,  0.0457, -0.0241, -0.1327,  0.0043,\n",
      "          -0.1249, -0.1551, -0.1007, -0.1600]],\n",
      "\n",
      "        [[-0.1163,  0.1850,  0.0356,  0.0567, -0.1702, -0.1192, -0.0607,\n",
      "           0.0321, -0.1608, -0.0787, -0.1094]],\n",
      "\n",
      "        [[ 0.0012, -0.0678, -0.0103, -0.1234, -0.1249, -0.1062, -0.0615,\n",
      "          -0.1433,  0.1537, -0.0356,  0.1634]],\n",
      "\n",
      "        [[ 0.0560, -0.1550,  0.1257, -0.0500, -0.0699, -0.1516, -0.1831,\n",
      "           0.0535, -0.0393,  0.0712, -0.1497]],\n",
      "\n",
      "        [[ 0.1422, -0.1345, -0.0377,  0.0337,  0.0969,  0.1476,  0.1663,\n",
      "          -0.1413,  0.0527, -0.0786, -0.0233]],\n",
      "\n",
      "        [[-0.1198,  0.1145, -0.2079,  0.1236,  0.1301,  0.0803, -0.0736,\n",
      "           0.2652,  0.0677, -0.0590, -0.2456]],\n",
      "\n",
      "        [[ 0.1502, -0.0735, -0.1424,  0.2689,  0.0577, -0.1470, -0.1313,\n",
      "           0.1132,  0.1494, -0.1046, -0.1300]],\n",
      "\n",
      "        [[-0.0700, -0.1512,  0.1684,  0.1679, -0.1863,  0.0924,  0.1767,\n",
      "           0.0214, -0.1126,  0.1464,  0.1585]],\n",
      "\n",
      "        [[ 0.0612, -0.0717, -0.0376,  0.0491, -0.0173, -0.0381,  0.1995,\n",
      "           0.2348, -0.0272, -0.0763,  0.0206]],\n",
      "\n",
      "        [[-0.0108,  0.0462,  0.0508, -0.1660, -0.0676,  0.1542,  0.0723,\n",
      "          -0.0081, -0.1098, -0.1115, -0.1638]],\n",
      "\n",
      "        [[-0.0569,  0.0619,  0.1249,  0.0829, -0.1626, -0.1097, -0.0234,\n",
      "           0.1781,  0.0308, -0.0434,  0.0810]],\n",
      "\n",
      "        [[-0.0696, -0.1183,  0.1344, -0.0832, -0.0367, -0.1819,  0.1228,\n",
      "           0.1391,  0.0670, -0.1273, -0.1803]],\n",
      "\n",
      "        [[-0.1775,  0.2065,  0.1946,  0.2557,  0.1679,  0.1560,  0.0136,\n",
      "          -0.2334, -0.2495, -0.2032,  0.1406]],\n",
      "\n",
      "        [[-0.0898, -0.0368, -0.1052, -0.0333, -0.1287, -0.1195,  0.0607,\n",
      "          -0.0543,  0.1129, -0.0587, -0.1341]],\n",
      "\n",
      "        [[-0.0332, -0.0896, -0.0570, -0.1743,  0.1005, -0.1268,  0.0921,\n",
      "           0.0817,  0.1289, -0.1400,  0.1316]],\n",
      "\n",
      "        [[-0.0728,  0.1125,  0.2337, -0.0355,  0.0336, -0.0626, -0.1551,\n",
      "           0.0778, -0.1683,  0.1687,  0.1670]]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "cnn1.0.bias Parameter containing:\n",
      "tensor([ 0.0270, -0.0286, -0.1643,  0.0585,  0.0627, -0.1837,  0.0335, -0.1067,\n",
      "        -0.0160, -0.0863, -0.0695, -0.0585, -0.1754,  0.0952,  0.1271,  0.0839,\n",
      "         0.0455, -0.0741, -0.1586, -0.1769,  0.1092, -0.1004, -0.1205,  0.0571],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "fc1.0.weight Parameter containing:\n",
      "tensor([[-0.0063, -0.0072,  0.0014,  ...,  0.0031,  0.0037, -0.0055],\n",
      "        [-0.0085,  0.0152, -0.0002,  ..., -0.0053,  0.0037, -0.0121],\n",
      "        [ 0.0073, -0.0009, -0.0090,  ..., -0.0011, -0.0112, -0.0129],\n",
      "        ...,\n",
      "        [-0.0061, -0.0086, -0.0147,  ...,  0.0102,  0.0009, -0.0147],\n",
      "        [-0.0048, -0.0122,  0.0032,  ...,  0.0145, -0.0044,  0.0130],\n",
      "        [-0.0102, -0.0057, -0.0091,  ..., -0.0014,  0.0071,  0.0132]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "fc1.0.bias Parameter containing:\n",
      "tensor([-0.0139, -0.0116,  0.0016,  ...,  0.0029,  0.0073,  0.0106],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "fc1.2.weight Parameter containing:\n",
      "tensor([[-0.0167, -0.0092,  0.0198,  ..., -0.0031, -0.0164, -0.0058],\n",
      "        [ 0.0007,  0.0060, -0.0124,  ...,  0.0069, -0.0034, -0.0173],\n",
      "        [-0.0124,  0.0184, -0.0061,  ...,  0.0036, -0.0149, -0.0057],\n",
      "        ...,\n",
      "        [-0.0184,  0.0053, -0.0133,  ..., -0.0169, -0.0189,  0.0144],\n",
      "        [-0.0003,  0.0046, -0.0019,  ...,  0.0126, -0.0069, -0.0180],\n",
      "        [ 0.0156,  0.0189,  0.0042,  ...,  0.0161, -0.0136,  0.0040]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "fc1.2.bias Parameter containing:\n",
      "tensor([-7.1235e-03,  6.8979e-03, -1.0655e-02, -1.7407e-02,  2.1485e-02,\n",
      "        -6.8554e-03, -1.5683e-02, -3.1714e-03, -2.0770e-03,  8.3786e-03,\n",
      "         9.5288e-03,  4.6958e-03, -1.8547e-03, -9.0096e-03,  1.9196e-03,\n",
      "         3.4466e-03, -1.8788e-02,  2.9061e-03,  1.0381e-02,  1.3106e-02,\n",
      "         1.5700e-02,  1.2154e-02,  6.6039e-03,  1.6958e-02,  5.0236e-03,\n",
      "         1.7987e-03, -1.1679e-02,  8.7765e-03, -2.4106e-02,  1.1181e-02,\n",
      "        -1.1946e-02,  1.5668e-02, -7.8762e-03, -2.2896e-03, -3.5685e-02,\n",
      "        -5.4487e-03, -2.6170e-02,  4.1094e-03, -6.5519e-03,  5.6510e-03,\n",
      "        -2.0410e-02, -8.1720e-03,  8.2673e-04,  1.0607e-02, -1.1410e-02,\n",
      "        -7.6128e-03, -9.4816e-03,  1.3405e-02, -3.6835e-03, -1.6683e-02,\n",
      "        -1.3942e-02, -1.3792e-02,  9.4471e-03, -8.9652e-03,  2.7347e-02,\n",
      "         2.0898e-02,  4.9623e-03, -1.6808e-05, -2.6520e-03,  1.1647e-02,\n",
      "        -1.0380e-02,  1.0258e-02, -1.4365e-02, -2.4067e-03,  2.4611e-03,\n",
      "        -3.4356e-02,  1.0146e-02, -3.6548e-03,  4.5760e-03,  4.7001e-03,\n",
      "         8.2549e-03,  1.0664e-02, -9.9208e-04,  1.5592e-02,  8.8220e-03,\n",
      "         6.7386e-04,  1.2294e-02,  1.9146e-03, -1.8557e-02,  1.5807e-03,\n",
      "        -8.0482e-03,  6.5828e-04, -2.0700e-02, -1.8775e-02,  1.4498e-02,\n",
      "        -1.7702e-03,  3.3606e-03,  1.0260e-02, -1.2938e-02,  8.5929e-04,\n",
      "        -1.7762e-02,  2.0553e-03,  1.0824e-02, -1.2022e-02,  3.0323e-02,\n",
      "        -3.2509e-03,  6.3522e-03,  1.0199e-02, -8.5364e-03, -1.5529e-02,\n",
      "         9.9728e-03,  8.0163e-03, -4.0480e-04,  1.0411e-02, -2.1125e-02,\n",
      "         6.8507e-03, -2.4799e-03,  3.3059e-03,  2.1562e-02, -9.3795e-03,\n",
      "        -1.4787e-02, -6.4737e-03, -6.5289e-03,  1.0457e-02, -1.2625e-02,\n",
      "         2.0103e-03, -2.9515e-03,  1.0454e-02, -6.8263e-03,  3.8408e-02,\n",
      "         2.0190e-02, -1.1249e-02,  1.6824e-02, -1.8365e-02, -1.2714e-02,\n",
      "        -1.5809e-02, -2.4427e-02, -8.3235e-03,  3.4968e-03, -5.2337e-03,\n",
      "         4.0661e-03,  1.2937e-02,  5.0996e-03, -1.3125e-02,  6.7655e-03,\n",
      "        -8.1656e-03, -4.5672e-03,  5.0537e-03, -1.3791e-02, -2.0749e-02,\n",
      "         2.0291e-02,  4.9078e-03, -1.5669e-03,  6.5855e-04, -1.0720e-02,\n",
      "         3.3243e-04,  8.9237e-03, -1.3128e-02, -6.7583e-03,  1.6381e-02,\n",
      "        -1.3247e-02,  3.0157e-02,  5.2167e-03, -2.7213e-03, -5.2794e-03,\n",
      "         1.9231e-02, -2.8575e-04, -1.7570e-02,  2.8012e-02, -9.0929e-03,\n",
      "         5.9580e-03, -1.8743e-03,  1.7570e-02, -3.7091e-03, -2.5054e-04,\n",
      "        -1.8783e-03,  4.1372e-03, -6.2426e-03, -7.8015e-03, -2.6055e-04,\n",
      "        -1.3971e-02,  1.5592e-02,  1.7089e-03,  9.1975e-04, -2.1562e-02,\n",
      "         4.4354e-03, -4.7120e-05,  4.0048e-03,  1.4419e-02,  1.5563e-02,\n",
      "         1.6344e-02, -1.4009e-03, -9.9228e-03, -1.3824e-03, -4.0550e-04,\n",
      "        -3.3453e-03,  9.4000e-03, -4.3326e-03, -1.4137e-02, -8.0890e-04,\n",
      "         1.0144e-02,  3.8446e-03, -1.3442e-03, -2.0073e-02,  9.1787e-03,\n",
      "        -4.1530e-03,  1.4768e-02, -3.4075e-03, -3.0464e-02, -1.1394e-02,\n",
      "         2.9156e-02,  8.0683e-03, -1.4697e-02, -6.8269e-03, -9.0260e-03,\n",
      "        -1.0290e-02,  1.4350e-02,  3.3045e-03, -1.7763e-02,  6.5125e-03,\n",
      "         1.9336e-02,  7.7919e-03,  1.0562e-02,  6.4707e-04,  1.5042e-02,\n",
      "         2.3830e-02, -1.2445e-02,  1.4491e-02,  8.8902e-03, -1.8267e-02,\n",
      "         2.3515e-02, -6.6489e-03,  3.5128e-02,  2.5170e-04, -4.8555e-03,\n",
      "        -2.7336e-02,  8.4858e-03, -5.8308e-03,  1.1585e-02, -1.6239e-02,\n",
      "        -1.1639e-02,  3.1951e-02, -1.4157e-02,  3.8314e-03, -6.4634e-03,\n",
      "        -5.2680e-03,  2.3853e-03,  5.2599e-03, -1.2509e-02,  4.7728e-03,\n",
      "         1.3971e-02,  1.5915e-02,  3.7735e-03,  2.1882e-03, -5.4084e-03,\n",
      "        -1.4288e-02,  7.2517e-03,  1.7150e-02, -7.7471e-03,  7.1999e-04,\n",
      "        -1.6703e-02,  6.7911e-03, -2.0801e-03, -3.5086e-03, -1.6184e-02,\n",
      "        -2.3799e-02,  4.0158e-03, -7.7625e-03, -2.9372e-03, -9.0689e-04,\n",
      "        -1.6794e-02,  1.4085e-03, -7.6271e-03,  2.4550e-02,  1.7928e-02,\n",
      "         1.2226e-02,  1.9779e-02, -3.5425e-03,  1.8690e-02, -2.4519e-03,\n",
      "        -2.3826e-02, -2.2335e-02, -5.1372e-03,  1.0011e-02,  7.4104e-03,\n",
      "        -1.0465e-02, -5.8360e-03,  1.9433e-03,  1.4997e-03,  9.7419e-03,\n",
      "         1.5800e-02, -3.7222e-03, -7.9968e-03, -1.9074e-03,  1.3791e-02,\n",
      "         3.9248e-03, -2.5746e-04, -2.1076e-02, -1.6894e-03, -1.4875e-03,\n",
      "        -3.9739e-03,  7.5566e-06,  6.8049e-03, -2.3059e-03,  1.8894e-03,\n",
      "         1.6424e-02, -1.2909e-03, -9.4796e-05,  3.7299e-02, -2.7499e-03,\n",
      "         4.1764e-04,  1.8316e-02, -2.0265e-03,  2.3555e-04, -1.1350e-02,\n",
      "        -3.0173e-03, -6.0814e-03,  1.2631e-02,  1.0258e-02, -1.4509e-02,\n",
      "        -9.6905e-04, -1.2531e-02,  6.4971e-03, -1.0675e-02,  2.6624e-02,\n",
      "        -5.9326e-03, -6.5250e-03,  3.1382e-02,  3.7653e-03, -3.3202e-03,\n",
      "         7.2544e-04,  6.6511e-03, -4.3270e-03, -4.4595e-03, -8.1593e-03,\n",
      "         1.2377e-02, -1.7702e-02, -8.0512e-03,  1.9036e-02, -2.8721e-03,\n",
      "        -1.3593e-02,  1.0723e-03, -3.9322e-02, -1.4113e-02, -9.0192e-03,\n",
      "        -3.2924e-04,  1.0870e-02,  3.8838e-03, -6.7254e-03, -6.6776e-03,\n",
      "        -3.2928e-03,  1.1470e-02, -1.1558e-02, -1.0245e-02,  4.1738e-04,\n",
      "        -8.0788e-04,  1.3948e-02, -2.5502e-04,  6.8214e-03, -7.1398e-03,\n",
      "         8.3090e-03, -1.3147e-02,  1.8793e-03, -1.2508e-02, -8.2878e-03,\n",
      "         2.8179e-02,  7.7402e-03, -3.8820e-02,  1.4820e-03, -8.7915e-03,\n",
      "        -8.3924e-03, -1.5372e-02, -9.0440e-03,  1.7169e-03, -1.0978e-02,\n",
      "         2.2745e-02, -8.6993e-03, -1.6516e-02, -9.2052e-03, -1.1570e-02,\n",
      "         5.7150e-03, -3.5914e-04,  2.9313e-03, -1.6454e-02,  1.5841e-02,\n",
      "         1.1594e-02, -1.1500e-02, -5.0780e-03, -5.9590e-03, -1.7685e-02,\n",
      "        -1.7995e-03,  1.1933e-02,  6.9028e-03,  8.0554e-03,  1.6181e-02,\n",
      "         1.6880e-02, -8.7138e-03, -1.1510e-03,  3.4486e-03,  7.7682e-03,\n",
      "        -8.4867e-03,  5.1097e-03, -5.9217e-03,  6.0970e-03,  1.4205e-02,\n",
      "        -1.2390e-02,  1.9071e-02,  8.6306e-03,  9.5892e-03, -7.7477e-05,\n",
      "        -5.9190e-03,  6.5829e-03,  1.9024e-02,  2.6710e-02,  3.1668e-02,\n",
      "         6.0262e-03, -1.0919e-02, -3.7968e-03, -1.3783e-02, -9.2213e-03,\n",
      "        -2.4451e-03,  1.3655e-02,  4.3972e-03,  8.2946e-03, -3.4014e-03,\n",
      "         1.0778e-02,  9.7088e-03, -2.7715e-03, -2.0191e-02,  1.6778e-02,\n",
      "        -3.9403e-03, -1.3868e-02, -1.2488e-02,  8.2350e-03, -2.9101e-03,\n",
      "         8.4639e-03,  1.0197e-02,  4.9247e-03,  8.0580e-03,  2.3652e-02,\n",
      "         2.0330e-02, -1.4096e-03, -1.2567e-02,  2.3937e-03, -8.5397e-04,\n",
      "        -1.4303e-02, -8.7737e-03, -9.1988e-03,  1.9144e-02, -1.0198e-02,\n",
      "        -8.6709e-04, -7.7083e-03,  6.4011e-03, -7.4581e-03, -1.1049e-02,\n",
      "         9.5746e-03,  3.6676e-03, -1.4837e-02, -9.3431e-03, -1.2170e-02,\n",
      "         1.1617e-02, -2.9470e-02,  1.0404e-02,  4.3709e-03,  6.3479e-03,\n",
      "         2.6396e-03, -1.1986e-02,  1.7167e-02, -5.6456e-03,  1.2092e-02,\n",
      "        -3.0918e-03,  1.5125e-02,  1.1277e-02, -2.3653e-03,  4.2142e-03,\n",
      "         9.7379e-04,  9.6297e-03,  1.2408e-02,  6.8383e-03, -6.4126e-03,\n",
      "         2.0081e-02,  3.1956e-03,  1.6447e-02, -2.4361e-02,  4.9499e-03,\n",
      "        -1.2174e-02,  8.9267e-03, -2.0476e-02, -1.1720e-02,  1.1190e-02,\n",
      "        -3.4523e-02, -2.1246e-03,  5.6201e-03,  1.5380e-02,  1.1789e-02,\n",
      "         2.2887e-02, -3.5305e-03, -4.4414e-03, -3.3251e-02, -5.3610e-03,\n",
      "        -1.3418e-02,  2.2603e-02, -2.2671e-03, -1.4760e-02,  6.9824e-03,\n",
      "         1.1244e-02, -8.7106e-03,  4.0613e-02,  2.7885e-03,  4.4191e-03,\n",
      "        -6.2706e-03,  1.3563e-02,  6.1932e-04,  4.5171e-02,  5.3947e-04,\n",
      "         1.7200e-03, -2.1877e-02,  1.0863e-02, -1.6263e-02, -2.3760e-03,\n",
      "        -1.2626e-02,  2.1996e-02], device='cuda:0', requires_grad=True)\n",
      "fc1.4.weight Parameter containing:\n",
      "tensor([[-2.1821e-02,  2.1651e-02,  1.1818e-02,  ..., -2.4238e-02,\n",
      "          1.2962e-02,  2.6580e-03],\n",
      "        [-1.7245e-02,  2.2996e-02, -2.0735e-02,  ..., -2.7232e-02,\n",
      "         -1.8610e-02, -1.3374e-02],\n",
      "        [-1.2016e-02,  8.4096e-03,  1.2458e-02,  ...,  9.3964e-05,\n",
      "          2.2693e-02,  1.7867e-02],\n",
      "        ...,\n",
      "        [-4.5754e-03, -1.6153e-02, -7.3686e-03,  ..., -1.1744e-02,\n",
      "          2.6790e-02, -2.2631e-02],\n",
      "        [-2.7722e-02,  4.1366e-03,  1.0771e-02,  ...,  3.1751e-03,\n",
      "          1.4371e-02, -1.1329e-03],\n",
      "        [ 7.3973e-03,  1.3753e-02, -5.9196e-03,  ...,  1.0884e-02,\n",
      "         -2.6151e-02, -1.8137e-02]], device='cuda:0', requires_grad=True)\n",
      "fc1.4.bias Parameter containing:\n",
      "tensor([ 0.0183,  0.0129, -0.0047, -0.0245,  0.0203, -0.0096, -0.0153,  0.0066,\n",
      "        -0.0214, -0.0234,  0.0162,  0.0255, -0.0096,  0.0180, -0.0095, -0.0025,\n",
      "        -0.0212,  0.0146, -0.0038,  0.0098,  0.0209, -0.0026, -0.0041, -0.0122,\n",
      "         0.0232, -0.0069, -0.0063, -0.0111,  0.0026,  0.0138,  0.0229,  0.0222,\n",
      "         0.0245,  0.0139,  0.0080,  0.0237,  0.0183,  0.0244, -0.0138, -0.0210,\n",
      "         0.0239, -0.0065, -0.0122, -0.0032, -0.0243, -0.0185, -0.0217, -0.0082,\n",
      "         0.0037, -0.0193, -0.0127, -0.0253,  0.0167, -0.0226, -0.0011, -0.0115,\n",
      "        -0.0169, -0.0223,  0.0097, -0.0148, -0.0124, -0.0022, -0.0123,  0.0214,\n",
      "        -0.0191, -0.0006, -0.0154, -0.0045, -0.0154, -0.0259,  0.0005, -0.0167,\n",
      "        -0.0014, -0.0077,  0.0239, -0.0206,  0.0097,  0.0110,  0.0052, -0.0055,\n",
      "         0.0154,  0.0216, -0.0080,  0.0261, -0.0037, -0.0151, -0.0239, -0.0261,\n",
      "         0.0058,  0.0050,  0.0198, -0.0118, -0.0041,  0.0174,  0.0240, -0.0128,\n",
      "         0.0118,  0.0192, -0.0173, -0.0256, -0.0068,  0.0170,  0.0091, -0.0078,\n",
      "         0.0227,  0.0126, -0.0131,  0.0233,  0.0250,  0.0035, -0.0260, -0.0169,\n",
      "        -0.0119,  0.0186, -0.0034,  0.0027, -0.0018,  0.0245, -0.0187, -0.0256,\n",
      "         0.0125,  0.0010, -0.0264,  0.0082,  0.0228,  0.0013, -0.0004,  0.0003,\n",
      "        -0.0168, -0.0088, -0.0014, -0.0067,  0.0041, -0.0086, -0.0041, -0.0052,\n",
      "         0.0095, -0.0191,  0.0259, -0.0229, -0.0070,  0.0034,  0.0231, -0.0034,\n",
      "         0.0214, -0.0042,  0.0103, -0.0112,  0.0017,  0.0009,  0.0254,  0.0024,\n",
      "         0.0047,  0.0210, -0.0087, -0.0110,  0.0173, -0.0044,  0.0260, -0.0210,\n",
      "         0.0041,  0.0260, -0.0114,  0.0150, -0.0197,  0.0016, -0.0152, -0.0018,\n",
      "         0.0203, -0.0188,  0.0193,  0.0072,  0.0089, -0.0174,  0.0137, -0.0033,\n",
      "         0.0121,  0.0032,  0.0244, -0.0164,  0.0264,  0.0122, -0.0188,  0.0170,\n",
      "        -0.0220, -0.0059, -0.0230,  0.0210, -0.0195, -0.0158, -0.0048, -0.0193,\n",
      "        -0.0241, -0.0051, -0.0201,  0.0250, -0.0023, -0.0046, -0.0074,  0.0008,\n",
      "        -0.0244,  0.0111,  0.0214, -0.0242, -0.0233, -0.0002, -0.0076,  0.0139,\n",
      "        -0.0042, -0.0123,  0.0152, -0.0014, -0.0010,  0.0191, -0.0130, -0.0113,\n",
      "        -0.0017,  0.0088, -0.0174, -0.0063, -0.0181,  0.0240, -0.0214,  0.0075,\n",
      "        -0.0245,  0.0118, -0.0090,  0.0184,  0.0233,  0.0208, -0.0026,  0.0122,\n",
      "        -0.0007,  0.0104, -0.0073, -0.0108,  0.0190, -0.0078, -0.0110, -0.0042,\n",
      "         0.0168, -0.0166, -0.0208,  0.0060,  0.0204,  0.0123,  0.0149, -0.0047,\n",
      "        -0.0167, -0.0100, -0.0079,  0.0257, -0.0010, -0.0025, -0.0017, -0.0121],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for x, y in model.named_parameters():\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4705e-06, -1.5091e-06,  2.6124e-06,  1.1937e-06, -9.5364e-07,\n",
      "          4.8526e-09,  2.9668e-06,  2.0681e-06,  3.0560e-06, -8.6873e-07,\n",
      "          2.4488e-06, -2.7257e-06, -2.9203e-06, -2.5097e-06,  2.0711e-06,\n",
      "         -1.0097e-06, -3.0860e-06,  2.6070e-06, -1.7544e-06, -2.1002e-06,\n",
      "          2.0549e-06, -1.7169e-06, -8.0820e-07, -2.8750e-06,  1.7985e-06,\n",
      "          1.3931e-06,  1.1287e-06, -2.0333e-06,  1.6162e-06, -1.7779e-06,\n",
      "         -2.3550e-06, -1.8789e-06],\n",
      "        [-1.4705e-06, -1.5091e-06,  2.6124e-06,  1.1937e-06, -9.5364e-07,\n",
      "          4.8526e-09,  2.9668e-06,  2.0681e-06,  3.0560e-06, -8.6873e-07,\n",
      "          2.4488e-06, -2.7257e-06, -2.9203e-06, -2.5097e-06,  2.0711e-06,\n",
      "         -1.0097e-06, -3.0860e-06,  2.6070e-06, -1.7544e-06, -2.1002e-06,\n",
      "          2.0549e-06, -1.7169e-06, -8.0820e-07, -2.8750e-06,  1.7985e-06,\n",
      "          1.3931e-06,  1.1287e-06, -2.0333e-06,  1.6162e-06, -1.7779e-06,\n",
      "         -2.3550e-06, -1.8789e-06],\n",
      "        [-1.4705e-06, -1.5091e-06,  2.6124e-06,  1.1937e-06, -9.5364e-07,\n",
      "          4.8526e-09,  2.9668e-06,  2.0681e-06,  3.0560e-06, -8.6873e-07,\n",
      "          2.4488e-06, -2.7257e-06, -2.9203e-06, -2.5097e-06,  2.0711e-06,\n",
      "         -1.0097e-06, -3.0860e-06,  2.6070e-06, -1.7544e-06, -2.1002e-06,\n",
      "          2.0549e-06, -1.7169e-06, -8.0820e-07, -2.8750e-06,  1.7985e-06,\n",
      "          1.3931e-06,  1.1287e-06, -2.0333e-06,  1.6162e-06, -1.7779e-06,\n",
      "         -2.3550e-06, -1.8789e-06],\n",
      "        [-1.4705e-06, -1.5091e-06,  2.6124e-06,  1.1937e-06, -9.5364e-07,\n",
      "          4.8526e-09,  2.9668e-06,  2.0681e-06,  3.0560e-06, -8.6873e-07,\n",
      "          2.4488e-06, -2.7257e-06, -2.9203e-06, -2.5097e-06,  2.0711e-06,\n",
      "         -1.0097e-06, -3.0860e-06,  2.6070e-06, -1.7544e-06, -2.1002e-06,\n",
      "          2.0549e-06, -1.7169e-06, -8.0820e-07, -2.8750e-06,  1.7985e-06,\n",
      "          1.3931e-06,  1.1287e-06, -2.0333e-06,  1.6162e-06, -1.7779e-06,\n",
      "         -2.3550e-06, -1.8789e-06],\n",
      "        [-1.4705e-06, -1.5091e-06,  2.6124e-06,  1.1937e-06, -9.5364e-07,\n",
      "          4.8526e-09,  2.9668e-06,  2.0681e-06,  3.0560e-06, -8.6873e-07,\n",
      "          2.4488e-06, -2.7257e-06, -2.9203e-06, -2.5097e-06,  2.0711e-06,\n",
      "         -1.0097e-06, -3.0860e-06,  2.6070e-06, -1.7544e-06, -2.1002e-06,\n",
      "          2.0549e-06, -1.7169e-06, -8.0820e-07, -2.8750e-06,  1.7985e-06,\n",
      "          1.3931e-06,  1.1287e-06, -2.0333e-06,  1.6162e-06, -1.7779e-06,\n",
      "         -2.3550e-06, -1.8789e-06]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  print(model.forward_once(dataset_val.rows[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15a3961c310>]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
