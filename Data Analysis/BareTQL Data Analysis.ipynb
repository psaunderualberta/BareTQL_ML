{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from itertools import permutations, accumulate\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import operator\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# https://medium.com/deep-learning-hk/compute-document-similarity-using-autoencoder-with-triplet-loss-eb7eb132eb38\n",
    "# https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73\n",
    "# FaceNet: https://arxiv.org/pdf/1503.03832.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfCheck(df):\n",
    "    \n",
    "    # Any column is > 20% null values\n",
    "    if any(cnt / len(df) > 0.2 for cnt in df.isna().sum()):\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 41575 tables\n",
      "Total tables analyzed: 39393\n",
      "Maximum row length: 1017\n"
     ]
    }
   ],
   "source": [
    "tables = defaultdict(list)\n",
    "path = \"../Data/tables_025.csv\"\n",
    "maxRowLen = 0\n",
    "maxRowLenR = \"\"\n",
    "maxRowLenT = \"\"\n",
    "allRows = []\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    tableCount = 0\n",
    "    for line in f:\n",
    "        if line.startswith(\"List of\"):\n",
    "            tableCount += 1\n",
    "            \n",
    "tableFrac = 0.25\n",
    "tablesLim = int(tableCount * tableFrac)\n",
    "tablesRead = 0\n",
    "\n",
    "print(\"Reading \" + str(tablesLim) + \" tables\")\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    tableTitle = \"\"\n",
    "    table = []\n",
    "    \n",
    "    for row in reader:\n",
    "        if len(row) == 1 and row[0].startswith(\"List of\"):\n",
    "            if tableTitle: # Ignore first table, doesn't exist yet\n",
    "                if dfCheck(pd.DataFrame(table)):\n",
    "                    joinedRows = ['|||'.join(row) for row in table]\n",
    "                    tables[tableTitle].append(joinedRows)\n",
    "                    allRows.extend(joinedRows)\n",
    "                table = []\n",
    "            tableTitle = row[0]\n",
    "            \n",
    "            tablesRead += 1\n",
    "            if tablesRead > tablesLim:\n",
    "                break\n",
    "            \n",
    "        elif len(row) != 0:\n",
    "            table.append(row)\n",
    "            \n",
    "            \n",
    "print(\"Total tables analyzed: \" + str(sum(len(val) for val in tables.values())))\n",
    "maxRowLen = max(len(row) for row in allRows)\n",
    "print(\"Maximum row length: \" + str(maxRowLen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(allRows)\n",
    "id_count = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of tables in dataset: 15762\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Convert each string into a list of its characters, converted to ints\n",
    "Filter out rows which are too long (i.e. not many other rows at that length)\n",
    "\"\"\"\n",
    "import json\n",
    "\n",
    "table_chars = []\n",
    "sizes = [len(seq) for seq in allRows]\n",
    "mu, sigma = np.mean(sizes), np.std(sizes)\n",
    "maxRowLen = int(mu)\n",
    "\n",
    "for i, category in enumerate(tables):\n",
    "    for i, table in enumerate(tables[category]):\n",
    "        tokens = tokenizer.texts_to_sequences(table)\n",
    "        char_table = np.array([np.pad(seq, (0, maxRowLen - len(seq))) for seq in tokens if len(seq) <= maxRowLen])\n",
    "\n",
    "        if len(char_table) >= 10:  # Remove tables that no longer have sufficient rows. We use 10 to allow for training & testing purposes   \n",
    "            table_chars.append(char_table)\n",
    "\n",
    "table_tensors = [torch.tensor(table) for table in table_chars]\n",
    "print(\"Total # of tables in dataset: \" + str(len(table_tensors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../Data/tables.json\", \"w\") as f:\n",
    "#     f.write(json.dumps([l.tolist() for l in table_chars]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "class TableDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tables):\n",
    "        self.sizes = torch.tensor([table.shape[0] for table in tables])\n",
    "        self.table_positions = self.get_table_positions()\n",
    "        self.rows = torch.cat(tables, 0).float()\n",
    "        self.max = torch.max(self.rows)\n",
    "        self.rows = self.rows / self.max  # Normalization\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sizes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx == 0:\n",
    "            return self.rows[:self.sizes[0]]\n",
    "            \n",
    "        return self.rows[self.sizes[idx-1]:self.sizes[idx]]\n",
    "\n",
    "    def get_table_positions(self):\n",
    "        return np.array([0] + list(accumulate(self.sizes, operator.add)), dtype=np.int32)\n",
    "\n",
    "    def shuffle(self):\n",
    "        idxs = np.random.permutation(len(self.sizes))\n",
    "        tables = torch.split(self.rows, list(self.sizes))  # split_size_or_sections needs a list for some reason\n",
    "        tables = [tables[idx] for idx in idxs]\n",
    "        self.rows = torch.cat(tables, 0)\n",
    "        self.sizes = self.sizes[idxs]\n",
    "        self.table_positions = self.get_table_positions()\n",
    "        return\n",
    "\n",
    "    def get_batch(self, model, start_idx, batch_size):\n",
    "        \"\"\"\n",
    "        Using an online novel triplet mining strategy described in https://arxiv.org/pdf/1503.03832.pdf\n",
    "        \n",
    "        We perform all computations in numpy since it is much faster than with pytorch\n",
    "        \"\"\"\n",
    "        # t = time.time()\n",
    "        table_positions = self.table_positions[start_idx: start_idx+batch_size+1]\n",
    "\n",
    "        # Get 5 rows per table for a subset of the tables\n",
    "        chosen_row_idxs = np.arange(table_positions[0], table_positions[-1])\n",
    "        chosen_rows = self.rows[table_positions[0]:table_positions[-1],:].float().to(device)\n",
    "\n",
    "        # turn off autograd to speed up computation\n",
    "        with torch.no_grad():\n",
    "            embeddings = np.array(model(chosen_rows).cpu())\n",
    "    \n",
    "        dists = np.matmul(embeddings, embeddings.T)\n",
    "\n",
    "        pos_mask = np.zeros(dists.shape)\n",
    "        for start, end in zip(table_positions, table_positions[1:]):\n",
    "            pos_mask[start:end,start:end] = 1\n",
    "        \n",
    "        # print(\"Elapsed: \" + str(time.time() - t), len(chosen_rows), len(anchor_idxs))\n",
    "        return (\n",
    "            chosen_rows.to(device), \n",
    "            torch.tensor(pos_mask).float().to(device)\n",
    "        )\n",
    "\n",
    "    def get_average_precision(self, model, first_tables=20):\n",
    "        with torch.no_grad():\n",
    "            embs = np.array(model(self.rows.long().to(device)).cpu())  # Our model uses small => better, avg_precision uses opposite\n",
    "        precs = []\n",
    "\n",
    "        dists = np.matmul(embs, embs.T)\n",
    "        pos_mask = np.zeros(dists.shape, dtype=bool)\n",
    "        \n",
    "        for table_start, table_end in zip(self.table_positions[:first_tables], self.table_positions[1:first_tables]):\n",
    "            pos_mask[table_start:table_end,table_start:table_end] = 1\n",
    "\n",
    "        for mask, scores in zip(pos_mask, dists):\n",
    "            # print(mask, scores)\n",
    "            avg_prec_score = average_precision_score(mask, scores)\n",
    "            precs.append(avg_prec_score)\n",
    "\n",
    "        return precs\n",
    "\n",
    "table_tensors_train = []\n",
    "table_tensors_val = []\n",
    "table_tensors_test = []\n",
    "take_tables = 100\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Generate training, validation, and testing datasets\n",
    "for table in table_tensors[:take_tables]:\n",
    "    # Select 5 random rows to be validation / test rows\n",
    "    non_train_idxs = torch.randint(0, table.shape[0], size=(1, 5)).flatten()\n",
    "    val_idxs, test_idxs = non_train_idxs[:3], non_train_idxs[3:]\n",
    "\n",
    "    # All other rows are training rows\n",
    "    train_idxs = torch.ones(table.shape[0], dtype=bool)\n",
    "    train_idxs[non_train_idxs] = False\n",
    "\n",
    "    # append the data to appropriate list\n",
    "    table_tensors_train.append(table[train_idxs])\n",
    "    table_tensors_val.append(table[val_idxs])\n",
    "    table_tensors_test.append(table[test_idxs])\n",
    "\n",
    "dataset_train = TableDataset(table_tensors_train)\n",
    "dataset_val = TableDataset(table_tensors_val)\n",
    "dataset_test = TableDataset(table_tensors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model based on https://towardsdatascience.com/a-friendly-introduction-to-siamese-networks-85ab17522942\n",
    "\"\"\"\n",
    "\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, emb_dim, h_activ=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Defining the fully connected layers\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(maxRowLen, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(128, emb_dim)\n",
    "          )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        return F.normalize(out, p=2, dim=-1)\n",
    "\n",
    "# class APLoss(nn.Module):\n",
    "#     \"\"\"\n",
    "#     https://openaccess.thecvf.com/content_ICCV_2019/papers/Revaud_Learning_With_Average_Precision_Training_Image_Retrieval_With_a_Listwise_ICCV_2019_paper.pdf\n",
    "#     \"\"\"\n",
    "#     def __init__(self, M):\n",
    "#         super(APLoss,self).__init__()\n",
    "#         self.M = torch.tensor(M)\n",
    "#         self.delta = torch.tensor(2 / (M - 1))\n",
    "#         self.bin_centers = torch.tensor([1 - (m - 1)*self.delta for m in range(1, M+1)])\n",
    "\n",
    "#     def forward(self, s, y):\n",
    "\n",
    "#         sims = torch.matmul(s, s.T)\n",
    "#         APs = torch.tensor([self.AP(sim.T, truth) for sim, truth in zip(sims, y)])\n",
    "#         mAP = torch.sum(APs)\n",
    "\n",
    "#         return 1 - mAP / s.shape[0]\n",
    "\n",
    "#     def AP(self, sims, truth):\n",
    "#         precisions = torch.tensor([self.quantized_precision(sims, truth, m_prime) for m_prime in range(1, self.M+1)])\n",
    "#         recalls = torch.tensor([self.incremental_recall(sims, truth, m_prime) for m_prime in range(1, self.M+1)])\n",
    "#         return torch.sum(precisions * recalls)\n",
    "\n",
    "#     def quantized_precision(self, S_q, Y_q, m):\n",
    "#         numerator = 0\n",
    "#         denominator = 0\n",
    "#         for m_prime in range(1, m+1):\n",
    "#             sigma_res = self.sigma(S_q, m_prime)\n",
    "#             numerator += torch.matmul(sigma_res.T, Y_q)\n",
    "#             denominator += torch.sum(sigma_res)\n",
    "        \n",
    "#         return numerator / denominator\n",
    "\n",
    "#     def incremental_recall(self, S_q, Y_q, m):\n",
    "#         N_q = torch.count_nonzero(Y_q)  # Not sure this is correct\n",
    "#         return torch.matmul(self.sigma(S_q, m).T, Y_q) / N_q\n",
    "\n",
    "#     def sigma(self, x, m):\n",
    "#         return torch.relu(1 - torch.abs((x - self.bin_centers[m-1]) / self.delta))\n",
    "\n",
    "\n",
    "class APLoss (nn.Module):\n",
    "    \"\"\" Differentiable AP loss, through quantization. From the paper:\n",
    "        Learning with Average Precision: Training Image Retrieval with a Listwise Loss\n",
    "        Jerome Revaud, Jon Almazan, Rafael Sampaio de Rezende, Cesar de Souza\n",
    "        https://arxiv.org/abs/1906.07589\n",
    "        Input: (N, M)   values in [min, max]\n",
    "        label: (N, M)   values in {0, 1}\n",
    "        Returns: 1 - mAP (mean AP for each n in {1..N})\n",
    "                 Note: typically, this is what you wanna minimize\n",
    "    \"\"\"\n",
    "    def __init__(self, nq=5, min=0, max=1):\n",
    "        nn.Module.__init__(self)\n",
    "        assert isinstance(nq, int) and 2 <= nq <= 100\n",
    "        self.nq = nq\n",
    "        self.min = min\n",
    "        self.max = max\n",
    "        gap = max - min\n",
    "        assert gap > 0\n",
    "        # Initialize quantizer as non-trainable convolution\n",
    "        self.quantizer = q = nn.Conv1d(1, 2*nq, kernel_size=1, bias=True)\n",
    "        q.weight = nn.Parameter(q.weight.detach(), requires_grad=False)\n",
    "        q.bias = nn.Parameter(q.bias.detach(), requires_grad=False)\n",
    "        a = (nq-1) / gap\n",
    "        # First half equal to lines passing to (min+x,1) and (min+x+1/a,0) with x = {nq-1..0}*gap/(nq-1)\n",
    "        q.weight[:nq] = -a\n",
    "        q.bias[:nq] = torch.from_numpy(a*min + np.arange(nq, 0, -1))  # b = 1 + a*(min+x)\n",
    "        # First half equal to lines passing to (min+x,1) and (min+x-1/a,0) with x = {nq-1..0}*gap/(nq-1)\n",
    "        q.weight[nq:] = a\n",
    "        q.bias[nq:] = torch.from_numpy(np.arange(2-nq, 2, 1) - a*min)  # b = 1 - a*(min+x)\n",
    "        # First and last one as a horizontal straight line\n",
    "        q.weight[0] = q.weight[-1] = 0\n",
    "        q.bias[0] = q.bias[-1] = 1\n",
    "\n",
    "    def forward(self, sims, label, qw=None, ret='1-mAP'):\n",
    "        x = torch.matmul(sims, sims.T).T\n",
    "        assert x.shape == label.shape  # N x M\n",
    "        N, M = x.shape\n",
    "        # Quantize all predictions\n",
    "        q = self.quantizer(x.unsqueeze(1))\n",
    "        q = torch.min(q[:, :self.nq], q[:, self.nq:]).clamp(min=0)  # N x Q x M\n",
    "\n",
    "        nbs = q.sum(dim=-1)  # number of samples  N x Q = c\n",
    "        rec = (q * label.view(N, 1, M).float()).sum(dim=-1)  # number of correct samples = c+ N x Q\n",
    "        prec = rec.cumsum(dim=-1) / (1e-16 + nbs.cumsum(dim=-1))  # precision\n",
    "        rec /= rec.sum(dim=-1).unsqueeze(1)  # norm in [0,1]\n",
    "\n",
    "        ap = (prec * rec).sum(dim=-1)  # per-image AP\n",
    "\n",
    "        if ret == '1-mAP':\n",
    "            if qw is not None:\n",
    "                ap *= qw  # query weights\n",
    "            return 1 - ap.mean()\n",
    "        elif ret == 'AP':\n",
    "            assert qw is None\n",
    "            return ap\n",
    "        else:\n",
    "            raise ValueError(\"Bad return type for APLoss(): %s\" % str(ret))\n",
    "\n",
    "    def measures(self, x, gt, loss=None):\n",
    "        if loss is None:\n",
    "            loss = self.forward(x, gt)\n",
    "        return {'loss_ap': float(loss)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3292/1126186371.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mdataset_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mtrain_losses\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;31m# if epoch % 100 == 0:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3292/1126186371.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, loss_fn, data, BATCH_SIZE)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mbatch_embs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_embs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# back-propagation, could do manually\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Adjusts the weights for us\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\pahas\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\pahas\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(model, optimizer, loss_fn, data, BATCH_SIZE=100):\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for i in range(0, len(data), BATCH_SIZE):\n",
    "        batch, mask = data.get_batch(model, i, BATCH_SIZE)\n",
    "\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        batch_embs = model(batch)    \n",
    "        loss = loss_fn(batch_embs, mask)\n",
    "        loss.backward() # back-propagation, could do manually\n",
    "        optimizer.step() # Adjusts the weights for us\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return losses\n",
    "\n",
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        precision_scores = data.get_average_precision(model)\n",
    "\n",
    "    return precision_scores\n",
    "\n",
    "EPOCHS = 1000\n",
    "model = LinearModel(256).to(device)\n",
    "loss_fn = APLoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-1, weight_decay=0.0005)\n",
    "train_losses = []\n",
    "precision_scores = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"EPOCH: \" + str(epoch))\n",
    "\n",
    "    dataset_train.shuffle()\n",
    "    train_losses += train(model, optimizer, loss_fn, dataset_train)\n",
    "\n",
    "    # if epoch % 100 == 0:\n",
    "        # dataset_val.shuffle()\n",
    "        # prec = evaluate(model, dataset_val)\n",
    "        # precision_scores.extend(prec)\n",
    "        # print(\"Average Precision at epoch %s: %s\" % (epoch, np.mean(prec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test to make sure that a single pass over 'get_triplets' works as intended\n",
    "\"\"\"\n",
    "batch, mask = dataset_val.get_batch(model, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x234af167160>]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('fc1.0.weight', Parameter containing:\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], requires_grad=True)), ('fc1.0.bias', Parameter containing:\n",
      "tensor([    nan,     nan,     nan,     nan,     nan, -0.0951,     nan, -0.1124,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan, -0.0728,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,  0.0093,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "        -0.0704,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan,\n",
      "            nan,     nan,     nan,     nan,     nan,     nan,     nan,     nan],\n",
      "       requires_grad=True)), ('fc1.3.weight', Parameter containing:\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], requires_grad=True)), ('fc1.3.bias', Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], requires_grad=True)), ('fc1.6.weight', Parameter containing:\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], requires_grad=True)), ('fc1.6.bias', Parameter containing:\n",
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       requires_grad=True))]\n",
      "tensor(nan, grad_fn=<RsubBackward1>)\n",
      "tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch, mask = dataset_train.get_batch(model, 0, 25)\n",
    "embs = model(batch) \n",
    "l = loss_fn(embs, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.0.weight Parameter containing:\n",
      "tensor([[-0.0449,  0.0578,  0.0582,  ...,  0.0014,  0.0099, -0.0116],\n",
      "        [ 0.1102,  0.1143,  0.0734,  ..., -0.0984, -0.0091, -0.0624],\n",
      "        [-0.0865,  0.0092, -0.1081,  ..., -0.0036, -0.0727,  0.0977],\n",
      "        ...,\n",
      "        [ 0.0140, -0.0578,  0.1152,  ..., -0.0385, -0.1113, -0.0516],\n",
      "        [ 0.0740, -0.1186, -0.0937,  ...,  0.0045, -0.0644,  0.0297],\n",
      "        [-0.0309, -0.0109, -0.0004,  ..., -0.0372, -0.1175,  0.0393]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "fc1.0.bias Parameter containing:\n",
      "tensor([-3.9403e-02,  8.3817e-02, -5.3455e-02,  6.8932e-02,  4.5411e-02,\n",
      "         6.7924e-03,  2.6026e-02,  4.2263e-02, -8.7272e-05, -1.0197e-01,\n",
      "        -2.3098e-02, -5.0014e-03,  1.0178e-01,  3.7622e-02, -9.9214e-02,\n",
      "        -1.0024e-01,  1.8203e-02,  1.0494e-01,  4.0220e-02,  7.0058e-02,\n",
      "        -7.5623e-02, -1.1379e-01,  1.1467e-01, -7.9106e-02, -8.6283e-02,\n",
      "         6.2961e-02, -6.5376e-02,  4.4567e-02,  7.6907e-02,  4.2444e-02,\n",
      "         4.4839e-02,  1.0053e-01,  4.1048e-02,  1.1051e-01,  1.7087e-02,\n",
      "        -5.7009e-02,  2.0000e-02,  6.4004e-02, -7.1457e-03,  9.9430e-02,\n",
      "        -8.4226e-02, -1.9506e-02, -3.8126e-02, -9.3475e-02, -2.5875e-02,\n",
      "         1.0667e-01,  5.2610e-02,  7.3913e-02,  8.1045e-02,  7.5433e-02,\n",
      "        -9.3835e-02, -3.5490e-02, -3.7505e-03,  1.1561e-01, -4.9072e-02,\n",
      "         7.7564e-02, -7.5232e-02,  1.0817e-01, -2.8212e-02,  5.3050e-02,\n",
      "        -4.9489e-02, -3.7598e-02, -2.4775e-02,  7.1770e-02, -1.3896e-02,\n",
      "         4.1433e-02,  6.0014e-02,  1.0843e-01, -1.3172e-02,  7.8498e-03,\n",
      "         5.7183e-02, -2.7496e-03,  7.7840e-02, -7.3101e-04, -6.3832e-02,\n",
      "         8.7789e-03,  1.1874e-01,  7.1713e-02, -6.3464e-02,  6.0888e-02,\n",
      "        -2.0341e-02, -7.6369e-02,  9.0623e-02,  6.4883e-02, -1.1929e-01,\n",
      "        -5.2192e-02,  1.0056e-01, -1.0460e-01,  9.9298e-02,  4.5130e-02,\n",
      "         7.8018e-02, -1.0534e-01, -1.1385e-02,  4.7988e-02, -8.7498e-02,\n",
      "         9.1687e-02,  1.1438e-01, -2.0666e-02,  9.1846e-03, -6.5903e-02,\n",
      "         2.1212e-02,  6.7258e-03, -6.2858e-02, -9.2796e-02,  1.0199e-01,\n",
      "         3.5597e-02,  3.3498e-02,  1.1129e-01,  8.9513e-02, -1.1554e-01,\n",
      "        -7.0067e-02, -1.6638e-02,  1.1580e-01,  6.0519e-02, -4.9568e-02,\n",
      "        -7.4355e-02,  1.0071e-01, -6.2808e-02, -5.8691e-02, -1.9247e-02,\n",
      "        -4.6282e-02, -2.1668e-02, -9.4275e-02, -1.2029e-02, -6.2241e-02,\n",
      "        -3.9474e-02, -8.2154e-02,  6.8361e-02], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "fc1.3.weight Parameter containing:\n",
      "tensor([[-0.0198,  0.0463, -0.0593,  ..., -0.0274, -0.0372, -0.0385],\n",
      "        [-0.0549,  0.0457,  0.0195,  ...,  0.0802, -0.0099,  0.0019],\n",
      "        [-0.0259,  0.0426, -0.0625,  ...,  0.0451, -0.0549, -0.0381],\n",
      "        ...,\n",
      "        [-0.0482,  0.0542, -0.0371,  ...,  0.0701, -0.0732, -0.0397],\n",
      "        [-0.0437, -0.0171,  0.0223,  ...,  0.0304,  0.0417,  0.0380],\n",
      "        [-0.0849,  0.0738, -0.0238,  ...,  0.0585,  0.0383,  0.0577]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "fc1.3.bias Parameter containing:\n",
      "tensor([-5.3352e-02,  5.5743e-02, -2.2500e-02,  2.0473e-02, -2.3820e-02,\n",
      "        -8.1501e-02, -6.0261e-02, -3.5406e-02,  1.2084e-02,  8.0700e-02,\n",
      "         8.0989e-02,  1.9611e-03,  8.1598e-02,  7.8808e-02, -7.3486e-03,\n",
      "         3.1071e-02,  8.4772e-02, -1.5657e-02,  2.0519e-03,  7.9197e-02,\n",
      "        -1.7033e-02, -7.6077e-02,  5.9621e-02, -4.4588e-02,  3.4195e-02,\n",
      "         1.1946e-02,  4.2160e-02, -7.7034e-02, -6.0800e-02,  8.2465e-02,\n",
      "         4.1824e-03,  4.6212e-02, -2.2573e-02, -4.4107e-02,  6.6888e-02,\n",
      "         6.2888e-05,  2.0765e-02, -2.0653e-03, -8.0526e-02,  8.7783e-02,\n",
      "        -7.0394e-02, -3.3190e-02,  7.5404e-02, -6.9348e-02,  2.5462e-02,\n",
      "         2.5598e-02,  4.3028e-02, -6.4697e-02, -4.1609e-02, -5.3871e-02,\n",
      "         3.6701e-02, -2.8031e-02,  4.3103e-02, -7.2583e-02,  8.0255e-02,\n",
      "         1.3654e-02, -8.9675e-03, -4.4450e-02,  5.2871e-02, -5.9782e-02,\n",
      "         7.3583e-02,  1.2163e-02,  7.2249e-02,  2.3147e-02, -8.4017e-02,\n",
      "        -3.0905e-02, -2.3002e-02, -6.2160e-02, -4.4096e-02, -7.0526e-02,\n",
      "         2.3975e-02,  1.9424e-02, -7.0599e-02, -7.0180e-02,  1.3285e-02,\n",
      "         2.2766e-02,  2.4215e-02, -2.4307e-02, -8.8154e-02,  2.9159e-02,\n",
      "         2.4249e-02, -1.6064e-02,  5.2326e-02, -7.6062e-02,  5.1688e-02,\n",
      "         3.0913e-02, -4.1253e-02,  5.8933e-02,  2.4686e-02, -5.5293e-02,\n",
      "         8.4089e-02,  8.3028e-02, -5.5336e-02, -1.1181e-02, -3.4241e-02,\n",
      "         1.4713e-02, -4.3653e-02,  6.2194e-02,  5.6194e-02,  2.9412e-02,\n",
      "        -1.6334e-02,  7.6830e-02,  4.7289e-02, -6.1702e-02,  3.9785e-02,\n",
      "         2.1623e-02, -5.1367e-02, -8.0324e-02,  4.4561e-02, -4.0927e-02,\n",
      "         6.1708e-02,  2.3401e-03, -6.8262e-02,  2.1424e-02, -6.6153e-02,\n",
      "        -5.3566e-02, -1.8435e-02, -3.3636e-02,  4.9877e-02,  2.3520e-02,\n",
      "         5.6472e-02, -1.7976e-02,  8.7236e-02,  1.5965e-02, -6.1976e-02,\n",
      "         7.6032e-02, -3.9913e-02,  8.8337e-02], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "fc1.6.weight Parameter containing:\n",
      "tensor([[ 0.0104, -0.0768, -0.0841,  ..., -0.0753, -0.0451, -0.0771],\n",
      "        [ 0.0430, -0.0060, -0.0846,  ..., -0.0327,  0.0094,  0.0760],\n",
      "        [ 0.0426, -0.0584,  0.0327,  ..., -0.0532, -0.0162, -0.0162],\n",
      "        ...,\n",
      "        [-0.0416, -0.0401,  0.0750,  ...,  0.0185,  0.0369,  0.0610],\n",
      "        [-0.0610, -0.0319, -0.0008,  ..., -0.0773,  0.0484, -0.0074],\n",
      "        [ 0.0191,  0.0083, -0.0429,  ...,  0.0598,  0.0096, -0.0335]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "fc1.6.bias Parameter containing:\n",
      "tensor([-0.0848,  0.0723, -0.0717,  0.0228,  0.0262,  0.0124, -0.0188,  0.0212,\n",
      "        -0.0130,  0.0420, -0.0150, -0.0154,  0.0808, -0.0560,  0.0066,  0.0566,\n",
      "        -0.0451,  0.0446, -0.0734, -0.0603,  0.0846,  0.0543,  0.0252, -0.0769,\n",
      "        -0.0204,  0.0799, -0.0802,  0.0368, -0.0066, -0.0616,  0.0795, -0.0526,\n",
      "        -0.0111, -0.0228,  0.0306, -0.0008, -0.0778,  0.0316,  0.0716,  0.0224,\n",
      "        -0.0876,  0.0367,  0.0229, -0.0269, -0.0393,  0.0733,  0.0255, -0.0185,\n",
      "        -0.0505,  0.0459,  0.0385, -0.0433, -0.0645,  0.0259, -0.0609, -0.0851,\n",
      "        -0.0503, -0.0436,  0.0761, -0.0633,  0.0641, -0.0765,  0.0481, -0.0704,\n",
      "        -0.0274, -0.0394, -0.0587,  0.0776, -0.0058,  0.0561,  0.0697,  0.0773,\n",
      "         0.0417,  0.0109,  0.0494, -0.0482, -0.0139, -0.0394,  0.0012, -0.0090,\n",
      "         0.0634, -0.0168, -0.0845,  0.0256, -0.0622, -0.0706, -0.0735, -0.0660,\n",
      "        -0.0115, -0.0774, -0.0465,  0.0755,  0.0779, -0.0149,  0.0063, -0.0576,\n",
      "        -0.0630,  0.0779,  0.0503,  0.0847, -0.0205, -0.0792,  0.0766,  0.0583,\n",
      "        -0.0857, -0.0349, -0.0664,  0.0822,  0.0710, -0.0574, -0.0760,  0.0294,\n",
      "         0.0372,  0.0583, -0.0742,  0.0796,  0.0273,  0.0836, -0.0235,  0.0813,\n",
      "         0.0386, -0.0712,  0.0265, -0.0115, -0.0390, -0.0017, -0.0010,  0.0197,\n",
      "        -0.0536,  0.0818, -0.0213,  0.0752,  0.0134, -0.0306, -0.0549, -0.0612,\n",
      "        -0.0243,  0.0028, -0.0113,  0.0804,  0.0385,  0.0714,  0.0587,  0.0348,\n",
      "         0.0256,  0.0714, -0.0225,  0.0714,  0.0633,  0.0375,  0.0050, -0.0362,\n",
      "        -0.0178, -0.0675,  0.0255, -0.0533,  0.0099,  0.0787,  0.0660,  0.0329,\n",
      "         0.0217, -0.0726,  0.0556,  0.0386, -0.0085,  0.0055,  0.0407, -0.0092,\n",
      "        -0.0560, -0.0480,  0.0607,  0.0737, -0.0829,  0.0118,  0.0234, -0.0393,\n",
      "         0.0428,  0.0880,  0.0659,  0.0158,  0.0320, -0.0315,  0.0122,  0.0352,\n",
      "        -0.0188, -0.0568, -0.0647,  0.0550, -0.0015, -0.0666, -0.0022, -0.0341,\n",
      "         0.0085, -0.0473,  0.0518, -0.0679, -0.0673,  0.0869,  0.0493,  0.0507,\n",
      "         0.0035, -0.0180,  0.0748,  0.0188, -0.0437,  0.0563,  0.0241, -0.0721,\n",
      "         0.0436, -0.0098,  0.0318, -0.0083, -0.0286,  0.0254,  0.0737, -0.0036,\n",
      "        -0.0325,  0.0079,  0.0511, -0.0456, -0.0402,  0.0386, -0.0271,  0.0202,\n",
      "        -0.0622, -0.0523,  0.0157, -0.0734, -0.0299, -0.0597,  0.0814,  0.0687,\n",
      "         0.0063, -0.0507, -0.0663,  0.0846, -0.0273, -0.0561,  0.0743, -0.0259,\n",
      "        -0.0667,  0.0251,  0.0699, -0.0532, -0.0592, -0.0603,  0.0028,  0.0621,\n",
      "        -0.0762,  0.0344,  0.0308,  0.0224,  0.0094,  0.0327,  0.0101, -0.0530],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for x, y in model.named_parameters():\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
