{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from itertools import permutations, accumulate\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import operator\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# https://medium.com/deep-learning-hk/compute-document-similarity-using-autoencoder-with-triplet-loss-eb7eb132eb38\n",
    "# https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73\n",
    "# FaceNet: https://arxiv.org/pdf/1503.03832.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfCheck(df):\n",
    "    \n",
    "    # Any column is > 20% null values\n",
    "    if any(cnt / len(df) > 0.2 for cnt in df.isna().sum()):\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 41575 tables\n",
      "Total tables analyzed: 39393\n",
      "Maximum row length: 1017\n"
     ]
    }
   ],
   "source": [
    "tables = defaultdict(list)\n",
    "path = \"../Data/tables_025.csv\"\n",
    "maxRowLen = 0\n",
    "maxRowLenR = \"\"\n",
    "maxRowLenT = \"\"\n",
    "allRows = []\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    tableCount = 0\n",
    "    for line in f:\n",
    "        if line.startswith(\"List of\"):\n",
    "            tableCount += 1\n",
    "            \n",
    "tableFrac = 0.25\n",
    "tablesLim = int(tableCount * tableFrac)\n",
    "tablesRead = 0\n",
    "\n",
    "print(\"Reading \" + str(tablesLim) + \" tables\")\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    tableTitle = \"\"\n",
    "    table = []\n",
    "    \n",
    "    for row in reader:\n",
    "        if len(row) == 1 and row[0].startswith(\"List of\"):\n",
    "            if tableTitle: # Ignore first table, doesn't exist yet\n",
    "                if dfCheck(pd.DataFrame(table)):\n",
    "                    joinedRows = ['|||'.join(row) for row in table]\n",
    "                    tables[tableTitle].append(joinedRows)\n",
    "                    allRows.extend(joinedRows)\n",
    "                table = []\n",
    "            tableTitle = row[0]\n",
    "            \n",
    "            tablesRead += 1\n",
    "            if tablesRead > tablesLim:\n",
    "                break\n",
    "            \n",
    "        elif len(row) != 0:\n",
    "            table.append(row)\n",
    "            \n",
    "            \n",
    "print(\"Total tables analyzed: \" + str(sum(len(val) for val in tables.values())))\n",
    "maxRowLen = max(len(row) for row in allRows)\n",
    "print(\"Maximum row length: \" + str(maxRowLen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(allRows)\n",
    "id_count = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Convert each string into a list of its characters, converted to ints\n",
    "Filter out rows which are too long (i.e. not many other rows at that length)\n",
    "\"\"\"\n",
    "import json\n",
    "\n",
    "table_chars = []\n",
    "sizes = [len(seq) for seq in allRows]\n",
    "mu, sigma = np.mean(sizes), np.std(sizes)\n",
    "maxRowLen = int(mu)\n",
    "\n",
    "for i, category in enumerate(tables):\n",
    "    for i, table in enumerate(tables[category]):\n",
    "        tokens = tokenizer.texts_to_sequences(table)\n",
    "        char_table = np.array([np.pad(seq, (0, maxRowLen - len(seq))) for seq in tokens if len(seq) <= maxRowLen])\n",
    "\n",
    "        if len(char_table) >= 10:  # Remove tables that no longer have sufficient rows. We use 10 to allow for training & testing purposes   \n",
    "            table_chars.append(char_table)\n",
    "\n",
    "table_tensors = [torch.tensor(table) for table in table_chars]\n",
    "print(\"Total # of tables in dataset: \" + str(len(table_tensors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../Data/tables.json\", \"w\") as f:\n",
    "#     f.write(json.dumps([l.tolist() for l in table_chars]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "class TableDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tables, rows_per_table=50):\n",
    "        self.sizes = torch.tensor([table.shape[0] for table in tables])\n",
    "        self.table_positions = self.get_table_positions()\n",
    "        self.rows = torch.cat(tables, 0).float()\n",
    "        self.max = torch.max(self.rows)\n",
    "        self.rows = self.rows / self.max  # Normalization\n",
    "        self.rows_per_table = rows_per_table\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sizes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx == 0:\n",
    "            return self.rows[:self.sizes[0]]\n",
    "            \n",
    "        return self.rows[self.sizes[idx-1]:self.sizes[idx]]\n",
    "\n",
    "    def get_table_positions(self):\n",
    "        return np.array([0] + list(accumulate(self.sizes, operator.add)), dtype=np.int32)\n",
    "\n",
    "    def shuffle(self):\n",
    "        idxs = np.random.permutation(len(self.sizes))\n",
    "        tables = torch.split(self.rows, list(self.sizes))  # split_size_or_sections needs a list for some reason\n",
    "        tables = [tables[idx] for idx in idxs]\n",
    "        self.rows = torch.cat(tables, 0)\n",
    "        self.sizes = self.sizes[idxs]\n",
    "        self.table_positions = self.get_table_positions()\n",
    "        return\n",
    "\n",
    "    def get_batch(self, start_idx, batch_size):\n",
    "        \"\"\"\n",
    "        Using an online novel triplet mining strategy described in https://arxiv.org/pdf/1503.03832.pdf\n",
    "        \n",
    "        We perform all computations in numpy since it is much faster than with pytorch\n",
    "        \"\"\"\n",
    "        # t = time.time()\n",
    "        table_positions = self.table_positions[start_idx: start_idx+batch_size+1]\n",
    "\n",
    "        # Get 5 rows per table for a subset of the tables\n",
    "        size = (self.rows_per_table, min(batch_size+1, table_positions.shape[0]) - 1)\n",
    "        chosen_row_idxs = np.random.randint(table_positions[:-1], table_positions[1:], size=size).T.flatten()\n",
    "        chosen_rows = self.rows[chosen_row_idxs].float()\n",
    "\n",
    "\n",
    "        pos_mask = np.zeros((chosen_rows.shape[0], chosen_rows.shape[0]))\n",
    "        table_positions -= np.min(table_positions)\n",
    "        for start in range(0, chosen_rows.shape[0], self.rows_per_table):\n",
    "            pos_mask[start:start+self.rows_per_table,start:start+self.rows_per_table] = 1\n",
    "        \n",
    "        # print(\"Elapsed: \" + str(time.time() - t), len(chosen_rows), len(anchor_idxs))\n",
    "        return (\n",
    "            chosen_rows.to(device), \n",
    "            torch.tensor(pos_mask).float().to(device)\n",
    "        )\n",
    "\n",
    "    def get_average_precision(self, model, first_tables=20):\n",
    "        with torch.no_grad():\n",
    "            embs = np.array(model(self.rows.long().to(device)).cpu())  # Our model uses small => better, avg_precision uses opposite\n",
    "        precs = []\n",
    "\n",
    "        dists = np.matmul(embs, embs.T)\n",
    "        pos_mask = np.zeros(dists.shape, dtype=bool)\n",
    "        \n",
    "        for table_start, table_end in zip(self.table_positions[:first_tables], self.table_positions[1:first_tables]):\n",
    "            pos_mask[table_start:table_end,table_start:table_end] = 1\n",
    "\n",
    "        for mask, scores in zip(pos_mask, dists):\n",
    "            # print(mask, scores)\n",
    "            avg_prec_score = average_precision_score(mask, scores)\n",
    "            precs.append(avg_prec_score)\n",
    "\n",
    "        return precs\n",
    "\n",
    "table_tensors_train = []\n",
    "table_tensors_val = []\n",
    "table_tensors_test = []\n",
    "take_tables = 20\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Generate training, validation, and testing datasets\n",
    "for table in table_tensors[:take_tables]:\n",
    "    # Select 5 random rows to be validation / test rows\n",
    "    non_train_idxs = torch.randint(0, table.shape[0], size=(1, 5)).flatten()\n",
    "    val_idxs, test_idxs = non_train_idxs[:3], non_train_idxs[3:]\n",
    "\n",
    "    # All other rows are training rows\n",
    "    train_idxs = torch.ones(table.shape[0], dtype=bool)\n",
    "    train_idxs[non_train_idxs] = False\n",
    "\n",
    "    # append the data to appropriate list\n",
    "    table_tensors_train.append(table[train_idxs])\n",
    "    table_tensors_val.append(table[val_idxs])\n",
    "    table_tensors_test.append(table[test_idxs])\n",
    "\n",
    "dataset_train = TableDataset(table_tensors_train)\n",
    "dataset_val = TableDataset(table_tensors_val)\n",
    "dataset_test = TableDataset(table_tensors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model based on https://towardsdatascience.com/a-friendly-introduction-to-siamese-networks-85ab17522942\n",
    "\"\"\"\n",
    "\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, emb_dim, h_activ=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Defining the fully connected layers\n",
    "        self.cnn1 = nn.Sequential(\n",
    "            nn.Conv1d(1, 24, kernel_size=11,stride=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LocalResponseNorm(5,alpha=0.0001,beta=0.75,k=2),\n",
    "            nn.MaxPool1d(3, stride=2),\n",
    "            \n",
    "            nn.Conv1d(24, 96, kernel_size=5,stride=1,padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LocalResponseNorm(5,alpha=0.0001,beta=0.75,k=2),\n",
    "            nn.MaxPool1d(3, stride=2),\n",
    "            nn.Dropout(p=0.3),\n",
    "\n",
    "            nn.Conv1d(96,128 , kernel_size=3,stride=1,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.LocalResponseNorm(5,alpha=0.0001,beta=0.75,k=2),\n",
    "            nn.MaxPool1d(3, stride=2),\n",
    "            nn.Dropout(p=0.3),\n",
    "\n",
    "            nn.Conv1d(128, 196, kernel_size=1,stride=1,padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Defining the fully connected layers\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(1176, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward pass \n",
    "        x = x.unsqueeze(1)\n",
    "        output = self.cnn1(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc1(output)\n",
    "        return F.normalize(output, p=2, dim=-1)\n",
    "        \n",
    "\n",
    "class APLoss (nn.Module):\n",
    "    \"\"\" Differentiable AP loss, through quantization. From the paper:\n",
    "        Learning with Average Precision: Training Image Retrieval with a Listwise Loss\n",
    "        Jerome Revaud, Jon Almazan, Rafael Sampaio de Rezende, Cesar de Souza\n",
    "        https://arxiv.org/abs/1906.07589\n",
    "        Input: (N, M)   values in [min, max]\n",
    "        label: (N, M)   values in {0, 1}\n",
    "        Returns: 1 - mAP (mean AP for each n in {1..N})\n",
    "                 Note: typically, this is what you wanna minimize\n",
    "    \"\"\"\n",
    "    def __init__(self, nq=5, min=0, max=1):\n",
    "        nn.Module.__init__(self)\n",
    "        assert isinstance(nq, int) and 2 <= nq <= 100\n",
    "        self.nq = nq\n",
    "        self.min = min\n",
    "        self.max = max\n",
    "        gap = max - min\n",
    "        assert gap > 0\n",
    "        # Initialize quantizer as non-trainable convolution\n",
    "        self.quantizer = q = nn.Conv1d(1, 2*nq, kernel_size=1, bias=True)\n",
    "        q.weight = nn.Parameter(q.weight.detach(), requires_grad=False)\n",
    "        q.bias = nn.Parameter(q.bias.detach(), requires_grad=False)\n",
    "        a = (nq-1) / gap\n",
    "        # First half equal to lines passing to (min+x,1) and (min+x+1/a,0) with x = {nq-1..0}*gap/(nq-1)\n",
    "        q.weight[:nq] = -a\n",
    "        q.bias[:nq] = torch.from_numpy(a*min + np.arange(nq, 0, -1))  # b = 1 + a*(min+x)\n",
    "        # First half equal to lines passing to (min+x,1) and (min+x-1/a,0) with x = {nq-1..0}*gap/(nq-1)\n",
    "        q.weight[nq:] = a\n",
    "        q.bias[nq:] = torch.from_numpy(np.arange(2-nq, 2, 1) - a*min)  # b = 1 - a*(min+x)\n",
    "        # First and last one as a horizontal straight line\n",
    "        q.weight[0] = q.weight[-1] = 0\n",
    "        q.bias[0] = q.bias[-1] = 1\n",
    "\n",
    "    def forward(self, sims, label, qw=None, ret='1-mAP'):\n",
    "        x = torch.matmul(sims, sims.T).T\n",
    "        assert x.shape == label.shape  # N x M\n",
    "        N, M = x.shape\n",
    "        # Quantize all predictions\n",
    "        q = self.quantizer(x.unsqueeze(1))\n",
    "        q = torch.min(q[:, :self.nq], q[:, self.nq:]).clamp(min=0)  # N x Q x M\n",
    "\n",
    "        nbs = q.sum(dim=-1)  # number of samples  N x Q = c\n",
    "        rec = (q * label.view(N, 1, M).float()).sum(dim=-1)  # number of correct samples = c+ N x Q\n",
    "        prec = rec.cumsum(dim=-1) / (1e-16 + nbs.cumsum(dim=-1))  # precision\n",
    "        rec /= rec.sum(dim=-1).unsqueeze(1)  # norm in [0,1]\n",
    "\n",
    "        ap = (prec * rec).sum(dim=-1)  # per-image AP\n",
    "\n",
    "        if ret == '1-mAP':\n",
    "            if qw is not None:\n",
    "                ap *= qw  # query weights\n",
    "            return 1 - ap.mean()\n",
    "        elif ret == 'AP':\n",
    "            assert qw is None\n",
    "            return ap\n",
    "        else:\n",
    "            raise ValueError(\"Bad return type for APLoss(): %s\" % str(ret))\n",
    "\n",
    "    def measures(self, x, gt, loss=None):\n",
    "        if loss is None:\n",
    "            loss = self.forward(x, gt)\n",
    "        return {'loss_ap': float(loss)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH 0: 100%|██████████| 2/2 [00:00<00:00,  2.32it/s]\n",
      "EPOCH 1: 100%|██████████| 2/2 [00:00<00:00,  2.30it/s]\n",
      "EPOCH 2: 100%|██████████| 2/2 [00:00<00:00,  2.35it/s]\n",
      "EPOCH 3: 100%|██████████| 2/2 [00:00<00:00,  2.21it/s]\n",
      "EPOCH 4: 100%|██████████| 2/2 [00:00<00:00,  2.07it/s]\n",
      "EPOCH 5: 100%|██████████| 2/2 [00:00<00:00,  2.15it/s]\n",
      "EPOCH 6: 100%|██████████| 2/2 [00:01<00:00,  1.99it/s]\n",
      "EPOCH 7:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7856/3672901061.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mdataset_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mtrain_losses\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# if epoch % 100 == 0:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7856/3672901061.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, loss_fn, data, epoch, BATCH_SIZE)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mbatch_embs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_embs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# back-propagation, could do manually\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Adjusts the weights for us\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\pahas\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\pahas\\anaconda3\\envs\\ml\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(model, optimizer, loss_fn, data, epoch, BATCH_SIZE=10):\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for i in tqdm(range(0, len(data), BATCH_SIZE), desc=\"EPOCH %s\" % (epoch,)):\n",
    "        batch, mask = data.get_batch(model, i, BATCH_SIZE)\n",
    "\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "        batch_embs = model(batch)    \n",
    "        loss = loss_fn(batch_embs, mask)\n",
    "        loss.backward() # back-propagation, could do manually\n",
    "        optimizer.step() # Adjusts the weights for us\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return losses\n",
    "\n",
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        precision_scores = data.get_average_precision(model)\n",
    "\n",
    "    return precision_scores\n",
    "\n",
    "EPOCHS = 1000\n",
    "model = LinearModel(256).to(device)\n",
    "loss_fn = APLoss().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-1, weight_decay=0.0005)\n",
    "train_losses = []\n",
    "precision_scores = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    dataset_train.shuffle()\n",
    "    train_losses += train(model, optimizer, loss_fn, dataset_train, epoch)\n",
    "\n",
    "    # if epoch % 100 == 0:\n",
    "        # dataset_val.shuffle()\n",
    "        # prec = evaluate(model, dataset_val)\n",
    "        # precision_scores.extend(prec)\n",
    "        # print(\"Average Precision at epoch %s: %s\" % (epoch, np.mean(prec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2529, 0.0510, 0.0000,  ..., 0.0981, 0.0309, 0.0000],\n",
       "        [0.2529, 0.0510, 0.0000,  ..., 0.0981, 0.0309, 0.0000],\n",
       "        [0.2529, 0.0509, 0.0000,  ..., 0.0981, 0.0309, 0.0000],\n",
       "        ...,\n",
       "        [0.2529, 0.0511, 0.0000,  ..., 0.0981, 0.0309, 0.0000],\n",
       "        [0.2529, 0.0510, 0.0000,  ..., 0.0980, 0.0309, 0.0000],\n",
       "        [0.2529, 0.0510, 0.0000,  ..., 0.0981, 0.0309, 0.0000]],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Test to make sure that a single pass over 'get_triplets' works as intended\n",
    "\"\"\"\n",
    "batch, mask = dataset_val.get_batch(model, 0, 2)\n",
    "model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8999999761581421"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)\n",
    "train_losses[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, mask = dataset_train.get_batch(model, 0, 25)\n",
    "embs = model(batch) \n",
    "l = loss_fn(embs, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.0.weight Parameter containing:\n",
      "tensor([[ 0.0988,  0.0213,  0.0840,  ...,  0.0461,  0.0407,  0.0945],\n",
      "        [-0.0587, -0.0775,  0.0603,  ...,  0.0918,  0.1036, -0.0902],\n",
      "        [ 0.0286, -0.0489, -0.0125,  ..., -0.0294,  0.0470, -0.0433],\n",
      "        ...,\n",
      "        [ 0.0870,  0.0244, -0.0646,  ...,  0.0622, -0.0769, -0.0935],\n",
      "        [-0.0344,  0.0914, -0.0333,  ..., -0.0046, -0.0903,  0.0679],\n",
      "        [ 0.0052, -0.0500, -0.1000,  ..., -0.0004,  0.0654,  0.0462]],\n",
      "       requires_grad=True)\n",
      "fc1.0.bias Parameter containing:\n",
      "tensor([-0.1088,  0.0772, -0.0111, -0.0768,  0.0766, -0.0921,  0.0533, -0.0935,\n",
      "        -0.0326, -0.1067,  0.0813,  0.0281,  0.0928,  0.0347, -0.0564,  0.0171,\n",
      "         0.0769, -0.0879, -0.0758, -0.0289, -0.0710, -0.0207,  0.1077,  0.0632,\n",
      "        -0.0922, -0.0613,  0.0912, -0.0087, -0.0479,  0.0222, -0.0181,  0.0383,\n",
      "         0.0021,  0.0646,  0.0626,  0.0399,  0.0352, -0.0292, -0.1052, -0.0109,\n",
      "        -0.0863, -0.0909, -0.1001, -0.0284, -0.0841,  0.0192,  0.0491,  0.0675,\n",
      "        -0.1097,  0.0957,  0.0338,  0.0923,  0.0551,  0.0207, -0.0218,  0.0951,\n",
      "        -0.0740,  0.0887,  0.0992,  0.0186, -0.0488, -0.0918, -0.0062,  0.0966,\n",
      "         0.0053, -0.0286, -0.0565,  0.0085,  0.0629,  0.0221,  0.0196, -0.0787,\n",
      "         0.0550,  0.0118, -0.1117,  0.0946,  0.0723,  0.0009, -0.0225,  0.1021,\n",
      "         0.1129,  0.0185,  0.1055,  0.0481,  0.0543, -0.0067, -0.0083,  0.0727,\n",
      "        -0.0498, -0.0812,  0.0710, -0.0273,  0.0803, -0.0189, -0.0693, -0.0019,\n",
      "        -0.0131,  0.0261, -0.0877, -0.0963,  0.0059,  0.1026, -0.0719, -0.0572,\n",
      "         0.0109,  0.0851,  0.0871, -0.0237, -0.0060, -0.0895,  0.1110,  0.0691,\n",
      "         0.0291, -0.1104,  0.0643, -0.0641,  0.0047,  0.0546, -0.0045, -0.0555,\n",
      "         0.0768,  0.0771,  0.0111, -0.0711, -0.0589, -0.1021, -0.0609, -0.1016],\n",
      "       requires_grad=True)\n",
      "fc1.3.weight Parameter containing:\n",
      "tensor([[-0.0074, -0.0597,  0.0479,  ..., -0.0451, -0.0066, -0.0070],\n",
      "        [ 0.0685,  0.0794,  0.0212,  ..., -0.0028, -0.0702, -0.0581],\n",
      "        [-0.0635,  0.0080,  0.0615,  ..., -0.0813,  0.0827, -0.0787],\n",
      "        ...,\n",
      "        [-0.0085,  0.0489,  0.0042,  ...,  0.0381,  0.0560,  0.0798],\n",
      "        [-0.0254,  0.0449,  0.0498,  ...,  0.0216, -0.0247,  0.0116],\n",
      "        [-0.0157, -0.0689,  0.0571,  ..., -0.0097,  0.0177,  0.0216]],\n",
      "       requires_grad=True)\n",
      "fc1.3.bias Parameter containing:\n",
      "tensor([-0.0544, -0.0475,  0.0697, -0.0361,  0.0438,  0.0792, -0.0764,  0.0137,\n",
      "         0.0211, -0.0522, -0.0210, -0.0401, -0.0834, -0.0292,  0.0769,  0.0799,\n",
      "        -0.0495, -0.0268, -0.0436,  0.0565, -0.0690, -0.0286, -0.0269, -0.0339,\n",
      "         0.0721, -0.0731, -0.0666, -0.0693, -0.0380,  0.0457,  0.0116, -0.0038,\n",
      "         0.0658,  0.0579,  0.0196,  0.0160, -0.0336, -0.0812,  0.0149,  0.0723,\n",
      "         0.0611, -0.0078, -0.0023,  0.0812, -0.0840,  0.0738,  0.0822,  0.0530,\n",
      "         0.0336,  0.0035,  0.0292, -0.0436, -0.0128, -0.0540,  0.0102, -0.0225,\n",
      "         0.0750,  0.0308,  0.0390, -0.0560, -0.0010,  0.0737,  0.0344, -0.0734,\n",
      "        -0.0347,  0.0483,  0.0448,  0.0118, -0.0229,  0.0491,  0.0233,  0.0085,\n",
      "         0.0259, -0.0482, -0.0582,  0.0543, -0.0459, -0.0611, -0.0680,  0.0666,\n",
      "         0.0344,  0.0364,  0.0606, -0.0790, -0.0680,  0.0319,  0.0726,  0.0379,\n",
      "         0.0078,  0.0817, -0.0368,  0.0723,  0.0687, -0.0655, -0.0074,  0.0698,\n",
      "         0.0779, -0.0167,  0.0736, -0.0682,  0.0228,  0.0151,  0.0240,  0.0455,\n",
      "         0.0145,  0.0432,  0.0661,  0.0477,  0.0538,  0.0837,  0.0659,  0.0051,\n",
      "        -0.0590,  0.0366,  0.0085,  0.0768, -0.0242,  0.0027, -0.0785, -0.0403,\n",
      "         0.0605, -0.0544, -0.0431,  0.0035,  0.0039,  0.0601, -0.0549,  0.0152],\n",
      "       requires_grad=True)\n",
      "fc1.6.weight Parameter containing:\n",
      "tensor([[-0.0527,  0.0420,  0.0322,  ..., -0.0833,  0.0410, -0.0634],\n",
      "        [-0.0349,  0.0722, -0.0074,  ...,  0.0042,  0.0223,  0.0258],\n",
      "        [ 0.0254,  0.0691, -0.0109,  ...,  0.0437, -0.0194,  0.0289],\n",
      "        ...,\n",
      "        [ 0.0241,  0.0184, -0.0145,  ...,  0.0291, -0.0563,  0.0540],\n",
      "        [ 0.0521,  0.0810,  0.0663,  ..., -0.0698, -0.0404,  0.0441],\n",
      "        [ 0.0209, -0.0154,  0.0626,  ...,  0.0309,  0.0563, -0.0159]],\n",
      "       requires_grad=True)\n",
      "fc1.6.bias Parameter containing:\n",
      "tensor([-0.0341,  0.0076, -0.0424, -0.0500,  0.0438, -0.0679,  0.0438,  0.0323,\n",
      "        -0.0550,  0.0553,  0.0796, -0.0490,  0.0535,  0.0542, -0.0760, -0.0784,\n",
      "        -0.0517,  0.0572, -0.0076,  0.0656, -0.0609, -0.0310, -0.0791,  0.0740,\n",
      "        -0.0711,  0.0597, -0.0613, -0.0068, -0.0602, -0.0042, -0.0267,  0.0625,\n",
      "        -0.0143,  0.0648, -0.0367,  0.0408,  0.0516, -0.0395, -0.0096,  0.0808,\n",
      "        -0.0161,  0.0269,  0.0498, -0.0775,  0.0192, -0.0097, -0.0324, -0.0712,\n",
      "         0.0120,  0.0706, -0.0509,  0.0049, -0.0797,  0.0551,  0.0226,  0.0567,\n",
      "        -0.0052, -0.0130, -0.0178,  0.0010,  0.0309,  0.0607, -0.0835,  0.0683,\n",
      "         0.0172, -0.0065, -0.0654, -0.0277, -0.0699, -0.0225,  0.0253, -0.0727,\n",
      "         0.0487,  0.0830,  0.0400,  0.0373,  0.0688, -0.0330, -0.0637,  0.0758,\n",
      "        -0.0592, -0.0807, -0.0207, -0.0791,  0.0489,  0.0059,  0.0814, -0.0336,\n",
      "        -0.0692, -0.0425, -0.0034,  0.0591,  0.0798,  0.0748,  0.0166,  0.0732,\n",
      "         0.0094, -0.0612,  0.0728,  0.0328,  0.0570,  0.0731,  0.0688, -0.0644,\n",
      "         0.0815, -0.0394,  0.0347, -0.0680,  0.0517, -0.0077,  0.0532,  0.0769,\n",
      "        -0.0151,  0.0219, -0.0174,  0.0254, -0.0763,  0.0167,  0.0483, -0.0661,\n",
      "        -0.0236, -0.0098,  0.0228,  0.0129,  0.0629, -0.0411, -0.0582,  0.0338],\n",
      "       requires_grad=True)\n",
      "fc1.9.weight Parameter containing:\n",
      "tensor([[-1.2469e-02,  3.0454e-02, -2.4487e-02,  ...,  7.9671e-02,\n",
      "         -6.4227e-02, -2.4629e-03],\n",
      "        [ 3.5144e-02,  3.4252e-02,  2.8721e-02,  ..., -9.9858e-03,\n",
      "         -7.5715e-02,  2.9530e-02],\n",
      "        [-8.2541e-02,  2.3101e-02, -7.4491e-04,  ..., -3.7491e-06,\n",
      "          9.6458e-03, -4.2343e-02],\n",
      "        ...,\n",
      "        [-1.9341e-02,  7.5610e-02, -3.8732e-02,  ..., -7.9582e-04,\n",
      "          2.1029e-02,  5.7184e-02],\n",
      "        [-3.7067e-02, -6.9880e-02, -5.0714e-02,  ...,  3.3915e-02,\n",
      "         -5.5878e-02, -6.1890e-02],\n",
      "        [ 7.9421e-02, -2.5683e-02, -5.4958e-02,  ...,  3.1078e-02,\n",
      "          7.2092e-02,  6.7583e-02]], requires_grad=True)\n",
      "fc1.9.bias Parameter containing:\n",
      "tensor([-0.0824,  0.0691, -0.0806, -0.0755, -0.0828,  0.0248, -0.0669,  0.0258,\n",
      "        -0.0329, -0.0207,  0.0341,  0.0635,  0.0095,  0.0500, -0.0176,  0.0621,\n",
      "         0.0526,  0.0784, -0.0632, -0.0826, -0.0565, -0.0488, -0.0470, -0.0537,\n",
      "         0.0138, -0.0196,  0.0798,  0.0312, -0.0230,  0.0793, -0.0400,  0.0501,\n",
      "         0.0316,  0.0711,  0.0346,  0.0004,  0.0629, -0.0241,  0.0778, -0.0746,\n",
      "        -0.0138,  0.0592,  0.0730, -0.0825,  0.0420, -0.0620,  0.0809,  0.0585,\n",
      "         0.0155, -0.0194, -0.0112, -0.0333,  0.0620,  0.0175, -0.0270,  0.0488,\n",
      "        -0.0210,  0.0034,  0.0740,  0.0266, -0.0372, -0.0213, -0.0051, -0.0255,\n",
      "        -0.0498,  0.0742, -0.0565,  0.0584,  0.0398, -0.0257, -0.0367, -0.0592,\n",
      "         0.0222, -0.0500,  0.0037, -0.0427, -0.0558,  0.0489, -0.0237, -0.0690,\n",
      "        -0.0728, -0.0422,  0.0148,  0.0037,  0.0799,  0.0239, -0.0631,  0.0396,\n",
      "         0.0010, -0.0638, -0.0376, -0.0266, -0.0267, -0.0046,  0.0695, -0.0342,\n",
      "         0.0689,  0.0683, -0.0071, -0.0619, -0.0622, -0.0097,  0.0770,  0.0221,\n",
      "        -0.0001, -0.0653, -0.0682, -0.0832,  0.0257,  0.0621,  0.0106, -0.0828,\n",
      "        -0.0233,  0.0668, -0.0813,  0.0618, -0.0732, -0.0625,  0.0074, -0.0456,\n",
      "         0.0179,  0.0116, -0.0163, -0.0700, -0.0452, -0.0392,  0.0330, -0.0689],\n",
      "       requires_grad=True)\n",
      "fc1.12.weight Parameter containing:\n",
      "tensor([[-0.0254,  0.0279, -0.0375,  ..., -0.0725, -0.0508, -0.0030],\n",
      "        [ 0.0754,  0.0261, -0.0087,  ...,  0.0605,  0.0340, -0.0666],\n",
      "        [-0.0388,  0.0056,  0.0231,  ...,  0.0678,  0.0085, -0.0150],\n",
      "        ...,\n",
      "        [ 0.0589,  0.0525, -0.0838,  ...,  0.0594,  0.0282, -0.0629],\n",
      "        [ 0.0055,  0.0275,  0.0467,  ..., -0.0415,  0.0654, -0.0063],\n",
      "        [-0.0481,  0.0618,  0.0542,  ...,  0.0397, -0.0007,  0.0801]],\n",
      "       requires_grad=True)\n",
      "fc1.12.bias Parameter containing:\n",
      "tensor([ 0.0841,  0.0114, -0.0836,  0.0770,  0.0804,  0.0212,  0.0376, -0.0447,\n",
      "         0.0757,  0.0403,  0.0749, -0.0795,  0.0316, -0.0159,  0.0506,  0.0039,\n",
      "        -0.0389,  0.0286, -0.0477,  0.0721,  0.0746, -0.0346,  0.0180,  0.0623,\n",
      "        -0.0294,  0.0499,  0.0010, -0.0359,  0.0055,  0.0815,  0.0013, -0.0833,\n",
      "        -0.0417,  0.0449,  0.0675, -0.0031,  0.0281, -0.0069, -0.0013, -0.0314,\n",
      "         0.0677, -0.0375, -0.0265, -0.0287,  0.0011,  0.0265,  0.0697, -0.0364,\n",
      "        -0.0371, -0.0768, -0.0578, -0.0683, -0.0343, -0.0687,  0.0221, -0.0059,\n",
      "         0.0205, -0.0505,  0.0354, -0.0766,  0.0177,  0.0772,  0.0626, -0.0094,\n",
      "        -0.0051,  0.0659, -0.0266, -0.0809, -0.0777,  0.0648,  0.0043,  0.0453,\n",
      "         0.0463,  0.0332,  0.0753, -0.0402,  0.0569, -0.0593,  0.0702,  0.0173,\n",
      "        -0.0566,  0.0592,  0.0782,  0.0184,  0.0441, -0.0423, -0.0023,  0.0671,\n",
      "        -0.0828, -0.0171, -0.0219, -0.0581,  0.0777,  0.0282, -0.0208,  0.0400,\n",
      "         0.0102,  0.0259,  0.0505,  0.0341,  0.0027, -0.0592,  0.0776,  0.0332,\n",
      "         0.0090, -0.0534, -0.0825, -0.0160, -0.0680,  0.0285,  0.0147,  0.0503,\n",
      "         0.0036,  0.0132,  0.0291,  0.0532,  0.0438, -0.0419, -0.0112,  0.0366,\n",
      "        -0.0162, -0.0562, -0.0798,  0.0101, -0.0014, -0.0741, -0.0575,  0.0071,\n",
      "         0.0270,  0.0497,  0.0547,  0.0614,  0.0449,  0.0378, -0.0746, -0.0101,\n",
      "        -0.0516, -0.0722, -0.0068, -0.0169, -0.0484,  0.0495,  0.0511,  0.0529,\n",
      "        -0.0723, -0.0139,  0.0294,  0.0094, -0.0398, -0.0802, -0.0047,  0.0199,\n",
      "         0.0037, -0.0803,  0.0277,  0.0179, -0.0410, -0.0221, -0.0041, -0.0372,\n",
      "         0.0052,  0.0473,  0.0722, -0.0694, -0.0720,  0.0285, -0.0009,  0.0214,\n",
      "        -0.0279,  0.0611,  0.0639, -0.0265, -0.0590, -0.0724,  0.0261,  0.0268,\n",
      "         0.0631,  0.0645, -0.0465, -0.0259,  0.0216, -0.0519,  0.0050,  0.0707,\n",
      "         0.0347,  0.0446, -0.0558, -0.0286, -0.0188,  0.0006, -0.0529, -0.0809,\n",
      "         0.0155, -0.0574, -0.0790,  0.0050,  0.0486, -0.0279,  0.0465, -0.0681,\n",
      "        -0.0295,  0.0027, -0.0441, -0.0398,  0.0603, -0.0468, -0.0080,  0.0713,\n",
      "        -0.0168,  0.0827, -0.0304,  0.0404, -0.0228,  0.0282, -0.0764, -0.0624,\n",
      "         0.0172,  0.0724, -0.0097, -0.0364, -0.0365,  0.0674, -0.0785, -0.0395,\n",
      "        -0.0326,  0.0609, -0.0051, -0.0184, -0.0507, -0.0388,  0.0834, -0.0275,\n",
      "         0.0709,  0.0592, -0.0577, -0.0583,  0.0659, -0.0316,  0.0307, -0.0137,\n",
      "         0.0626,  0.0815, -0.0073,  0.0735,  0.0316, -0.0047,  0.0612, -0.0644,\n",
      "        -0.0533, -0.0406,  0.0540, -0.0401, -0.0472,  0.0782,  0.0624, -0.0491],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for x, y in model.named_parameters():\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class APLoss(nn.Module):\n",
    "#     \"\"\"\n",
    "#     https://openaccess.thecvf.com/content_ICCV_2019/papers/Revaud_Learning_With_Average_Precision_Training_Image_Retrieval_With_a_Listwise_ICCV_2019_paper.pdf\n",
    "#     \"\"\"\n",
    "#     def __init__(self, M):\n",
    "#         super(APLoss,self).__init__()\n",
    "#         self.M = torch.tensor(M)\n",
    "#         self.delta = torch.tensor(2 / (M - 1))\n",
    "#         self.bin_centers = torch.tensor([1 - (m - 1)*self.delta for m in range(1, M+1)])\n",
    "\n",
    "#     def forward(self, s, y):\n",
    "\n",
    "#         sims = torch.matmul(s, s.T)\n",
    "#         APs = torch.tensor([self.AP(sim.T, truth) for sim, truth in zip(sims, y)])\n",
    "#         mAP = torch.sum(APs)\n",
    "\n",
    "#         return 1 - mAP / s.shape[0]\n",
    "\n",
    "#     def AP(self, sims, truth):\n",
    "#         precisions = torch.tensor([self.quantized_precision(sims, truth, m_prime) for m_prime in range(1, self.M+1)])\n",
    "#         recalls = torch.tensor([self.incremental_recall(sims, truth, m_prime) for m_prime in range(1, self.M+1)])\n",
    "#         return torch.sum(precisions * recalls)\n",
    "\n",
    "#     def quantized_precision(self, S_q, Y_q, m):\n",
    "#         numerator = 0\n",
    "#         denominator = 0\n",
    "#         for m_prime in range(1, m+1):\n",
    "#             sigma_res = self.sigma(S_q, m_prime)\n",
    "#             numerator += torch.matmul(sigma_res.T, Y_q)\n",
    "#             denominator += torch.sum(sigma_res)\n",
    "        \n",
    "#         return numerator / denominator\n",
    "\n",
    "#     def incremental_recall(self, S_q, Y_q, m):\n",
    "#         N_q = torch.count_nonzero(Y_q)  # Not sure this is correct\n",
    "#         return torch.matmul(self.sigma(S_q, m).T, Y_q) / N_q\n",
    "\n",
    "#     def sigma(self, x, m):\n",
    "#         return torch.relu(1 - torch.abs((x - self.bin_centers[m-1]) / self.delta))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
