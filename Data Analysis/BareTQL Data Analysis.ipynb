{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from itertools import permutations, accumulate\n",
    "import time\n",
    "import operator\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# https://medium.com/deep-learning-hk/compute-document-similarity-using-autoencoder-with-triplet-loss-eb7eb132eb38\n",
    "# https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73\n",
    "# FaceNet: https://arxiv.org/pdf/1503.03832.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfCheck(df):\n",
    "    \n",
    "    # Any column is > 20% null values\n",
    "    if any(cnt / len(df) > 0.2 for cnt in df.isna().sum()):\n",
    "        return False\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 41575 tables\n",
      "Total tables analyzed: 39393\n",
      "Maximum row length: 1017\n"
     ]
    }
   ],
   "source": [
    "tables = defaultdict(list)\n",
    "path = \"../Data/tables_025.csv\"\n",
    "maxRowLen = 0\n",
    "maxRowLenR = \"\"\n",
    "maxRowLenT = \"\"\n",
    "allRows = []\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    tableCount = 0\n",
    "    for line in f:\n",
    "        if line.startswith(\"List of\"):\n",
    "            tableCount += 1\n",
    "            \n",
    "tableFrac = 0.25\n",
    "tablesLim = int(tableCount * tableFrac)\n",
    "tablesRead = 0\n",
    "\n",
    "print(\"Reading \" + str(tablesLim) + \" tables\")\n",
    "\n",
    "with open(path, \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    tableTitle = \"\"\n",
    "    table = []\n",
    "    \n",
    "    for row in reader:\n",
    "        if len(row) == 1 and row[0].startswith(\"List of\"):\n",
    "            if tableTitle: # Ignore first table, doesn't exist yet\n",
    "                if dfCheck(pd.DataFrame(table)):\n",
    "                    joinedRows = ['|||'.join(row) for row in table]\n",
    "                    tables[tableTitle].append(joinedRows)\n",
    "                    allRows.extend(joinedRows)\n",
    "                table = []\n",
    "            tableTitle = row[0]\n",
    "            \n",
    "            tablesRead += 1\n",
    "            if tablesRead > tablesLim:\n",
    "                break\n",
    "            \n",
    "        elif len(row) != 0:\n",
    "            table.append(row)\n",
    "            \n",
    "            \n",
    "print(\"Total tables analyzed: \" + str(sum(len(val) for val in tables.values())))\n",
    "maxRowLen = max(len(row) for row in allRows)\n",
    "print(\"Maximum row length: \" + str(maxRowLen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(allRows)\n",
    "id_count = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of tables in dataset: 15762\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Convert each string into a list of its characters, converted to ints\n",
    "Filter out rows which are too long (i.e. not many other rows at that length)\n",
    "\"\"\"\n",
    "import json\n",
    "\n",
    "table_chars = []\n",
    "sizes = [len(seq) for seq in allRows]\n",
    "mu, sigma = np.mean(sizes), np.std(sizes)\n",
    "maxRowLen = int(mu)\n",
    "\n",
    "for i, category in enumerate(tables):\n",
    "    for i, table in enumerate(tables[category]):\n",
    "        tokens = tokenizer.texts_to_sequences(table)\n",
    "        char_table = np.array([np.pad(seq, (0, maxRowLen - len(seq))) for seq in tokens if len(seq) <= maxRowLen])\n",
    "\n",
    "        if len(char_table) >= 10:  # Remove tables that no longer have sufficient rows. We use 10 to allow for training & testing purposes   \n",
    "            table_chars.append(char_table)\n",
    "\n",
    "table_tensors = [torch.tensor(table) for table in table_chars]\n",
    "print(\"Total # of tables in dataset: \" + str(len(table_tensors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../Data/tables.json\", \"w\") as f:\n",
    "#     f.write(json.dumps([l.tolist() for l in table_chars]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model based on https://towardsdatascience.com/a-friendly-introduction-to-siamese-networks-85ab17522942\n",
    "\"\"\"\n",
    "\n",
    "class TripletModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, emb_dim, h_activ=nn.ReLU()):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Defining the fully connected layers\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(maxRowLen, 128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(128, emb_dim)\n",
    "          )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc1(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken, d_model, nhead, d_hid, nlayers, dropout = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: Tensor, shape [seq_len, batch_size]\n",
    "            src_mask: Tensor, shape [seq_len, seq_len]\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
    "        \"\"\"\n",
    "        src = src.T\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src)\n",
    "        output = torch.sum(output, axis=0)\n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout = 0.1, max_len = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "class TableDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tables):\n",
    "        self.sizes = torch.tensor([table.shape[0] for table in tables])\n",
    "        self.table_positions = self.get_table_positions()\n",
    "        self.rows = torch.cat(tables, 0)\n",
    "        self.max = torch.max(self.rows)\n",
    "        self.rows = self.rows / self.max  # Normalization\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sizes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx == 0:\n",
    "            return self.rows[:self.sizes[0]]\n",
    "            \n",
    "        return self.rows[self.sizes[idx-1]:self.sizes[idx]]\n",
    "\n",
    "    def get_table_positions(self):\n",
    "        return np.array([0] + list(accumulate(self.sizes, operator.add)), dtype=np.int32)\n",
    "\n",
    "    def shuffle(self):\n",
    "        idxs = np.random.permutation(len(self.sizes))\n",
    "        tables = torch.split(self.rows, list(self.sizes))  # split_size_or_sections needs a list for some reason\n",
    "        tables = [tables[idx] for idx in idxs]\n",
    "        self.rows = torch.cat(tables, 0)\n",
    "        self.sizes = self.sizes[idxs]\n",
    "        self.table_positions = self.get_table_positions()\n",
    "        return\n",
    "\n",
    "    def get_triplets(self, model, start_idx, batch_size, rows_per_table=3):\n",
    "        \"\"\"\n",
    "        Using an online novel triplet mining strategy described in https://arxiv.org/pdf/1503.03832.pdf\n",
    "        \n",
    "        We perform all computations in numpy since it is much faster than with pytorch\n",
    "        \"\"\"\n",
    "        # t = time.time()\n",
    "        table_positions = self.table_positions[start_idx: start_idx+batch_size+1]\n",
    "\n",
    "        # Get 5 rows per table for a subset of the tables\n",
    "        size = (rows_per_table, min(batch_size+1, table_positions.shape[0]) - 1)\n",
    "        chosen_row_idxs = np.random.randint(table_positions[:-1], table_positions[1:], size=size).T.flatten()\n",
    "        chosen_rows = self.rows[chosen_row_idxs].long()\n",
    "\n",
    "        anchor_idxs = np.arange(chosen_rows.shape[0])\n",
    "        positive_idxs = np.array([])\n",
    "        negative_idxs = np.array([])\n",
    "\n",
    "        mask = np.ones(chosen_rows.shape[0], dtype=bool)\n",
    "\n",
    "        # turn off autograd to speed up computation\n",
    "        with torch.no_grad():\n",
    "            embeddings = np.array(model(chosen_rows.to(device)).cpu())\n",
    "    \n",
    "        dists = np.matmul(embeddings, embeddings.T)\n",
    "\n",
    "        pos_mask = np.zeros(dists.shape, dtype=bool)\n",
    "        for i in range(0, embeddings.shape[0], rows_per_table):\n",
    "            pos_mask[i:i+rows_per_table,i:i+rows_per_table] = 1\n",
    "\n",
    "        neg_mask = ~pos_mask\n",
    "\n",
    "        # Ensure the same rows are not comapred against each other\n",
    "        # np.fill_diagonal(pos_mask,0)\n",
    "        positive_dists = np.where(pos_mask, dists, 0)\n",
    "        negative_dists = np.where(neg_mask, dists, 0)\n",
    "\n",
    "        positive_idxs = np.argmax(positive_dists, axis=1)\n",
    "        negative_idxs = np.argmin(negative_dists, axis=1)\n",
    "        \n",
    "        # print(\"Elapsed: \" + str(time.time() - t), len(chosen_rows), len(anchor_idxs))\n",
    "        return (\n",
    "            chosen_rows.to(device), \n",
    "            torch.tensor(anchor_idxs).long().to(device), \n",
    "            torch.tensor(positive_idxs).long().to(device), \n",
    "            torch.tensor(negative_idxs).long().to(device)\n",
    "        )\n",
    "\n",
    "    def get_average_precision(self, model, first_tables=20):\n",
    "        with torch.no_grad():\n",
    "            embs = np.array(model(self.rows.long().to(device)).cpu())  # Our model uses small => better, avg_precision uses opposite\n",
    "        precs = []\n",
    "\n",
    "        dists = np.matmul(embs, embs.T)\n",
    "        pos_mask = np.zeros(dists.shape, dtype=bool)\n",
    "        \n",
    "        for table_start, table_end in zip(self.table_positions[:first_tables], self.table_positions[1:first_tables]):\n",
    "            pos_mask[table_start:table_end,table_start:table_end] = 1\n",
    "\n",
    "        for mask, scores in zip(pos_mask, dists):\n",
    "            # print(mask, scores)\n",
    "            avg_prec_score = average_precision_score(mask, scores)\n",
    "            precs.append(avg_prec_score)\n",
    "\n",
    "        return precs\n",
    "\n",
    "table_tensors_train = []\n",
    "table_tensors_val = []\n",
    "table_tensors_test = []\n",
    "take_tables = 2\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Generate training, validation, and testing datasets\n",
    "for table in table_tensors[:take_tables]:\n",
    "    # Select 5 random rows to be validation / test rows\n",
    "    non_train_idxs = torch.randint(0, table.shape[0], size=(1, 5)).flatten()\n",
    "    val_idxs, test_idxs = non_train_idxs[:3], non_train_idxs[3:]\n",
    "\n",
    "    # All other rows are training rows\n",
    "    train_idxs = torch.ones(table.shape[0], dtype=bool)\n",
    "    train_idxs[non_train_idxs] = False\n",
    "\n",
    "    # append the data to appropriate list\n",
    "    table_tensors_train.append(table[train_idxs])\n",
    "    table_tensors_val.append(table[val_idxs])\n",
    "    table_tensors_test.append(table[test_idxs])\n",
    "\n",
    "dataset_train = TableDataset(table_tensors_train)\n",
    "dataset_val = TableDataset(table_tensors_val)\n",
    "dataset_test = TableDataset(table_tensors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_mask = generate_square_subsequent_mask(maxRowLen)\n",
    "batch, anchors, positives, negatives = dataset_val.get_triplets(model, 0, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision at epoch 0: 0.6055555555555555\n",
      "Average Precision at epoch 100: 0.6055555555555555\n",
      "Average Precision at epoch 200: 0.487037037037037\n",
      "Average Precision at epoch 300: 0.5166666666666667\n",
      "Average Precision at epoch 400: 0.6055555555555555\n",
      "Average Precision at epoch 500: 0.6055555555555555\n",
      "Average Precision at epoch 600: 0.487037037037037\n",
      "Average Precision at epoch 700: 0.487037037037037\n",
      "Average Precision at epoch 800: 0.6055555555555555\n",
      "Average Precision at epoch 900: 0.487037037037037\n",
      "Average Precision at epoch 1000: 0.6055555555555555\n",
      "Average Precision at epoch 1100: 0.6055555555555555\n",
      "Average Precision at epoch 1200: 0.487037037037037\n",
      "Average Precision at epoch 1300: 0.6055555555555555\n",
      "Average Precision at epoch 1400: 0.5166666666666667\n",
      "Average Precision at epoch 1500: 0.487037037037037\n",
      "Average Precision at epoch 1600: 0.5166666666666667\n",
      "Average Precision at epoch 1700: 0.487037037037037\n",
      "Average Precision at epoch 1800: 0.487037037037037\n",
      "Average Precision at epoch 1900: 0.6055555555555555\n",
      "Average Precision at epoch 2000: 0.487037037037037\n",
      "Average Precision at epoch 2100: 0.487037037037037\n",
      "Average Precision at epoch 2200: 0.487037037037037\n",
      "Average Precision at epoch 2300: 0.6055555555555555\n",
      "Average Precision at epoch 2400: 0.6055555555555555\n",
      "Average Precision at epoch 2500: 0.487037037037037\n",
      "Average Precision at epoch 2600: 0.487037037037037\n",
      "Average Precision at epoch 2700: 0.6055555555555555\n",
      "Average Precision at epoch 2800: 0.6055555555555555\n",
      "Average Precision at epoch 2900: 0.6055555555555555\n",
      "Average Precision at epoch 3000: 0.6055555555555555\n",
      "Average Precision at epoch 3100: 0.487037037037037\n",
      "Average Precision at epoch 3200: 0.6055555555555555\n",
      "Average Precision at epoch 3300: 0.487037037037037\n",
      "Average Precision at epoch 3400: 0.6055555555555555\n",
      "Average Precision at epoch 3500: 0.6055555555555555\n",
      "Average Precision at epoch 3600: 0.487037037037037\n",
      "Average Precision at epoch 3700: 0.487037037037037\n",
      "Average Precision at epoch 3800: 0.487037037037037\n",
      "Average Precision at epoch 3900: 0.487037037037037\n",
      "Average Precision at epoch 4000: 0.6055555555555555\n",
      "Average Precision at epoch 4100: 0.5166666666666667\n",
      "Average Precision at epoch 4200: 0.487037037037037\n",
      "Average Precision at epoch 4300: 0.487037037037037\n",
      "Average Precision at epoch 4400: 0.6055555555555555\n",
      "Average Precision at epoch 4500: 0.487037037037037\n",
      "Average Precision at epoch 4600: 0.487037037037037\n",
      "Average Precision at epoch 4700: 0.487037037037037\n",
      "Average Precision at epoch 4800: 0.487037037037037\n",
      "Average Precision at epoch 4900: 0.487037037037037\n",
      "Average Precision at epoch 5000: 0.487037037037037\n",
      "Average Precision at epoch 5100: 0.6055555555555555\n",
      "Average Precision at epoch 5200: 0.6055555555555555\n",
      "Average Precision at epoch 5300: 0.487037037037037\n",
      "Average Precision at epoch 5400: 0.6055555555555555\n",
      "Average Precision at epoch 5500: 0.5166666666666667\n",
      "Average Precision at epoch 5600: 0.487037037037037\n",
      "Average Precision at epoch 5700: 0.487037037037037\n",
      "Average Precision at epoch 5800: 0.6055555555555555\n",
      "Average Precision at epoch 5900: 0.6055555555555555\n",
      "Average Precision at epoch 6000: 0.487037037037037\n",
      "Average Precision at epoch 6100: 0.6055555555555555\n",
      "Average Precision at epoch 6200: 0.6055555555555555\n",
      "Average Precision at epoch 6300: 0.487037037037037\n",
      "Average Precision at epoch 6400: 0.487037037037037\n",
      "Average Precision at epoch 6500: 0.6055555555555555\n",
      "Average Precision at epoch 6600: 0.487037037037037\n",
      "Average Precision at epoch 6700: 0.487037037037037\n",
      "Average Precision at epoch 6800: 0.6055555555555555\n",
      "Average Precision at epoch 6900: 0.6055555555555555\n",
      "Average Precision at epoch 7000: 0.487037037037037\n",
      "Average Precision at epoch 7100: 0.6055555555555555\n",
      "Average Precision at epoch 7200: 0.6055555555555555\n",
      "Average Precision at epoch 7300: 0.487037037037037\n",
      "Average Precision at epoch 7400: 0.487037037037037\n",
      "Average Precision at epoch 7500: 0.487037037037037\n",
      "Average Precision at epoch 7600: 0.6055555555555555\n",
      "Average Precision at epoch 7700: 0.487037037037037\n",
      "Average Precision at epoch 7800: 0.6055555555555555\n",
      "Average Precision at epoch 7900: 0.6055555555555555\n",
      "Average Precision at epoch 8000: 0.487037037037037\n",
      "Average Precision at epoch 8100: 0.5166666666666667\n",
      "Average Precision at epoch 8200: 0.487037037037037\n",
      "Average Precision at epoch 8300: 0.6055555555555555\n",
      "Average Precision at epoch 8400: 0.5166666666666667\n",
      "Average Precision at epoch 8500: 0.6055555555555555\n",
      "Average Precision at epoch 8600: 0.487037037037037\n",
      "Average Precision at epoch 8700: 0.487037037037037\n",
      "Average Precision at epoch 8800: 0.6055555555555555\n",
      "Average Precision at epoch 8900: 0.6055555555555555\n",
      "Average Precision at epoch 9000: 0.6055555555555555\n",
      "Average Precision at epoch 9100: 0.6055555555555555\n",
      "Average Precision at epoch 9200: 0.6055555555555555\n",
      "Average Precision at epoch 9300: 0.6055555555555555\n",
      "Average Precision at epoch 9400: 0.6055555555555555\n",
      "Average Precision at epoch 9500: 0.6055555555555555\n",
      "Average Precision at epoch 9600: 0.6055555555555555\n",
      "Average Precision at epoch 9700: 0.487037037037037\n",
      "Average Precision at epoch 9800: 0.5166666666666667\n",
      "Average Precision at epoch 9900: 0.487037037037037\n"
     ]
    }
   ],
   "source": [
    "def train(model, optimizer, loss_fn, data, BATCH_SIZE=2):\n",
    "    model.train()\n",
    "    losses = []\n",
    "\n",
    "    for i in range(0, len(data), BATCH_SIZE):\n",
    "        batch, anchors, positives, negatives = data.get_triplets(model, i, BATCH_SIZE)\n",
    "        optimizer.zero_grad()\n",
    "        anchor_embs = model(batch[anchors])\n",
    "        positive_embs = model(batch[positives])\n",
    "        negative_embs = model(batch[negatives])\n",
    "            \n",
    "        loss = loss_fn(anchor_embs, positive_embs, negative_embs)\n",
    "        \n",
    "        loss.backward() # back-propagation, could do manually\n",
    "        optimizer.step() # Adjusts the weights for us\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return losses\n",
    "\n",
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        precision_scores = data.get_average_precision(model)\n",
    "\n",
    "    return precision_scores\n",
    "\n",
    "ntokens = id_count + 1  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0.2  # dropout probability\n",
    "EPOCHS = 10000\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)\n",
    "loss_fn = nn.TripletMarginLoss(1, 2).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-3, weight_decay=0.0005)\n",
    "train_losses = []\n",
    "precision_scores = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # print(\"EPOCH: \" + str(epoch))\n",
    "\n",
    "    dataset_train.shuffle()\n",
    "    train_losses += train(model, optimizer, loss_fn, dataset_train)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        # dataset_val.shuffle()\n",
    "        prec = evaluate(model, dataset_val)\n",
    "        precision_scores.extend(prec)\n",
    "        print(\"Average Precision at epoch %s: %s\" % (epoch, np.mean(prec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b6b83287c0>]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWsAAAD4CAYAAAAqw8chAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAtMUlEQVR4nO3dd3wUZf4H8M8XEgi9RkGCBlBQUGkRpIgISNPTO9vhHf7U00Msp95ZDk49y3mK5fSwyyk2sFFsIIjSFKUl1ACJ9JBQEkIIJQlpz++P3U22zOzO7s7s7mw+79eLF9nZ2ZnvbPnOM888RZRSICKi2FYv2gEQEVFgTNZERDbAZE1EZANM1kRENsBkTURkAwlWbLRt27YqNTXVik0TEcWljIyMw0qpZL3nLUnWqampSE9Pt2LTRERxSUT2+nue1SBERDbAZE1EZANM1kRENsBkTURkA0zWREQ2wGRNRGQDTNZERDYQc8k6Y28Rth04Fu0wiIhiiiWdYsJx7Zu/AAD2TLkiypEQEcUOQyVrEfmriGwRkUwR+UREkqwOjIiIagVM1iLSAcC9ANKUUucDqA9gnBXBnDhVacVmiYhsz2iddQKARiKSAKAxgP1WBHOwuMyKzRIR2V7AZK2UygPwIoAcAAcAFCulFnmvJyITRCRdRNILCgpCCuaVxdtDeh0RUbwzUg3SCsDVADoBOANAExEZ772eUmqaUipNKZWWnKw7yp9fX2+0pMBORGR7RqpBRgDYrZQqUEpVAJgLYKC1YRERkTsjyToHwMUi0lhEBMBwANusDYuIiNwZqbNeDWA2gHUANjtfM83iuIiIyI2hTjFKqccBPG5xLEREpCPmupsTEZEvJmsiIhtgsiYisgEmayIiG2CyJiKyASZrIiIbYLImIrIBJmsiIhtgsiYisgEmayIiG2CyJiKyASZrIiIbYLImIrKBuEnW+cfLkDppPlbtKox2KEREpoubZJ2xpwgA8P7Pe6IbCBGRBYzMwdhNRDa4/TsmIvdHILYaEz/KQKfJ8yO5SyKimBJw8gGlVDaAXgAgIvUB5AH4wtqwPC3ccjCSuyMiijnBVoMMB7BTKbXXimDMoKCiHQIRkemCTdbjAHxiRSDeTlVWBbW+iEWBEBHFAMPJWkQaALgKwCyd5yeISLqIpBcUFIQd2B0fZYS9DSKieBFMyXoMgHVKqUNaTyqlpiml0pRSacnJyWEHtiw7uISvWPtBRHEsmGR9IyJUBRIOAetDiCj+GErWItIYwOUA5lobTmjKKqrw2FeZAHiDkYjiU8CmewCglCoB0MbiWEL21YY8HD5RHu0wiIgsExc9GKtZmCaiOBcXyZqIKN4xWRMR2UBMJ+sd+cejHQIRUUyI6WS9LudotEMgIooJMZ2siYjIIS6StXs3GPZkJKJ4FBfJmigeKaUwac4mbM4tjnYoFAOYrIli1OET5fh07T7c+v6aaIdCMYDJmojIBmybrKuqFYa+sBTfbNwf7VCIiCxn22RdWlGFPYUlmDRnU7RDISKynG2TNRFRXRLTydroyNSc0ouI4l1MJ+tQsJk1EcWjuEjW7AhD8YgTaZA7ozPFtBSR2SKSJSLbRGSA1YGFijUiFH/4rSaDM8UAmApgoVLqOucs540tjKkGyxVERA4Bk7WINAcwBMAtAKCUKgcQM3NoeSd0JngiikdGqkE6AygA8J6IrBeRd0SkifdKIjJBRNJFJL2goMD0QH3257Fvy3dnmslzN+O299dGOwwishkjyToBQB8AbyqlegM4CWCS90pKqWlKqTSlVFpycrIpweUeKcHv3vjZlG3Fik/W5GBxVn60wyAimzFSZ50LIFcptdr5eDY0krUVXl+2E1WcDZeIKHDJWil1EMA+EenmXDQcwFZLoyIiIg9GW4P8BcBMZ0uQXQButS6kWjaqiiYispShZK2U2gAgzdpQgsPKESKqS4yWrGPKroITeHFRdrTDICKKGFsm6/s/24BNblMdCStMiCjOxcXYIO44TggRxaO4S9YUGcfLKlDNZpVEEVMnknXe0VIMmrIEeUdLAQD7jpSg66MLsCP/RJQjs6cTpypxwROL8NzCrGiHQlRnxF2y1up6/tnafcg7WopZ6fsAAPM2HUB5ZTVmZeyLcHTx4XhZBQDgyw15UY6EqO6wVbLOP1YGwLP9dWWV56V4OHXWuwpO4Nb31qCsoir0jdQB9Z1nRNaCWIzvL7mJ6WRd6ZUN7v9sg8865VXVhrenFPDE11vw66Hjms8/+c1WLM0uwMpdhUHFGQkzVu3Fa0u2RzsMAIA4k7Xi3dyIsNNAZWQdWzXdK3WWeI2kiHU5RTjntKZolpRYs2z/0VLMysgNO44Fmw/grDZN0P2M5mFvy6hHv8wEANwz7JyI7VNPPWfyYMmaKHJiumTtzVWQc29jDQAPz9nk8bisogrXvPEL/vxhut/tvb18Fw6fOBV0HHfOXIexr/xkeP3MvGJk7D0S9H5iVb2aahBma6JIsVWyBmBoFD5X9clmr6SuZUPO0XBDCujKV1fg2jdXWr6fSKlJ1ixaE0WMrZK11XV3TD0GBagGqaiqxsHissjFQ5ZavasQmXmBCz5kLVsla6Uic1OL93PC8/jXW3Dxs4txzNnEj+zt99NW4cpXV0Q7jDrPVsk60gpPnAqpTjvuBThfLt52CABQcio6TSB35B83VAUW63ilR+5s1RoEqG02pk//K671jL/N9X36BwDAnilXBA7MS2l5FSqqq9HcrTVKvInVK5ARL/0IILTPjShWGSpZi8geEdksIhtExH8TC4sFUw1ysjxwyc59c2ZWsQx5YSkufGKRaduzE623sayiCu//vJs3JYMQqydDio5gqkEuU0r1UkrF1CQEWvS+5Ea//IFL7/6VVVSh4Hh8VJ98u/kAiktDq3t2fxtf/uFXPPHNVnyzab9JkRHVLXFZZ+1edsvMK8bJU5U+y12saGGyYd9R8zfqJu9oKXYWBDcIVW5RCdL3BNfWO6ewBHfNXId7P1nvsVyFUJtaXOJI+Pd9ukG3BykR6TOarBWARSKSISITtFYQkQkiki4i6QUFBeZFqBFIMK58dQXeXbHb8VqTr8CjNfP6oClLMPw/y4N6zeDnluK6t4y39d524FhNiXq/c7RCHyGe6NYGedIgY7YdOIYZq/ZGOwyyiNFkPUgp1QfAGAB3i8gQ7xWUUtOUUmlKqbTk5GRTgzSL2aXoR77YbO4GY0TRyXKMmfoTHpq9MdqhUBDGTP2pZlgCij+GkrVSar/z/3wAXwDoZ2VQ4QrmRmE4CXy2CeOMxKKT5Y5qo6yDrK4gihUBk7WINBGRZq6/AYwEELXTt5E8/IOzna9Z26tdV6HoZLnxF8SgdTlFlh+DWfcGSsurMHP13jo7ul/dPGrSY6RkfTqAFSKyEcAaAPOVUgutDUubkS9vZbXCwWLtlhjh/uY/XLkXvf/1fcD1Yjm3XPPGLxg3bVVQr9nuNaOO0eMLt9bpmW+34ZEvMrHsV+vugdhBKO/jzNV746ZFEjkE7BSjlNoFoGcEYjEkUEuEZdkFWJZt/McdTIlvWXa+x2Pv8bbtIjvKrTGMJvvCk45kE62ekHaVU1iCR77IxNx1eZhz58Boh0Mmicume3r8JebcohL8tP2wY70IxWPEkqxDfsfYqK5WeGXxdhwtsXf1DJmnotoxIYfdq+zIk62Sdbi937RKdL/sKMTrS3dg8HNLw9q2FfYfLcWf3k/HXz5er7vO8u0FeOn7X/HYV1tM228w1TjzNu3HoClLUBlwxp5YOgUS2Y+tkvWhY2Wm1we/s2I3Xvgu2+86053ttLV6NuYdLcXHq3NQEcT0Yka5ZsZZ7qfOtqLSsd9SA13rzeL+Efxj7mbkHS3FyTpSVfHuit3Yd6Qk2mGYrqpaBd3RyojqaoVnvt1m+XtWWl6FQ8fie1heWyXrcC3JMt5KxN1T87bqPjdoyhL844vNeNptnVB6+MWC/GNlOOHs7WmEXlk50AnVrnMKFhw/hX/N24qbp6+JdiimqK5WePTLzdhVcAL/WZSN4f9Zjt2HT5q6j60HjmHaj7twzyf6V4fBWph5ED9t9yzA/OGdVej/zGLT9hGLbJWsFYBjIY5TAQBFJcZeG0oy+WBlbc+xpVn5ftYMTClV00U+kvo9sxhXBDFdmTvNE1SYSTmWWtU88sVmrN7tmEj5eBQ+GytkHTyOGatycNfMdTW9Ss1uQeL6DKuqzbvynDgjAze963nCXB+BGZ+izVbJGkBUZx4PlHtyixyXev/7aXdY+/l4TQ56PP4dcqJwub23UHufem2djQ565b5asEk4FkriM1fn4B4/9w6sEEsnq3i0LqcIT32jf9Uca2yVrCP15d1+6AQuePw7r30H3nmJRr3xmt3Bj4OxaIujumZ3gfFL0mA6AoUr6+Cxmr+NdliJlcRTXFphyqX+t5sP4PWlO0yIyP7W5RQhddJ8rM8pinYoQbnmjV8w/efwClaRZKtkXVJeiTy9QYVMNGP1Xp9L3VAbotzwdugT5RopUYY7nGso/vC/1YbXLTpZjj+9vzasZmRaif5UZRXyQ7ih9JtXV+CyF5eFHIvLXTPXBbwxHa5YuKIwwtWvwd+N8HAdPnEKpyqtv4mdmVdcM/BbrLHVTDEl5VV4fqG1PxA90RhhL1a/NO7cTxbLsvNrZj53+WDlHiwJsQ7fX7K6c8Y6LMnKD3o2GDOqlmLlKkFPLMYXbkxpT/+AS85pi49u629OQDpcc03eNriTpfsJha1K1tEUjWSdWxTcVYTefJFabaC926xP+ND/BECBfmxHTpbjlvfW4v+mr0EkRrUI5QQQbJXUwswDSJ003xZN9c57LPgRILROhmaPw2Lm1YGr01qseHj2Rny1IS9i+2OyNsiM5ng5OjfvMvOKcbA4/DaiaU//4NP07uuN+3H2Iwt81vX+ES3aGl6d9zCN8bUliOYgJeWVPi1gzC4hBlslNXed44e4ZX/sTr67I/8EUifNr2mTD4SWIIP5rPTEYoneSp+n5+K+TzdEbH9M1hp0pwUL8/s8ZuqPmsuvfHUFBj+3pOZxON/5Eq+EtzDzQBhbC5/RJND7qe/Rw+umbs02bFJ3G2l7Dp/0Ga8mGvx9PP4SeEVVdVzMQh8pTNYaQr1pF+hV/ibwraxW2OEc3S6cNqnBJPpHv9yMa9/8xfD6k+du9hjD28ybm6cqw2+HW3D8FFInzTchGodoFxRdiS7/+ClM+3FnzfLvtx5C6qT5GPrispreteHux+qOXFpflecXZuE3r63Adk7zZgiTtYYd+b7dbiNxiTfipeVYmp2Pcj+J66OVe0zb34xVOcjYa6y5VUV1NT5Zk4MHZwU3e4yRfB5sRwy9btFGjyV4kSnab8o9iqfnbdWsN3a/sT7PbdLh/WFUn2l9NuGcgIP9iWxylqoPn4jdAad+PXQcx/0MpBZJtmoNEn3W/2hvfW8tWjdpoPu8mQM2BUPvZGVG4fq3r//s8fjxrzLxw7Z8XJjSQnP9sVND62UZOs+D17uRG65r3vgFldUK7Vok4fZLOluyD3dmFUCMfAe09hXtKxcjRr78I3qmtMBX9wyOdijGS9YiUl9E1ovIPCsDilVrIjjJazDjc0Sb5o/QbZmRXO7ddv6DlXs9lnnvQ6/KxOy6bdfmIn3j7On52yK6P8/epeEfbOqk+TXzkxrrKxD2Li21MUbq1YOpBrkPQGS/RTHk1vfWmtpLcNWuQizachDpYZ4E/hxmkzsr6f0I7VCiAmI3zkC5LdjPXClzWoO4m7k6J6TXbdh3FO9FuFehXaaNM1QNIiIpAK4A8G8Af7M0ojoi2Km1tBi5HA+3SZ6L1ve52MCgWmaUmqwseS3/tQD3frIeKycPQ+MGtT8H9wkfYqHkF1I6CRB31I9L46BcVWL9O7VB9zOaRzig2Ga0ZP1fAA8D0L3zJSITRCRdRNILCur2nHnhMvobSnv6h4Dr6HXmue6t0LvBBxKNckr+sTJke83GbmTsjucWZKG4tAK73MZhWZh5ABc+sQgb9h0FYN3VycLMAyGNrhhOOMUlFfj3/K1InTQfx0rNrW6rrlbYduBY4BW9aH3fx77yE0rK7VMdGAlGZje/EkC+UirD33pKqWlKqTSlVFpycrJpAZI1rGs5YcyqXYUBq4BcSfKumesCbm/AlCV46ftfPZYZGbujXj3PfQHAzzscIzu6WqkcLzM/aWw7cAwTZ6zDo19m1iyLREn3hrdX1owK6eqZmX3oeM1UYMHKKSypGffltaU7MCaIm7+u5oK/HjqOB2dt9ClYVFT6npa872+s3FloanPNWGakZD0IwFUisgfApwCGicgMS6Oq46J9eXrJ875TnAXTDjfgULJHSjBu2qqApfuFWw4a3meowwG4xjKp9lN8fnjOppC27Y+rRO0aVhfwLcHrveeGvx4K+GjVXmTm1d4g05ssOdSBtoa8sNRjLPdQPPbVFszOyMWewsCjIQ6assTj8VvLd+qs6VBcWoFRL/+IX+OgLXfAZK2UmqyUSlFKpQIYB2CJUmq85ZHVYWZddkdy3AJ3hW4/fK12u2//uKvm72iPu+GKzl+yDtWxsgps3e+/WiC3qBSpk+bXVLnoCeYmmPtb/tiXmTWDE9VFP/5agOxDxzF18XbddWxyf5GdYuKZ0XELvOt6tbyx1H8JRosAWBCgu7tWKT6SXCcT99H4Qr2yuXNGBh74vLbT0E3vrMZYnZl3XEPwHnB2avlsrW/rCbNbaPhsP8pXcGYkSTOOwcqhXc0UVLJWSi1TSl1pVTDkYMYXMJgJfEf9V3vMEnevhTDQfpVSyMwL/oaT2Y6VVeBFnfpr13ttxoA8CzIPYs662u74/trn3u1VDx9M4rKiIBiNwmWgfRacOIW7ZmZgo5+rDjPON0fCGGs9ktiD0URmlVTCKXFUVStsyj2KFo0SzQkmDGZXcSil8PXG/YFX9DJlQRY+1mn36z3+drBOnqrEiJeWY+q43kG9TmtWIfdQvGfqdq9O8jccgZ14V+14f+9HvOQYyXFTCJ1S1uUU4Zo3fsFDo7qFHF+sYTWIiYpKKjwG3ImGqT9sx1Wv/YzP1u6LahxWWLT1UEgl4LIK/QG06oV5gu3x+Hc4UFwW1oxAgG+i8p6pO5g661BO9tFuch0qvbFMZqU7rnBWRHAM7J5PLsJjbq17zMaStYmut7DtslFbDjhKIW8si+5JAwCWZgVfF/g/t5uP3o6WBH+5WlxS4fd627te2IxmYCXllR4dbIxYojHU6cXPLtZYMwghdIrxl+dPVVahYUL9kEIxMjaI3kWOv4ufQCeZmgm2vXY2K923MBPoZrA/v+w8jOLSCny0ai/+9dvzQ96OPyxZx6BwrswPHbNmkKFQhFLP7d1WOlwPBBol0IIi5QVPLPIoCf97fu0M2gszD2qeEIIdedBqpeVVPnMezt/kebPYSA9Wq4X6W3lotm9zTL2bwUa8vVy/kGEWJusYFE4rgFj70Zvl9aU7UFEV/PV9wfEyv6VF93farM4VVdUKE2fU9iFzdUIBgA9NGOI2Eq04zvvnQgx9YZnfdf762QbD29tqoGejmU3oghkjO9zdvrV8p6E24uFiso5BVg8EH8tKdeqXX/guGzNW+Xa+CFSXWxmFuTMB4Lst5g36BVjTWsP95Ke1/QN+xsouPHEq6Hkwn/pmq8djc5Kz55lrXY6jZ256ED10H5y10adH76FjZej+T/15LV9xa7c9ZUEW9upM2WcmJusYVFYRH3f7zZZloD24t2hMdGw1s9pfP6xRFWB0y3d/HHgIAG/TA4ym592V3CWY473mDe2Zj5Zm5+P+T9frvu7JbzzHiV+Sla/ZYsfF7Oo6I5isKa7pJWu7zFruTimF77ceMn7l5bba8bIK06p5tuwvxo78wJf993+6Hp+s0R8q1fsobp6+JszI9JWUV+HLDcE3+4wlbA1Ccc1fyXpZBHuufbUhD/d9ugEXdNCe/caI2Rm5mjfGvLnqtHcdrk2o/qo03OUWlaJ/gHWueMVY9/WAyTHMepBQSrd//WwDhp17Wlj7jRYma4prldVKt157webIzfzuah++r0i/NO+43NdPYN4dZfQE03vV2wOzNuLavik1j5fGwOzpel5ZvB1NGgTXlPCL9Xn4Yr3vmDk+g2jFYO0Zq0HI1gL9qPyVrH/ZWWjZfIqh8Fe9EUzyGP1f8+aoPBjGhLyBGD0kf61fTmrUK2/KPRp0LFkH/bdWWbH9MOa6DSUQDUzWZGu/5vu/6RhrrUX8dRUP1DQxnNJeqK/dka89k3yoiksq8PhXmX57lYbrqtd+DrySl0Dv/fh3V+Nvn/uOuR1JTNZka1e96v+HWVGt/Nadhjs2SLD8tTCwUqjNQQOVOIM1ae4mfLByL+asyzV8AolEszijbvxf+NPxhYrJmmytPED9bKBOQnYdEyNYczL8X8IbSZxmnNeOljh6PVYrYHNe8AM0Rdua3eFNcB0OJmuyvXCapEV7TOdghHMBvj/EumfXFGeAOTfdXGN12OhtjxlG5mBMEpE1IrJRRLaIyJORCIwoEvRGbYs3ewN0hzbyNvzt8wDjrNQR1VGqtzbSdO8UgGFKqRMikghghYgsUEpFr/KGqA5aHGT3bnfbDhjv/bkwU3/uS7PG0rbzOfJzjRH7IiFgslaO2+muW8KJzn8x2AqRKL75mzElkGBaMbgPQuVtwkfpIcdgJ5V+ZnufNHdzBCOpZajOWkTqi8gGAPkAvldKrdZYZ4KIpItIekGBPeY0IyIHo/XRy7LN+W1bPb9kuP751ZbAK0WYoWStlKpSSvUCkAKgn4j4jK6tlJqmlEpTSqUlJyebHCaRNWI7ZcSvbQaGTCVPwU6YexTAMgCjrQiGKNK2m9zpw64iXYf8kcZwt+SfkdYgySLS0vl3IwAjAGRZHBcRRVA8DiUbb4y0BmkP4AMRqQ9Hcv9cKTXP2rCIKJJyi7THkqbYYaQ1yCYAvSMQCxER6WAPRiIiG2CyJiKyASZrIiIbYLImIrIBJmsiIhtgsiYisgEmayIiG2CyJiKyASZrIiIbYLImIrIBJmsiIhtgsiYisgEmayIiG2CyJiKyASZrIiIbMDJTTEcRWSoi20Rki4jcF4nAiIiolpGZYioBPKCUWicizQBkiMj3SqmtFsdGREROAUvWSqkDSql1zr+PA9gGoIPVgRERUa2g6qxFJBWOKb5WWxINERFpMpysRaQpgDkA7ldKHdN4foKIpItIekFBgZkxEhHVeYaStYgkwpGoZyql5mqto5SappRKU0qlJScnmxkjEVGdZ6Q1iAB4F8A2pdRL1odERETejJSsBwG4CcAwEdng/DfW4riIiMhNwKZ7SqkVACQCsRARkQ72YCQisgEmayIiG2CyJiKyASZrIiIbYLImIrIBJmsiIhtgsiYisgEmayIiG2CyJiKyASZrIiIbYLImIrIBJmsiIhtgsiYisgEmayIiG2CyJiKyASZrIiIbMDKt13QRyReRzEgEREREvoyUrN8HMNriOIiIyI+AyVop9SOAIxGIhYiIdJhWZy0iE0QkXUTSCwoKzNosERHBxGStlJqmlEpTSqUlJyebtVkiIgJbgxAR2QKTNRGRDRhpuvcJgJUAuolIrojcZn1YRETkLiHQCkqpGyMRCBER6WM1SIQ1Twp4fiQi8sFkHWG/6XlGtEMgqpNaNk6MdghhYbKuI5o0qB/tEIiiqmdKy2iHEBYm6zoiLbV1tEMgojAwWUeYinYARHWUiO+yXh1bRjyOUNkuWZ/brlm0QyAiG9LI1TjntKa4IS0l4rGEwnbJup7W6dFGFIvWFGG7nx0b7RBM8/kdA0J+rWjkjnoiqDbhN3nlhe3D30gAMZmsL+9+erRDsFDtNyNSl2Bvje+reQkYLSmtGhle95Jz2loYSWh+eviyaIcQdS0bJ2LRX4dEdJ8PjuyK3me2NHWb9eoJ+ncK/37Oa3/oY0I0/sVksn78N91N29aL1/f0ePzA5V3x7s1ppm0/WO4l6y/vHhSRfY4+v11Yr//6nkGYcVt/k6IB7ri0C6bd1LfmcfOkBIw4z/gJOqGe+H1stY6tG+POoV0ius9waJUo3bVoFHyTtvH9z0LX0yNfJRnOJ6312oYJ9XBdX1aDhCylVWPsmXJFzeM/X9Ip5G15X54kN2uI4eedjlWTh4e8zUho1zwJ57ZrhmUPDjVle+F8yS9MaYnBfkq4L17fE1PH9TK8vQGdW2Nkj9oTyC0DU/GOzgn0/A4tPB5/dFs/n3XG9etoeN89Tbqa+fvoc7H5iZFBv26MnxOn1v2YpERzf6L/GHuuz7Kf/m7sSuGGtBSkPzoCK/5+Gf52eVcAjtJupHifdILNC1rnrM7JTTRPZpd1i72RQ2MyWXtLrF8b5mnNGxp+3Z4pVyAp0bN9setzadciCR1bG78ct8rKycN8lt3YryNW/WM4Ft4/BKltm4S0Xe/qg1E9witd+3Nd3xRc3atDyK8ffI7nD2NA5zY1f9877Jyav1dNHo5LztH+EbVtavx7YZZmSeZ1suhzZkvMmlhbH/vqjb0x8dIu2PKksUmafnr4Mow477SA6/2x/1k+y5oHcRxtmzZESqvGqOe8mrnH7fMBgI9vN3YFpnXSMCK86mXfpNxMp0dxv05tNJdHky2S9US3S87kKPwow9G0of/u5e1b+J4wnr3mQs11L+3qm6i2PDnKUBy/v6gjHhrVzdC6kdbPq87Q/bK8nts3tF2LJN1tLHtoqMfjiZdqV1NMGu2bJL65Z3DAGLue3jTgOkZ00jj5bn1qFGZNHOiR/H/T8wxMGnMu6hus4unYurHhm9eNEo11kAqlfnjg2dpXYOMvPtMjQV/VswMmjTkXF3RogeHnBj7JhOOxK7ujZ0oLn5L1v67ugat7+hYy5tw5EBMv7eyxzEgpfuXkYch4dERYsfpji2Std+Zv0SgRv+sdeonOat3bN8fdl53tsczM1iBz7hyAJg0TMHVcLzw0qptuggIcl5ANE4x93A+P9p/UG7htx4r64uvdmlKJnwqcmwfUlhK9T4qTxmiX3AZ0aeNR3fDtvZfgghTPqpYhGifFr+8ZjFkTB2D6LZ7VNc2c++3QshGSm+kXJFZNHo4P/9QPf73ct9qgcYMEv0l5+UNDcV775rrPu3Rs3bjmb70Sroj21ZyWNk2MFYz03mt3T151PiYM6YLT3a6MJ17aBd/8ZTCuMNiSwjvZ6tXFu1ehXnlhe9w6MBVfaZyQbxqQWnOF4D5mT8fWjXy2fVm3wCeU9i0aoY2FhUlbJGs9Gx8fiZd/3wt/1ygtRUpKq0Zo4KymGX/xmR7PddBo9aA0LuSMJFER30vAvmc5SqRX9+qAuy8729CPRo/7zc7xF/teKnvE4vz/wZFdMf/eS3TX805s3kZ7Vc2kPzoC6x67HH3ObFW7L43f41vj+6J/p9bocpqjtKt3AvzH2HM9bjDfO/wcn/W7n+GZBHc/OxYf3HqRx7L7R5yDpMT6uCi1NYad63kjtE3TBgAcdelrH9EvVbVrkYQhXZM9qvSMOqtNE7QyMK7FULd6Vr0SbqPE+mjZuEHNY++WLe6J02g1oXch4enfno+XbvC8se/6GLu1c7zf7if8Hmd4nixdvG98alXhBPLaH/rUJGR//m9Aas3fCfV8PyP3r5j7ySCSbJ2sXe4c2gUb/nm5x7LzO2iXREb3qP0yhlLK1WoF4CqZ/bZXB2x9qrZa4j839MRve5+BM1okYeq4XmjaMAFpZ/k2E1r20FDMvWug7j7XP3Y5Nvwz+JtZ3vwdr3szwgYBEoqr6d3tl3RGN42bYrueGYvdz471SGyr/zEcXZI9qwDeHN8Hu56pbQPctmlDtG7SAIGM6H46PrtjQMBWDhOGdPG40++6KaZ1wnQREZ/tXuSnq36gGCJpaLfT8NFt/fDxn/Xrjb3jdZXGV/z9MvzwtyH4Y//aAof3yd/fVY573e/4i8/CNX08W1i43vHX/9AbH/+5v8fn3K1dM6xwu8npGsfGff9NGyagRaNEjyu5s9rUXkkY4Xrl89dd6HPj/oGRXbFy8jBMHddL8zvYWGdsnRHnnYbsp43dVwiXofE6RWQ0gKkA6gN4Ryk1xdKonC5MaYFNucUey1wf+vPXetbrupcW5t41EF3a+tYxPjSqG1oEKKFsfmIkLnhikeZznZOb4I/9z8Sby3bWLLu2TwpW7Dhc87hxg9q3tHlSIponJeIXZ8uTq3t1wE/bHZMJuyeR9i0a1dRda30BWzm/PCrIs8tz116AopKKmseuJHVtnxQcPFaKn3cUar4uKbE+fnzoMgx5YanH8gX3XYKmDRPQMKEeMvYW+dy8ddEqyZze3Le+2ZEY/R9DYv16ePfmNM33JdG5n1BKq0ZlPjkq4H0HIPQbX6N6BNenoHWTBjhyslz3efcbsKsmD0e18zsz584BaKdxf8QlpZXj/c0/fqpmWcME44N/LX7gUhwqPhVwvWZJiRjYxbfU754g7x/RFR1bN8LI7u0wee5mj/XcTzZ/6HcmOrdtihv/t6pmmb9CtOulzZMSfG7ciwjat2ikeaN86rhe6O12tefuur4pQb1P4Qj4LRSR+gBeB3A5gFwAa0Xka6XUVquDm3F7f+w7UgLAcUm270ipW2D6r+uj88Z61x9rcb/Jc2O/jhh9fnvcPH0NAGDJA0MBAIPOboPeHVvhL8PPRoP69TySNQAse3AoTpyq1Nz+4LPb4smreuBajbadWf8a7beHZqvGgUudk8aciykLsgAAv7/Is1rGletbN0nElGsvQHllNXo8/p3mds7USI7u9aZjLvCtZ3zvlotQXlUdMMZAWjZOxB1DOuN3fRw/nOE6bbCv6ZOC3YUncY/zc01t0xh7CksCbt/1PtzhdRPJ251DuwRM1K4SZSg9azc9MdLnZt/8ewcj68Bx3df85/qe6NquGQZNWQLAUQCp0umC535Dtq/GFZ0W13G0d772sSu749/ztwbs5XdasySc1szzhLxq8nBc/OxiQ/tt3CABmU+Owg9bD+E3Pc/wW4f/8u97Ykf+CYgIBnTxbLUx+07HFeqI807DD9vyNV8f7BW1VgK/vm8KZmXkoomBE7lZjOypH4AdSqldACAinwK4GoDpybpl40RUVdW+k82TEmvqs2ZPHIhNucVYmu34AMy4saV3V/zdm9OQW1SKmwemaj4/8/aLPR7fkJaCjL1FOKuN42ztr7mdiOhuV6+k6vL0787H1xv3+11n3EUd8fzCLM3qGlcJNLF+vZp/D47sipPlVX63adRlGnf1p9+Shk7Oq5zr0zpiyoIsJDfVb9UBON6jyWPPC7i/Bgn1MHlM7Xpf3j0I324+iLNP87yqevumvjXJB3BcBr/wXTYeuFz/RqrResm3b+qLbzbuR6rz5FZPgGoFTLupL05vnoSrX/9Z97VaN857nNFCsw7X9d1omFgPHVrWlpCNFEC0vPz7niiv9D2x9kttjTsu7Yw/DXK0frhtcCc0T0rAQ7M3edQzG9GuRRJ+njQMa3cfMdSqpWnDBPxWp8FAI7dqiN/19izodGrbBLsPn8Tyh4bW/AZf/2MfFLtdVQK1v3ejLWy8ve92L+PJq3ugz1mtMFjn3oAllFJ+/wG4Do6qD9fjmwC8prHeBADpANLPPPNMFYqKyipVUVnld52jJeXqmW+3qnKN9eZt3K+WZh0yvDyn8KQa9fJytTTrkPrwl91q076jmvtctfOwmpW+z+BRWCt9zxE16uXlKjNPO1Z/Sssr1TPzt6oTZRWaz6/be0TNXLW35vH0FbvUlrzikGP1Vl1dHfDzjZaJH6Wr15duD3s7J09VeLy/q3cVqs/X5nisc8eH6erpeVuC2m7+sTL1wsIsVVVVrZRSKn1Podpz+ETY8RpRXlmlnvl2qyouLY/I/txNX7FL3fvJOrUz/7juOhWVVZr5wFvRyVPqxe+yVKXzPQxk7e5ClVN40nCs4QKQrvzkYlEBrglE5HoAo5RStzsf3wSgn1LqL3qvSUtLU+np6eGfSYiI6ggRyVBK6TahMnJdkwvAvT9vCgD/1+JERGQqI8l6LYBzRKSTiDQAMA7A19aGRURE7gLeYFRKVYrIPQC+g6Pp3nSl1BbLIyMiohqG2p0opb4F8K3FsRARkY646MFIRBTvmKyJiGyAyZqIyAaYrImIbCBgp5iQNipSAGBviC9vC+BwwLXiC485/tW14wV4zME6SymlO5+YJck6HCKS7q8XTzziMce/una8AI/ZbKwGISKyASZrIiIbiMVkPS3aAUQBjzn+1bXjBXjMpoq5OmsiIvIViyVrIiLywmRNRGQDMZOsRWS0iGSLyA4RmRTteMIhIh1FZKmIbBORLSJyn3N5axH5XkS2O/9v5faayc5jzxaRUW7L+4rIZudzr0gsTaftRUTqi8h6EZnnfBzvx9tSRGaLSJbzsx5QB475r87vdKaIfCIiSfF2zCIyXUTyRSTTbZlpxygiDUXkM+fy1SKSaigwf9PIROofHEOv7gTQGUADABsBdI92XGEcT3sAfZx/NwPwK4DuAJ4HMMm5fBKA55x/d3cec0MAnZzvRX3nc2sADIBjiuAFAMZE+/j8HPffAHwMYJ7zcbwf7wcAbnf+3QBAy3g+ZgAdAOwG0Mj5+HMAt8TbMQMYAqAPgEy3ZaYdI4C7ALzl/HscgM8MxRXtN8YZ8AAA37k9ngxgcrTjMvH4voJjdvhsAO2dy9oDyNY6XjjGDh/gXCfLbfmNAN6O9vHoHGMKgMUAhqE2Wcfz8TZ3Ji7xWh7Px9wBwD4AreEYXnkegJHxeMwAUr2StWnH6FrH+XcCHD0eJVBMsVIN4voSuOQ6l9me8xKnN4DVAE5XSh0AAOf/runA9Y6/g/Nv7+Wx6L8AHgbgPmV2PB9vZwAFAN5zVv28IyJNEMfHrJTKA/AigBwABwAUK6UWIY6P2Y2Zx1jzGqVUJYBiAG0CBRAryVqrvsr2bQpFpCmAOQDuV0od87eqxjLlZ3lMEZErAeQrpTKMvkRjmW2O1ykBjkvlN5VSvQGchOPyWI/tj9lZT3s1HJf7ZwBoIiLj/b1EY5mtjtmAUI4xpOOPlWQdd5PyikgiHIl6plJqrnPxIRFp73y+PYB853K94891/u29PNYMAnCViOwB8CmAYSIyA/F7vIAj1lyl1Grn49lwJO94PuYRAHYrpQqUUhUA5gIYiPg+Zhczj7HmNSKSAKAFgCOBAoiVZB1Xk/I67/q+C2CbUuolt6e+BnCz8++b4ajLdi0f57xL3AnAOQDWOC+3jovIxc5t/p/ba2KGUmqyUipFKZUKx2e3RCk1HnF6vACglDoIYJ+IdHMuGg5gK+L4mOGo/rhYRBo7Yx0OYBvi+5hdzDxG921dB8fvJfCVRbQr8t0q4MfC0WpiJ4BHoh1PmMcyGI7Lmk0ANjj/jYWjXmoxgO3O/1u7veYR57Fnw+3OOIA0AJnO516DgRsRUT72oai9wRjXxwugF4B05+f8JYBWdeCYnwSQ5Yz3IzhaQcTVMQP4BI46+Qo4SsG3mXmMAJIAzAKwA44WI52NxMXu5kRENhAr1SBEROQHkzURkQ0wWRMR2QCTNRGRDTBZExHZAJM1EZENMFkTEdnA/wPy+ol1TpxMhwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x136222fe340>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(precision_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-62.8419,  39.8016,  80.7338,  ...,  -7.5753, 108.8597, -80.1905],\n",
      "        [-64.5987,  42.8025,  85.3388,  ...,  -6.4767, 103.6926, -82.6397],\n",
      "        [-65.4186,  44.3794,  82.0520,  ...,  -5.9828, 110.5686, -84.1612],\n",
      "        [-67.7970,  44.8762,  83.7106,  ...,  -5.7798, 113.0596, -86.0408],\n",
      "        [-61.9913,  44.5986,  85.8799,  ...,  -6.1727, 106.8777, -83.6437],\n",
      "        [-63.5410,  44.5264,  86.0984,  ...,  -4.6227, 112.9923, -84.2130]],\n",
      "       device='cuda:0')\n",
      "tensor([[-69.3643,  44.7305,  86.9587,  ...,  -6.7774, 111.6379, -77.0954],\n",
      "        [-64.3062,  44.9177,  86.6477,  ...,  -7.6228, 102.0001, -82.2597],\n",
      "        [-64.7123,  42.2519,  82.8041,  ...,  -3.3238, 111.3392, -89.5604],\n",
      "        [-62.6974,  43.3751,  85.0456,  ...,  -5.7928, 115.2749, -81.9084],\n",
      "        [-69.6486,  43.2332,  84.9230,  ..., -11.6431, 104.2225, -79.3309],\n",
      "        [-64.9196,  43.1676,  86.7087,  ...,  -4.9971, 102.2482, -83.0609]],\n",
      "       device='cuda:0')\n",
      "tensor([[-61.9141,  44.5090,  86.4286,  ...,  -9.3240, 104.8498, -84.6133],\n",
      "        [-61.8099,  41.5211,  80.9820,  ...,  -5.0009, 107.8650, -82.8675],\n",
      "        [-64.9715,  42.8502,  86.8441,  ...,  -7.7572, 102.9031, -85.6837],\n",
      "        [-59.3907,  41.1544,  79.7867,  ...,  -8.0617, 106.6432, -78.9236],\n",
      "        [-64.5748,  42.5969,  85.1042,  ...,  -4.8867, 106.0099, -87.7743],\n",
      "        [-65.4337,  43.9824,  79.2740,  ..., -11.0008, 112.3588, -83.4025]],\n",
      "       device='cuda:0')\n",
      "tensor(2.1672, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    batch, anchors, positives, negatives = dataset_train.get_triplets(model, 0, 2)\n",
    "    positive_embs = model(batch[positives]) \n",
    "    negative_embs = model(batch[negatives]) \n",
    "    anchor_embs = model(batch[anchors])\n",
    "\n",
    "    print(anchor_embs)\n",
    "    print(positive_embs)\n",
    "    print(negative_embs)\n",
    "\n",
    "    print(loss_fn(anchor_embs, positive_embs, negative_embs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.0.weight Parameter containing:\n",
      "tensor([[ 0.0598,  0.0683,  0.0481,  ..., -0.0751,  0.0030, -0.0806],\n",
      "        [-0.0280,  0.0521,  0.0593,  ..., -0.0045,  0.0641,  0.0511],\n",
      "        [ 0.0378,  0.0125,  0.0244,  ..., -0.0393,  0.0073,  0.0021],\n",
      "        ...,\n",
      "        [-0.0146, -0.0776, -0.0010,  ...,  0.0701,  0.0474, -0.0784],\n",
      "        [ 0.0889,  0.0721, -0.0528,  ...,  0.0137,  0.0360,  0.0532],\n",
      "        [ 0.0141, -0.0414,  0.0252,  ..., -0.0462, -0.0711, -0.0621]],\n",
      "       requires_grad=True)\n",
      "fc1.0.bias Parameter containing:\n",
      "tensor([ 1.1631e-03, -1.5671e-02,  4.2719e-02, -1.9671e-02, -6.1954e-02,\n",
      "         6.9833e-02,  2.1540e-02, -8.8451e-02,  1.9791e-02,  9.5775e-03,\n",
      "         4.4858e-02, -5.1670e-02,  5.3314e-03, -7.9615e-02,  1.5851e-02,\n",
      "         6.1985e-02, -3.0717e-02, -1.6099e-03, -8.3902e-02,  4.0252e-02,\n",
      "        -3.2124e-02,  3.7077e-02, -8.6877e-02,  1.3426e-02, -6.0268e-02,\n",
      "        -8.3155e-02, -4.3461e-02,  4.7141e-02,  6.0672e-02, -6.7554e-03,\n",
      "         7.8005e-02,  3.7435e-02,  6.0804e-02,  5.5458e-02,  4.5368e-02,\n",
      "         2.3457e-02, -8.3799e-03, -6.8228e-02,  8.6676e-02, -8.2791e-02,\n",
      "         6.6383e-04,  1.1280e-04, -4.0606e-02, -7.2245e-03, -4.3357e-02,\n",
      "         7.7502e-03, -4.3273e-02, -4.8686e-02, -5.6347e-02, -7.8363e-02,\n",
      "         4.3351e-02,  3.1544e-02,  8.9673e-02, -1.7652e-02,  7.8565e-02,\n",
      "         2.1334e-02, -4.8589e-02, -4.2796e-02, -2.8305e-02, -2.3388e-02,\n",
      "         2.1918e-02,  3.4423e-02, -7.4028e-02,  8.3276e-03,  2.3234e-02,\n",
      "         3.3391e-02, -2.5238e-02,  2.3749e-02,  5.2422e-02,  8.1921e-02,\n",
      "        -3.9434e-02,  1.3690e-02, -1.5717e-02, -2.0686e-02,  4.4221e-02,\n",
      "        -9.1190e-02, -4.8240e-03, -4.9779e-02,  6.3286e-02,  3.3176e-02,\n",
      "         8.4715e-02,  4.7024e-02,  5.8938e-02, -6.0710e-02,  3.9550e-03,\n",
      "         8.4223e-02,  8.7472e-03,  3.0879e-02,  7.4523e-02, -6.1882e-02,\n",
      "        -6.9758e-02, -5.0106e-02,  4.8725e-02,  3.8173e-02,  3.5293e-02,\n",
      "        -2.4684e-02, -9.4327e-03,  4.3561e-03, -6.5855e-03, -3.9492e-03,\n",
      "        -7.4307e-02, -1.2853e-03, -5.8568e-02,  2.0611e-02, -2.3057e-02,\n",
      "        -5.0097e-02, -1.8272e-02,  3.8016e-02,  6.7616e-02,  5.0433e-02,\n",
      "        -1.5999e-02, -3.1252e-02,  1.9553e-02,  1.4534e-02,  6.4717e-02,\n",
      "        -4.8537e-02,  2.9676e-02,  4.3725e-02, -5.4115e-03, -8.8915e-02,\n",
      "         6.7356e-02,  4.6485e-02, -7.2155e-02, -9.0655e-02,  3.0815e-02,\n",
      "        -3.5817e-02, -1.2840e-02, -6.0059e-02,  1.5590e-02,  6.7545e-02,\n",
      "         4.6798e-02, -2.1610e-02, -2.8427e-02, -8.0566e-03,  6.6123e-02,\n",
      "         6.5378e-02, -2.3722e-02, -6.7027e-02, -4.8288e-02,  9.1420e-03,\n",
      "        -6.2714e-02,  1.5343e-02, -7.3846e-02,  2.2691e-02,  5.7871e-03,\n",
      "        -7.5305e-02,  3.0309e-02, -2.4821e-02, -2.5022e-02, -6.4941e-02,\n",
      "        -8.6190e-02,  2.4725e-05,  4.7293e-02, -9.0687e-02, -5.4716e-02,\n",
      "        -8.6605e-02,  1.0514e-02,  9.0422e-02, -5.8504e-02, -8.5044e-02,\n",
      "        -6.9972e-02,  5.0354e-02, -8.9870e-03, -4.4053e-02, -5.4271e-02,\n",
      "        -1.2166e-02, -6.3773e-02, -2.5345e-02, -7.4672e-02, -2.9846e-02,\n",
      "        -6.2021e-02, -2.5727e-02,  2.2455e-04,  5.6246e-02,  6.2730e-03,\n",
      "        -5.1638e-02, -4.5789e-03, -5.7288e-02, -1.3141e-02,  7.3239e-02,\n",
      "         5.5328e-02, -5.6595e-02, -6.7589e-02, -4.2624e-02,  3.0119e-02,\n",
      "        -6.2653e-02,  5.6835e-02,  7.4473e-02,  6.6240e-02, -3.3905e-02,\n",
      "         8.7421e-02, -4.4283e-02,  5.6765e-02,  7.1904e-03,  3.0857e-02,\n",
      "        -4.3944e-03,  5.7084e-02, -7.3299e-02, -2.1695e-02, -5.7038e-02,\n",
      "         1.5900e-02,  4.4830e-02, -3.1064e-02,  4.8557e-02,  4.2294e-02,\n",
      "        -1.0089e-02, -4.0236e-03,  4.0675e-03, -3.4604e-02,  5.3873e-02,\n",
      "        -5.1481e-02,  7.6553e-02,  2.1168e-02,  8.4695e-03, -4.7855e-03,\n",
      "        -5.1094e-02, -7.8632e-02, -9.1403e-02, -2.0474e-02, -6.4890e-02,\n",
      "         3.1208e-02,  9.5829e-03, -4.7429e-02, -2.4879e-02,  4.5388e-02,\n",
      "         6.2137e-02,  8.5112e-02, -3.1506e-02,  3.4418e-03, -8.9826e-02,\n",
      "        -2.2426e-02,  5.1137e-02, -4.3730e-02,  4.5308e-02,  2.9040e-02,\n",
      "         3.5792e-02,  8.2119e-02,  8.1693e-02, -4.0832e-02,  7.6441e-02,\n",
      "        -1.8802e-02,  3.4553e-02, -7.2076e-02,  1.8293e-02,  2.1863e-02,\n",
      "         8.1715e-02,  6.6647e-02, -2.7777e-02, -7.3928e-02, -5.2512e-02,\n",
      "        -4.5586e-02,  7.8904e-02, -3.2549e-02,  6.0860e-02, -8.5347e-03,\n",
      "         7.6957e-02,  1.7556e-02,  6.5060e-02,  8.7203e-02,  5.6875e-02,\n",
      "         4.6585e-02, -8.2998e-02, -2.1828e-02, -3.6091e-02, -3.8711e-02,\n",
      "        -5.0289e-03,  4.8036e-02,  1.1995e-02, -1.7936e-02,  5.6645e-02,\n",
      "        -2.2745e-02, -2.1822e-02, -1.9147e-02,  8.0396e-02,  7.6255e-02,\n",
      "        -6.1054e-02,  3.3860e-02, -2.2397e-02,  8.8514e-02, -8.4822e-02,\n",
      "        -2.6230e-02,  6.6985e-03, -4.4415e-02, -2.4266e-02, -1.5584e-03,\n",
      "         7.6813e-02,  1.4259e-02,  7.6666e-03, -2.7202e-02, -4.6482e-02,\n",
      "        -8.7925e-02,  1.2781e-02,  3.2884e-02, -5.8324e-02,  7.2836e-02,\n",
      "        -1.1326e-02,  5.3886e-02,  8.3362e-02, -5.3710e-02,  8.3245e-02,\n",
      "         8.6751e-02, -8.3429e-02,  1.6568e-02,  9.3461e-03, -4.7576e-02,\n",
      "        -3.6816e-02,  5.3004e-03,  5.5301e-02, -8.7692e-02, -7.7370e-02,\n",
      "         8.2679e-02, -1.3373e-02, -3.3426e-02,  2.9629e-02, -6.3583e-02,\n",
      "         7.7708e-02, -2.4321e-02,  3.3412e-02, -2.1436e-02,  2.1214e-02,\n",
      "        -7.9576e-02, -8.0808e-02,  3.7507e-02, -8.0551e-02, -7.0495e-02,\n",
      "         5.0775e-02, -5.6093e-02, -1.5382e-02, -2.8726e-02, -5.7570e-02,\n",
      "         5.1079e-02, -5.4600e-03,  6.5734e-02, -6.5439e-02,  8.6237e-02,\n",
      "        -5.1197e-03, -7.5121e-02, -1.0329e-02,  5.5495e-02, -5.5413e-02,\n",
      "        -9.0791e-02,  4.3938e-02, -4.7895e-02,  8.0751e-02, -2.1974e-02,\n",
      "        -2.2732e-02, -5.6036e-03, -2.4479e-02,  1.2779e-02,  8.7078e-02,\n",
      "        -8.7635e-02, -8.7459e-02,  1.8708e-02, -1.8520e-02,  8.2291e-03,\n",
      "        -2.6275e-03, -6.4756e-02,  4.1950e-02,  5.9253e-02, -8.5098e-02,\n",
      "        -8.9274e-02,  3.3844e-02, -8.0508e-02, -7.0276e-02,  5.6893e-02,\n",
      "        -7.6808e-02, -5.9355e-02, -8.5215e-02, -2.9830e-02, -7.1209e-02,\n",
      "         5.2504e-02,  3.7744e-02,  9.0220e-02,  6.1291e-02,  5.6893e-02,\n",
      "        -4.4711e-02, -7.9585e-02,  6.3231e-02, -7.3110e-02,  6.2147e-02,\n",
      "         8.3518e-02,  7.6684e-02, -5.2680e-02,  3.6908e-02,  3.1609e-02,\n",
      "         2.6863e-03, -5.4953e-02,  3.0852e-02,  4.9090e-02, -1.5187e-02,\n",
      "        -6.7921e-02,  8.2634e-02,  8.4728e-03, -8.7184e-02, -3.7595e-02,\n",
      "         5.4050e-02,  3.5410e-03, -4.5309e-02, -5.5969e-02,  2.4401e-02,\n",
      "        -6.4204e-02,  6.7614e-02,  6.7998e-02, -7.1338e-02,  5.1019e-02,\n",
      "        -8.9735e-02,  8.7382e-02, -1.3250e-03, -2.1996e-02,  8.9293e-02,\n",
      "        -8.3608e-03, -1.2230e-02, -3.2131e-03, -1.4439e-02, -4.9363e-02,\n",
      "        -5.0340e-02, -2.8603e-02, -9.3394e-04,  3.2526e-02,  7.5720e-03,\n",
      "         1.4202e-02, -8.9916e-02, -6.9745e-02,  1.4245e-02,  5.4149e-03,\n",
      "         3.9867e-02,  2.5546e-02, -4.2313e-02, -7.1988e-02,  7.2909e-02,\n",
      "        -5.0397e-02, -4.1453e-02, -5.6434e-02,  8.2080e-02,  3.0717e-02,\n",
      "         4.2321e-02,  2.9453e-03, -2.3572e-02,  5.0276e-02, -8.9467e-02,\n",
      "         8.7648e-02, -6.0212e-02,  8.9478e-02,  6.2653e-02, -3.6038e-02,\n",
      "         3.1170e-02, -2.0434e-02, -6.8068e-03,  3.7446e-02,  7.2894e-02,\n",
      "        -2.5189e-02, -4.1067e-02,  5.5542e-02,  1.5574e-02,  8.1888e-02,\n",
      "         9.2461e-04, -7.0231e-02, -3.5129e-02, -6.0469e-02,  2.3386e-02,\n",
      "        -8.6802e-02,  5.0142e-02, -7.6198e-02,  4.8846e-02,  8.2209e-03,\n",
      "         5.4397e-02,  6.7018e-02,  6.0843e-02,  2.2116e-02, -6.6815e-02,\n",
      "        -5.1320e-02, -1.0333e-02,  1.2357e-02, -8.7838e-02, -8.4630e-02,\n",
      "        -3.5897e-02,  3.4529e-02,  2.7124e-02, -4.3304e-02,  7.5583e-02,\n",
      "        -9.0891e-02,  3.3050e-02, -7.8463e-02,  2.5359e-02, -1.9137e-02,\n",
      "         7.6229e-02, -3.8414e-03, -7.0913e-02,  5.4834e-02,  7.8874e-02,\n",
      "        -1.3610e-03, -3.7705e-02, -8.8280e-03, -6.9353e-02, -1.8672e-02,\n",
      "        -3.6251e-02,  6.0498e-02,  5.4572e-02, -3.8571e-02, -5.6225e-02,\n",
      "        -2.1154e-02,  9.1503e-03, -3.5410e-02, -5.5934e-02,  4.1255e-03,\n",
      "        -5.2752e-02, -5.6327e-02, -2.1370e-02,  6.9386e-02, -8.0749e-02,\n",
      "        -5.9961e-02, -4.5978e-02], requires_grad=True)\n",
      "fc1.3.weight Parameter containing:\n",
      "tensor([[-0.0382, -0.0370, -0.0206,  ...,  0.0258,  0.0106, -0.0113],\n",
      "        [ 0.0150,  0.0016, -0.0375,  ..., -0.0058, -0.0401,  0.0338],\n",
      "        [-0.0073, -0.0026, -0.0324,  ...,  0.0236, -0.0295,  0.0336],\n",
      "        ...,\n",
      "        [-0.0176, -0.0030, -0.0239,  ..., -0.0260, -0.0245,  0.0363],\n",
      "        [-0.0268, -0.0041,  0.0149,  ..., -0.0370, -0.0254,  0.0087],\n",
      "        [ 0.0217,  0.0074, -0.0297,  ..., -0.0284,  0.0193,  0.0179]],\n",
      "       requires_grad=True)\n",
      "fc1.3.bias Parameter containing:\n",
      "tensor([-3.2155e-02,  1.0966e-02, -2.1511e-02, -3.8471e-02,  1.4097e-02,\n",
      "        -2.8084e-02,  1.3440e-02, -3.3179e-03,  3.4971e-02,  1.6580e-02,\n",
      "        -3.1564e-02, -1.5527e-02, -3.2949e-02, -2.4176e-02, -9.5887e-03,\n",
      "        -1.6987e-03, -2.6040e-02, -1.9810e-02,  6.8049e-03, -1.4182e-03,\n",
      "         2.9451e-02,  1.1167e-02, -1.6170e-02, -1.5691e-02, -1.9839e-03,\n",
      "         2.8058e-02,  2.7842e-02, -2.0024e-02, -9.1367e-03, -2.9851e-02,\n",
      "        -6.2468e-03, -6.3873e-03,  3.5968e-02, -1.0126e-02,  4.9903e-03,\n",
      "        -1.7980e-02,  2.5010e-02,  1.2284e-02, -2.5376e-04,  3.0868e-02,\n",
      "         2.8916e-02, -4.3288e-03, -3.3161e-02,  1.7462e-02,  1.7235e-02,\n",
      "         1.3627e-02,  1.9101e-02,  3.1933e-02, -1.0155e-02,  2.5986e-02,\n",
      "        -2.6173e-02, -3.0330e-02, -2.5006e-02,  4.2013e-03,  3.1402e-02,\n",
      "        -5.5320e-03,  1.4413e-02, -2.1447e-05, -1.1066e-02,  4.1290e-02,\n",
      "        -9.3421e-03,  1.2576e-02, -1.9030e-03, -2.3026e-02, -1.4176e-03,\n",
      "        -3.1543e-02,  1.9736e-03,  3.2162e-03, -3.1777e-02,  9.3941e-03,\n",
      "         2.9015e-02, -1.7773e-02, -1.1742e-02, -3.9689e-02,  8.2546e-03,\n",
      "        -3.3113e-02,  1.6117e-02, -4.0824e-02, -2.8030e-02,  2.9548e-02,\n",
      "         3.2950e-02, -1.4801e-02, -5.5658e-05, -8.8821e-03,  1.9914e-02,\n",
      "         1.5938e-02, -1.7737e-02, -2.2411e-02,  1.1384e-02, -3.2842e-02,\n",
      "         4.2222e-02, -2.0589e-02,  2.9203e-02,  9.0268e-03,  3.3815e-02,\n",
      "         3.6383e-03, -3.0563e-02, -2.4769e-02,  3.9276e-02,  2.4016e-03,\n",
      "        -2.3098e-04, -1.7763e-02,  3.3547e-02,  1.3945e-02, -3.6822e-02,\n",
      "         4.3826e-04, -4.2089e-02,  3.6735e-02, -2.6012e-03,  3.7832e-02,\n",
      "         2.6303e-02, -1.7186e-02,  1.3304e-02,  1.4324e-02,  8.2129e-03,\n",
      "         2.7523e-02, -9.0636e-03, -3.1501e-03,  2.7949e-02, -2.9346e-02,\n",
      "        -1.8060e-03, -9.3859e-03, -2.8412e-02,  2.7058e-02, -1.3379e-02,\n",
      "        -3.5901e-02, -2.9448e-02,  3.1862e-03, -3.4559e-02,  1.3101e-02,\n",
      "         3.8882e-02,  1.0700e-02,  2.7866e-03,  2.1269e-02, -1.0971e-02,\n",
      "        -8.4482e-03, -1.2411e-02, -3.9547e-02, -4.1465e-02, -5.0092e-04,\n",
      "        -2.8426e-02,  3.5538e-02, -1.5268e-02,  3.9612e-02,  4.0296e-02,\n",
      "        -1.9601e-02, -3.6982e-02, -2.3766e-02, -2.6342e-02,  3.0585e-02,\n",
      "         2.3827e-02,  6.4239e-03,  3.0917e-02,  1.1397e-02,  2.3683e-02,\n",
      "         2.0379e-02,  2.7752e-02, -2.6696e-02, -1.8615e-02, -2.6164e-02,\n",
      "         2.5489e-02,  2.1219e-02, -2.4037e-03, -1.9545e-02, -1.5766e-03,\n",
      "        -2.4617e-02, -3.9108e-03,  7.8358e-03, -7.3693e-03, -3.2902e-02,\n",
      "        -2.9853e-02, -2.6482e-02, -2.4723e-02, -2.7520e-03,  2.3446e-02,\n",
      "         2.0164e-02, -1.4966e-02, -1.6873e-03,  5.1143e-03, -1.7518e-03,\n",
      "         1.1964e-02,  1.6596e-02, -3.3190e-02, -8.5445e-03,  1.0414e-02,\n",
      "        -2.5309e-02, -3.1138e-02,  3.7776e-02, -3.6155e-02,  1.8183e-02,\n",
      "        -3.9614e-02,  1.3054e-02, -3.5058e-02, -2.0303e-02,  1.6731e-02,\n",
      "         2.7090e-02, -8.0893e-03, -2.4894e-02,  1.5318e-02, -3.1159e-02,\n",
      "        -3.5656e-02, -2.0320e-02, -3.2700e-02,  2.9401e-03,  2.7934e-02,\n",
      "        -1.7241e-02, -1.2199e-03, -3.3202e-02,  3.2905e-02, -1.4708e-02,\n",
      "         4.8424e-03,  4.1733e-02, -1.2836e-02, -6.8612e-03, -1.4901e-02,\n",
      "         2.3849e-02, -4.0271e-02, -1.8558e-02,  5.0359e-03, -6.5201e-03,\n",
      "         2.0487e-02,  4.1586e-02, -5.2974e-03, -2.9317e-02, -4.0800e-02,\n",
      "        -1.6876e-02,  3.8965e-02, -2.3794e-02, -6.2380e-04,  1.5034e-03,\n",
      "         1.2833e-02, -3.4639e-02, -1.7618e-03,  3.1842e-02,  3.3823e-02,\n",
      "        -3.0674e-02,  2.4716e-02,  2.9767e-02, -3.8037e-02, -3.8096e-02,\n",
      "        -2.9604e-02,  1.6746e-02,  1.3485e-02,  3.9810e-02, -5.9649e-03,\n",
      "        -2.6755e-02, -3.3475e-02, -2.6894e-02,  7.9410e-03,  1.3681e-02,\n",
      "         1.7916e-02, -1.3101e-02,  3.2960e-02,  4.7896e-03, -3.5990e-04,\n",
      "         1.2462e-02,  3.4656e-02, -1.9400e-02, -4.0627e-02,  1.6301e-03,\n",
      "         4.0108e-02,  6.1438e-03, -2.3841e-02, -3.0591e-02, -2.0911e-02,\n",
      "         2.4074e-02,  1.0153e-02,  3.9501e-02, -2.6711e-02, -1.1610e-02,\n",
      "         4.2701e-04, -1.1919e-03, -2.2846e-02,  1.0138e-02, -4.1546e-02,\n",
      "        -2.6445e-02, -1.5061e-02,  1.1111e-02, -5.6065e-03,  2.3728e-02,\n",
      "        -3.2723e-02,  2.2993e-02,  1.7384e-02, -3.7805e-03, -1.3598e-03,\n",
      "        -1.4680e-02, -3.6054e-02, -2.2164e-02,  7.8815e-03,  1.2378e-02,\n",
      "         3.0009e-02, -1.9277e-02, -6.8122e-03,  2.1491e-02, -5.2350e-04,\n",
      "         2.8385e-02,  1.6314e-02,  2.8084e-02, -3.4609e-02, -2.1896e-02,\n",
      "         2.0280e-02, -1.5092e-02,  2.1816e-02, -2.9252e-02, -2.8051e-02,\n",
      "        -1.1306e-02,  1.7229e-02,  7.5552e-03,  1.4496e-03,  1.5623e-02,\n",
      "        -2.2433e-02,  1.2649e-02,  3.8827e-02,  5.6014e-03, -8.0764e-04,\n",
      "        -3.2235e-02, -3.7268e-02, -1.2771e-02, -1.0947e-02, -1.7678e-02,\n",
      "         1.5668e-02,  3.6345e-03, -3.0779e-03,  3.0361e-02, -2.7204e-02,\n",
      "        -1.5962e-02, -2.6846e-02, -2.1363e-02,  2.3289e-02, -3.4310e-02,\n",
      "         3.8000e-03, -2.9325e-02,  2.9813e-02,  2.2048e-02, -2.1327e-02,\n",
      "         2.3695e-02, -1.5503e-02, -3.3362e-02,  3.7893e-02,  3.9441e-02,\n",
      "         4.0087e-02, -1.3038e-02, -2.5649e-02, -2.8771e-02,  1.9729e-02,\n",
      "        -1.5966e-02,  1.7066e-02,  1.2583e-03, -1.4837e-02, -5.9711e-03,\n",
      "         2.5158e-02,  1.5715e-02,  1.9046e-02, -4.0662e-02,  3.6838e-02,\n",
      "        -3.9207e-04,  8.8454e-03,  1.9332e-02, -4.0830e-02,  1.7687e-02,\n",
      "        -6.7779e-03,  3.4852e-02, -3.8727e-02,  8.7249e-03, -4.0619e-03,\n",
      "        -3.2564e-02,  3.9478e-02, -3.2624e-02, -7.1517e-03, -1.2313e-03,\n",
      "         7.4919e-05, -4.0800e-02,  2.1144e-02,  2.1252e-02, -3.0674e-02,\n",
      "        -3.2039e-02,  3.5828e-02,  3.9347e-02,  4.1352e-02,  2.0275e-02,\n",
      "        -2.6639e-02, -2.7693e-02, -2.3934e-02, -5.9062e-03,  1.5504e-02,\n",
      "        -2.6651e-02,  3.4680e-02, -2.0772e-02, -1.1046e-03,  7.3437e-03,\n",
      "        -3.6698e-02,  3.5677e-02, -1.3274e-02,  2.4862e-02,  3.1662e-02,\n",
      "        -2.3240e-02, -7.6842e-03,  3.3526e-03,  3.2204e-02,  1.2132e-03,\n",
      "        -7.7907e-03, -1.7082e-02,  2.4532e-02, -2.5090e-02,  2.9916e-02,\n",
      "         6.2880e-03, -2.3659e-02,  2.4877e-02,  6.9298e-03, -1.7660e-02,\n",
      "         3.6729e-02, -3.7653e-03, -1.4229e-02, -3.3879e-02,  1.0940e-02,\n",
      "        -7.6260e-03,  1.4001e-02, -5.7035e-03,  3.3372e-02, -4.0471e-02,\n",
      "         3.9893e-02,  3.0240e-02, -8.7803e-03,  4.0926e-02,  4.1051e-03,\n",
      "         3.0787e-03, -4.0434e-02,  2.9597e-02,  1.7544e-02,  8.8776e-03,\n",
      "        -3.0640e-02,  3.4491e-02,  3.5597e-02, -2.2166e-02, -1.5429e-02,\n",
      "         1.4353e-02,  7.4668e-04,  3.3028e-02, -1.1294e-02, -3.3502e-03,\n",
      "        -4.2279e-02, -1.4061e-02, -2.2049e-02,  7.2092e-03, -1.1499e-02,\n",
      "         8.1017e-03, -1.7218e-02, -4.9920e-03, -1.5006e-02, -2.3580e-02,\n",
      "         2.8326e-02,  7.0628e-03,  1.7594e-02,  1.2864e-02,  2.1017e-02,\n",
      "         3.0347e-02, -4.2096e-02,  3.7939e-02,  2.5479e-02, -4.0901e-02,\n",
      "         2.3452e-03,  2.4227e-02, -1.0725e-02,  3.5368e-02,  4.1565e-02,\n",
      "        -1.2928e-02,  7.0988e-03,  2.6762e-02,  1.6616e-03, -2.1435e-02,\n",
      "         1.3587e-02,  3.3741e-02, -2.1793e-02, -6.9508e-03, -1.3312e-02,\n",
      "         3.9596e-02,  1.6681e-03, -2.8490e-02,  1.5621e-02,  1.9920e-03,\n",
      "        -3.4723e-03, -2.0488e-02, -4.0348e-02,  3.8822e-02, -1.0141e-02,\n",
      "         2.2626e-02,  3.3845e-02,  2.6098e-02,  3.6243e-02,  1.3824e-02,\n",
      "        -2.5546e-03, -3.0636e-02, -3.5788e-02,  2.8198e-02,  1.5726e-02,\n",
      "        -3.1178e-02, -2.4291e-02, -1.6855e-02, -3.0040e-02,  2.9050e-02,\n",
      "         2.3269e-02,  2.8150e-02,  2.4345e-02,  4.1114e-02, -3.5954e-02,\n",
      "         2.5561e-02,  5.4061e-03,  1.3075e-03,  4.1791e-02, -3.7616e-02,\n",
      "        -2.1007e-02,  1.4190e-02], requires_grad=True)\n",
      "fc1.6.weight Parameter containing:\n",
      "tensor([[ 0.0066,  0.0005,  0.0340,  ..., -0.0397, -0.0399, -0.0199],\n",
      "        [-0.0365,  0.0369,  0.0032,  ..., -0.0242,  0.0061, -0.0308],\n",
      "        [ 0.0323, -0.0147, -0.0066,  ..., -0.0352,  0.0238,  0.0332],\n",
      "        ...,\n",
      "        [-0.0118, -0.0052,  0.0416,  ...,  0.0190,  0.0399, -0.0401],\n",
      "        [ 0.0275,  0.0225,  0.0433,  ..., -0.0186, -0.0234, -0.0078],\n",
      "        [ 0.0358,  0.0145, -0.0244,  ...,  0.0332, -0.0074,  0.0011]],\n",
      "       requires_grad=True)\n",
      "fc1.6.bias Parameter containing:\n",
      "tensor([-1.0397e-02, -2.9969e-02, -3.3155e-02,  1.9100e-03, -2.1131e-02,\n",
      "        -2.2360e-02, -3.2391e-02, -2.9548e-02, -3.1344e-02,  2.9953e-02,\n",
      "        -1.4929e-02,  1.4527e-02, -1.8140e-02, -4.0774e-02, -1.1002e-02,\n",
      "        -1.5634e-02,  3.4963e-02, -3.7609e-03,  2.0078e-03,  1.1896e-02,\n",
      "         8.6224e-03,  1.6652e-03, -2.5011e-02,  9.7382e-03, -1.9338e-02,\n",
      "        -1.3239e-02, -2.4277e-03, -2.8302e-02,  4.2683e-02, -1.8721e-02,\n",
      "        -3.4118e-02, -1.1335e-03,  1.6156e-02,  4.2260e-03,  3.4151e-02,\n",
      "        -3.9589e-02,  1.0963e-03, -2.7844e-02,  3.8521e-02,  1.7056e-02,\n",
      "         1.9414e-02, -1.7455e-03, -1.5040e-02, -3.7164e-02, -4.0906e-02,\n",
      "        -1.8287e-02,  1.1881e-02, -1.1192e-02,  3.9143e-02, -1.5782e-02,\n",
      "        -2.4111e-03,  1.4623e-02,  1.0533e-02,  2.2225e-02,  1.6301e-02,\n",
      "         1.5347e-02,  4.0923e-02,  4.3956e-02,  2.0128e-02,  2.0685e-02,\n",
      "        -2.6398e-02, -2.7176e-02,  1.9340e-02, -3.1562e-03, -1.3445e-02,\n",
      "         1.3584e-02, -8.0742e-03,  3.5040e-02, -2.5979e-02,  1.8455e-02,\n",
      "        -3.8474e-02, -1.0814e-02,  8.5697e-03,  1.4721e-02, -3.3890e-02,\n",
      "        -3.9733e-02,  1.6680e-02,  3.4404e-02,  3.7252e-02, -7.5480e-03,\n",
      "        -3.8798e-02, -1.8328e-02,  3.2918e-02,  1.1497e-02, -5.9950e-03,\n",
      "        -3.8090e-02, -1.4614e-02, -3.1476e-02,  2.6456e-02,  3.5170e-02,\n",
      "        -3.6264e-02,  2.6079e-02, -3.7116e-02,  6.5943e-03, -2.6705e-02,\n",
      "        -1.8664e-02,  3.9767e-02,  5.5932e-04,  8.9403e-04,  4.9005e-03,\n",
      "         1.2045e-02,  1.9012e-02,  3.2654e-02, -1.2632e-02, -1.1938e-02,\n",
      "         4.3314e-02, -3.5590e-02,  3.1949e-02, -2.9601e-02, -2.0536e-02,\n",
      "        -6.3787e-03, -6.9490e-03, -2.9757e-03,  1.9420e-02, -3.9326e-02,\n",
      "         4.8572e-03,  4.6208e-02,  3.0601e-02, -2.3814e-02, -3.1290e-02,\n",
      "        -2.3322e-02, -4.0681e-02, -1.9718e-02, -3.1454e-02,  4.2755e-02,\n",
      "         1.8303e-02, -9.5790e-03, -2.4545e-04, -1.0074e-02, -4.6650e-03,\n",
      "         1.3457e-02,  2.1727e-02,  1.3391e-02, -1.6372e-02,  1.4077e-03,\n",
      "         3.7510e-02, -9.4853e-03,  1.8015e-02, -3.1518e-02,  1.8554e-02,\n",
      "        -5.9941e-03, -1.8407e-02,  3.9735e-02,  9.8783e-03,  4.2129e-02,\n",
      "         3.8024e-02, -3.3904e-02,  1.1465e-02, -1.0025e-02,  2.1053e-02,\n",
      "        -2.2577e-02, -3.1254e-03,  2.9537e-02, -3.7198e-02, -1.7427e-02,\n",
      "        -1.0667e-02, -2.2741e-03,  3.5875e-02,  4.3149e-02, -1.8087e-02,\n",
      "        -5.1560e-03, -2.3809e-02, -2.2700e-02, -2.9137e-02,  2.4879e-02,\n",
      "        -1.3393e-02,  1.5112e-02, -2.5833e-02,  2.3616e-02,  1.3802e-02,\n",
      "         7.9598e-06,  5.3420e-04,  1.5432e-02,  2.3686e-02,  3.6770e-03,\n",
      "        -2.5791e-02,  3.9702e-02,  3.0301e-02,  3.8822e-02,  5.5314e-03,\n",
      "         1.1037e-02, -3.6068e-02, -2.6800e-03,  6.4777e-05,  2.6620e-02,\n",
      "         4.3118e-02, -1.5874e-02, -2.1145e-02, -3.9127e-02, -4.4660e-04,\n",
      "         3.8123e-03,  3.9189e-02,  4.1210e-02, -2.9678e-02, -1.2563e-02,\n",
      "         4.2098e-02, -1.3651e-02, -3.2937e-02,  3.0246e-02,  2.1655e-02,\n",
      "         1.6674e-02,  3.0228e-02,  3.5799e-02,  3.3242e-02,  4.0443e-02,\n",
      "         2.8889e-02,  1.3892e-02, -1.5947e-03, -3.2958e-04,  1.3122e-02,\n",
      "         3.3077e-02, -7.1924e-03, -1.1899e-03, -1.9403e-02, -1.0638e-02,\n",
      "        -3.8329e-02,  3.1815e-02,  1.2054e-03,  5.8055e-03,  2.4421e-02,\n",
      "        -3.3627e-02,  5.2269e-03,  3.8313e-02,  1.6801e-02, -1.1666e-02,\n",
      "        -2.9102e-02,  3.3283e-02,  4.5106e-02,  1.4793e-02, -2.2635e-02,\n",
      "         1.9313e-02, -9.5373e-06,  3.3414e-02, -3.7179e-02, -2.1869e-02,\n",
      "        -1.0195e-02,  3.4958e-02, -1.6645e-02, -3.3712e-03,  1.6923e-03,\n",
      "        -3.6106e-02, -6.8035e-03, -2.6885e-02,  1.7801e-02, -8.3234e-03,\n",
      "        -3.4994e-02,  3.9695e-02,  9.9640e-03,  1.8480e-02,  4.0416e-03,\n",
      "         3.0031e-02, -7.6172e-03,  2.0013e-02,  8.3025e-03,  2.7208e-02,\n",
      "         1.5396e-02, -8.4637e-03, -1.5059e-02, -1.5586e-02,  3.0360e-02,\n",
      "        -2.6228e-02,  1.1016e-02,  1.2203e-02,  1.0747e-02,  2.9808e-02,\n",
      "        -1.2679e-02,  3.4387e-02, -2.4515e-02,  9.9020e-03, -3.3958e-02,\n",
      "         2.1813e-02,  9.1316e-03, -4.0102e-02, -2.8958e-02,  1.6813e-02,\n",
      "         3.6261e-02,  9.8490e-03, -3.4639e-02,  2.8369e-02,  4.2128e-02,\n",
      "        -2.5940e-02,  3.4401e-02,  3.1455e-04, -2.2616e-03, -2.0576e-03,\n",
      "        -1.0060e-02,  3.0150e-02, -3.3372e-02,  3.6834e-02,  3.9530e-02,\n",
      "        -3.7617e-02,  4.3256e-02, -2.5428e-02,  2.4402e-02,  9.7474e-03,\n",
      "         1.9451e-02, -3.4950e-02, -4.3214e-03,  1.6884e-02,  3.9557e-02,\n",
      "        -7.2324e-03,  3.3106e-03,  2.6886e-02,  1.8734e-02, -3.5059e-02,\n",
      "         1.6033e-02,  3.2171e-02, -4.9731e-03,  3.3130e-02,  3.6328e-02,\n",
      "        -6.0242e-03, -3.2871e-02, -1.8492e-02, -4.6002e-03,  4.3362e-02,\n",
      "         1.3028e-02,  1.0364e-04, -1.5787e-02,  4.6260e-02,  1.6769e-02,\n",
      "         3.0943e-02,  2.4484e-02, -2.9042e-02,  3.4370e-02,  2.9535e-02,\n",
      "         4.5234e-02,  3.0048e-02, -4.6756e-03,  6.2638e-03,  8.0337e-03,\n",
      "        -2.1567e-02, -1.4566e-02,  1.0553e-02, -4.7877e-03, -3.1128e-02,\n",
      "        -4.2019e-02,  7.5819e-03,  4.0059e-02, -2.8215e-02,  3.8241e-02,\n",
      "        -6.8605e-03,  2.1310e-02, -7.9535e-03,  3.8690e-02, -1.4543e-02,\n",
      "        -3.0136e-02,  2.5470e-02,  3.4479e-02,  4.2398e-02, -1.2217e-02,\n",
      "        -2.4591e-02,  3.2901e-02, -1.2373e-02, -1.7442e-02, -2.1610e-02,\n",
      "         1.7447e-04,  2.7244e-02,  2.0681e-02,  1.1658e-02,  4.2305e-02,\n",
      "        -2.9411e-02,  2.9868e-03,  4.2328e-03, -1.0467e-02, -2.8603e-02,\n",
      "        -2.9077e-03,  3.6078e-02,  8.3910e-03,  1.0320e-02, -3.7276e-02,\n",
      "        -2.0211e-02,  1.8150e-02, -2.8383e-02,  4.1006e-02,  4.3009e-02,\n",
      "        -2.1322e-02,  3.6989e-02, -2.5512e-02,  4.0204e-02, -7.7215e-04,\n",
      "         9.5801e-03, -7.3708e-03, -2.0604e-03, -3.6413e-02,  2.1296e-02,\n",
      "         1.1921e-02,  1.5068e-02,  1.7289e-02,  3.7481e-02,  3.1781e-02,\n",
      "        -1.1639e-02,  3.2253e-02,  1.6826e-02,  4.0455e-02, -2.3710e-02,\n",
      "        -2.6077e-02,  8.7674e-04,  1.3771e-02, -1.3589e-03,  1.8376e-02,\n",
      "         8.0723e-03, -3.6582e-02,  3.5727e-02,  2.1576e-02, -7.6174e-03,\n",
      "        -3.1127e-02,  2.0950e-02, -1.1371e-02, -3.9629e-02, -3.7901e-02,\n",
      "        -7.8535e-03,  2.2923e-02,  3.6410e-02,  2.9661e-02,  8.0722e-03,\n",
      "        -3.5449e-03, -2.5659e-02,  1.8820e-02,  2.9090e-02,  2.1656e-02,\n",
      "        -3.0294e-02, -2.3700e-04,  3.7700e-02,  3.4965e-02,  2.5316e-03,\n",
      "        -3.2705e-02, -7.9628e-03,  1.3161e-02,  3.2343e-02, -3.4613e-02,\n",
      "        -8.6845e-03,  1.3019e-02, -7.9210e-03, -2.5303e-02, -1.3693e-02,\n",
      "        -8.9310e-03,  1.2437e-02, -3.1775e-02, -4.0837e-02,  2.9747e-02,\n",
      "        -3.4088e-02, -1.7913e-02, -3.1135e-02,  1.8129e-03,  4.1649e-02,\n",
      "        -1.6897e-02, -2.3861e-02, -2.3923e-02, -3.1414e-03, -4.0119e-02,\n",
      "         4.3439e-02,  1.1999e-02,  2.2480e-02,  2.9067e-02,  2.0531e-03,\n",
      "        -8.0976e-03,  4.2106e-02, -3.3113e-02, -3.1459e-02,  2.9227e-03,\n",
      "        -2.3225e-02, -1.7581e-03,  6.2443e-03, -7.3027e-03, -1.0673e-02,\n",
      "         2.7561e-02,  3.7951e-02,  3.6754e-02, -4.0222e-02,  2.7481e-02,\n",
      "         7.6320e-05, -1.0824e-02, -9.7030e-03, -1.9078e-02, -2.4198e-02,\n",
      "        -9.5766e-03,  3.9309e-02, -3.5296e-02, -6.4200e-03, -2.3290e-02,\n",
      "         3.0445e-02, -2.2752e-02, -3.8594e-03, -1.9641e-02, -2.3353e-02,\n",
      "         1.3347e-02, -2.1890e-02,  3.9148e-02, -3.2036e-02, -1.0553e-02,\n",
      "         3.0487e-02,  3.3110e-02,  3.3131e-02, -1.1062e-02,  3.2009e-02,\n",
      "         2.0412e-02, -1.2824e-03, -3.2336e-02, -1.3577e-02, -1.2519e-02,\n",
      "         4.0079e-02,  4.1096e-03,  1.7689e-02,  4.4146e-02, -1.5850e-02,\n",
      "         4.3148e-02, -1.2807e-02, -2.7445e-02,  1.7129e-02,  3.8155e-02,\n",
      "         1.0672e-02,  2.6926e-02], requires_grad=True)\n",
      "fc1.9.weight Parameter containing:\n",
      "tensor([[-0.0373,  0.0170, -0.0487,  ..., -0.0221, -0.0018,  0.0141],\n",
      "        [-0.0049, -0.0083,  0.0069,  ..., -0.0144,  0.0233,  0.0257],\n",
      "        [-0.0293, -0.0157,  0.0208,  ..., -0.0343, -0.0465,  0.0059],\n",
      "        ...,\n",
      "        [-0.0427,  0.0008, -0.0208,  ..., -0.0312,  0.0350,  0.0102],\n",
      "        [-0.0409,  0.0077, -0.0224,  ..., -0.0403, -0.0116,  0.0290],\n",
      "        [ 0.0270, -0.0453, -0.0334,  ...,  0.0038, -0.0132, -0.0171]],\n",
      "       requires_grad=True)\n",
      "fc1.9.bias Parameter containing:\n",
      "tensor([-0.0293, -0.0075,  0.0147,  0.0212, -0.0380,  0.0037, -0.0329, -0.0388,\n",
      "        -0.0293, -0.0169, -0.0184, -0.0216, -0.0268,  0.0322, -0.0469, -0.0109,\n",
      "        -0.0018,  0.0215, -0.0229,  0.0253,  0.0256, -0.0352,  0.0133,  0.0011,\n",
      "         0.0057,  0.0226,  0.0064,  0.0250, -0.0099, -0.0311, -0.0398, -0.0108,\n",
      "         0.0271,  0.0028, -0.0367,  0.0327,  0.0077, -0.0244, -0.0157, -0.0431,\n",
      "        -0.0090, -0.0311,  0.0061, -0.0025, -0.0336, -0.0392, -0.0401, -0.0428,\n",
      "        -0.0360,  0.0156,  0.0205, -0.0437, -0.0499, -0.0024, -0.0348,  0.0057,\n",
      "        -0.0120, -0.0380,  0.0015,  0.0002, -0.0401,  0.0033, -0.0147, -0.0417,\n",
      "        -0.0377, -0.0100,  0.0110,  0.0113, -0.0192, -0.0228,  0.0027, -0.0268,\n",
      "        -0.0258, -0.0395, -0.0276, -0.0179,  0.0134,  0.0100, -0.0101, -0.0153,\n",
      "        -0.0054, -0.0217, -0.0487,  0.0031,  0.0031, -0.0276, -0.0133,  0.0124,\n",
      "        -0.0197,  0.0070, -0.0025, -0.0069, -0.0098, -0.0493,  0.0225,  0.0201,\n",
      "        -0.0242,  0.0019,  0.0177, -0.0348, -0.0355, -0.0075, -0.0368,  0.0266,\n",
      "         0.0213,  0.0248, -0.0351,  0.0323,  0.0276, -0.0060, -0.0240,  0.0277,\n",
      "         0.0271, -0.0371,  0.0251,  0.0125,  0.0142, -0.0093, -0.0171, -0.0045,\n",
      "        -0.0288,  0.0223, -0.0122, -0.0506, -0.0489, -0.0541, -0.0273,  0.0252,\n",
      "        -0.0029, -0.0287, -0.0409, -0.0054, -0.0011,  0.0328, -0.0158, -0.0388,\n",
      "         0.0281, -0.0494, -0.0438, -0.0084, -0.0401, -0.0222,  0.0220, -0.0004,\n",
      "        -0.0117, -0.0167, -0.0181,  0.0305,  0.0303, -0.0584, -0.0409, -0.0546,\n",
      "        -0.0239, -0.0080, -0.0033, -0.0070, -0.0356, -0.0080, -0.0409,  0.0309,\n",
      "        -0.0004,  0.0280, -0.0131, -0.0398,  0.0059, -0.0311, -0.0137, -0.0145,\n",
      "        -0.0281,  0.0228, -0.0038, -0.0253, -0.0371, -0.0204,  0.0232,  0.0001,\n",
      "         0.0159, -0.0391, -0.0046,  0.0160,  0.0164, -0.0123,  0.0211, -0.0353,\n",
      "         0.0229,  0.0184, -0.0455, -0.0010, -0.0304,  0.0169, -0.0324, -0.0269,\n",
      "         0.0198,  0.0348,  0.0245,  0.0148,  0.0058,  0.0083, -0.0279,  0.0140,\n",
      "        -0.0387,  0.0080,  0.0241, -0.0411,  0.0132, -0.0051, -0.0249,  0.0156,\n",
      "        -0.0285, -0.0450,  0.0139, -0.0308, -0.0099, -0.0255, -0.0421, -0.0276,\n",
      "        -0.0054, -0.0077,  0.0165, -0.0141, -0.0387,  0.0169, -0.0116, -0.0210,\n",
      "        -0.0199, -0.0274,  0.0048, -0.0049, -0.0378,  0.0213, -0.0484, -0.0430,\n",
      "        -0.0518, -0.0301, -0.0392, -0.0299, -0.0190,  0.0238, -0.0451, -0.0249,\n",
      "        -0.0444,  0.0234, -0.0287, -0.0094, -0.0256,  0.0002, -0.0096, -0.0016,\n",
      "        -0.0410,  0.0248,  0.0009,  0.0231,  0.0023, -0.0457, -0.0095,  0.0250,\n",
      "        -0.0070, -0.0381, -0.0320, -0.0425, -0.0075, -0.0115,  0.0245,  0.0045,\n",
      "        -0.0396,  0.0153,  0.0100, -0.0198, -0.0154, -0.0259, -0.0383,  0.0133,\n",
      "        -0.0446, -0.0110,  0.0309,  0.0054, -0.0115, -0.0190,  0.0138,  0.0157,\n",
      "         0.0027, -0.0212, -0.0127,  0.0044,  0.0079,  0.0113,  0.0194,  0.0113,\n",
      "         0.0199, -0.0196, -0.0132,  0.0049, -0.0192, -0.0260,  0.0170, -0.0511,\n",
      "         0.0230, -0.0361, -0.0513,  0.0084, -0.0161, -0.0053,  0.0220, -0.0515,\n",
      "        -0.0077,  0.0115,  0.0135, -0.0240, -0.0048, -0.0274,  0.0124, -0.0182,\n",
      "        -0.0164,  0.0233, -0.0159, -0.0011, -0.0206,  0.0094, -0.0402,  0.0029,\n",
      "        -0.0489, -0.0052, -0.0329,  0.0179, -0.0285, -0.0464, -0.0476, -0.0190,\n",
      "        -0.0162, -0.0491,  0.0091,  0.0322, -0.0456, -0.0231,  0.0229,  0.0244,\n",
      "        -0.0036, -0.0519,  0.0244, -0.0356, -0.0358,  0.0027, -0.0196,  0.0074,\n",
      "        -0.0187, -0.0442, -0.0431, -0.0466,  0.0112,  0.0304, -0.0469, -0.0341,\n",
      "        -0.0367, -0.0457, -0.0235, -0.0472,  0.0161,  0.0220,  0.0113, -0.0139,\n",
      "        -0.0254, -0.0315, -0.0454,  0.0093, -0.0242, -0.0092, -0.0252, -0.0523,\n",
      "        -0.0437, -0.0221,  0.0324, -0.0174, -0.0342,  0.0041, -0.0068,  0.0282,\n",
      "        -0.0258, -0.0123, -0.0055, -0.0296, -0.0214, -0.0311, -0.0029, -0.0406,\n",
      "        -0.0485, -0.0417,  0.0155,  0.0100, -0.0337,  0.0232, -0.0484, -0.0406,\n",
      "        -0.0195, -0.0064, -0.0372, -0.0510, -0.0083, -0.0234, -0.0229,  0.0080,\n",
      "        -0.0150, -0.0031,  0.0163, -0.0130, -0.0511,  0.0235, -0.0115,  0.0101,\n",
      "        -0.0344, -0.0491,  0.0213, -0.0242, -0.0230, -0.0456, -0.0466, -0.0331,\n",
      "        -0.0467, -0.0189,  0.0152,  0.0161, -0.0157, -0.0215, -0.0505, -0.0459,\n",
      "         0.0272, -0.0144,  0.0223, -0.0176,  0.0310, -0.0149, -0.0194,  0.0248,\n",
      "         0.0156,  0.0142,  0.0210, -0.0101, -0.0313,  0.0245,  0.0195, -0.0278,\n",
      "         0.0236, -0.0169, -0.0531, -0.0255, -0.0024,  0.0177,  0.0156,  0.0217,\n",
      "         0.0077, -0.0397,  0.0173,  0.0208, -0.0291,  0.0353, -0.0137, -0.0229,\n",
      "         0.0106, -0.0401, -0.0047,  0.0087, -0.0041, -0.0097, -0.0468,  0.0140,\n",
      "        -0.0457, -0.0239,  0.0058, -0.0357,  0.0164,  0.0089, -0.0077, -0.0096,\n",
      "        -0.0256, -0.0352, -0.0040, -0.0330,  0.0022, -0.0329,  0.0158,  0.0252,\n",
      "         0.0027, -0.0337, -0.0322, -0.0165, -0.0092, -0.0061,  0.0294, -0.0382,\n",
      "         0.0057,  0.0137,  0.0184, -0.0210, -0.0377, -0.0041,  0.0048, -0.0567,\n",
      "        -0.0038, -0.0482, -0.0427, -0.0479,  0.0156, -0.0545, -0.0136, -0.0185,\n",
      "         0.0246, -0.0493, -0.0312,  0.0270, -0.0530, -0.0248,  0.0187,  0.0253],\n",
      "       requires_grad=True)\n",
      "fc1.12.weight Parameter containing:\n",
      "tensor([[-0.0269,  0.0243, -0.0002,  ..., -0.0153,  0.0094, -0.0117],\n",
      "        [ 0.0328, -0.0183, -0.0493,  ..., -0.0322, -0.0394,  0.0300],\n",
      "        [-0.0335, -0.0250,  0.0187,  ..., -0.0166,  0.0257, -0.0200],\n",
      "        ...,\n",
      "        [ 0.0197,  0.0350,  0.0030,  ..., -0.0316, -0.0228, -0.0164],\n",
      "        [ 0.0339,  0.0165, -0.0292,  ..., -0.0084,  0.0198,  0.0165],\n",
      "        [-0.0509, -0.0319,  0.0374,  ...,  0.0150, -0.0027,  0.0128]],\n",
      "       requires_grad=True)\n",
      "fc1.12.bias Parameter containing:\n",
      "tensor([-0.0279,  0.0037, -0.0020,  0.0324,  0.0232, -0.0416,  0.0185, -0.0006,\n",
      "        -0.0315,  0.0303,  0.0331, -0.0401, -0.0114, -0.0130, -0.0066,  0.0267,\n",
      "         0.0254, -0.0035,  0.0008,  0.0148,  0.0235,  0.0384, -0.0153, -0.0223,\n",
      "         0.0130, -0.0141,  0.0176,  0.0332,  0.0215, -0.0366, -0.0111,  0.0370,\n",
      "         0.0249, -0.0187, -0.0371,  0.0380, -0.0138,  0.0401, -0.0238, -0.0224,\n",
      "         0.0236,  0.0340, -0.0413,  0.0069, -0.0322, -0.0303,  0.0288, -0.0298,\n",
      "         0.0132,  0.0309, -0.0117,  0.0128,  0.0179,  0.0385, -0.0377,  0.0322,\n",
      "         0.0188,  0.0031,  0.0272, -0.0174,  0.0389,  0.0285,  0.0085,  0.0216,\n",
      "        -0.0337, -0.0283,  0.0411,  0.0276, -0.0148, -0.0009,  0.0223,  0.0052,\n",
      "        -0.0323, -0.0126,  0.0336,  0.0073,  0.0392,  0.0124, -0.0048, -0.0175,\n",
      "         0.0316, -0.0160, -0.0308,  0.0255, -0.0294, -0.0288, -0.0051,  0.0037,\n",
      "        -0.0078, -0.0392,  0.0121,  0.0052,  0.0025,  0.0137, -0.0390, -0.0346,\n",
      "         0.0062, -0.0370,  0.0161, -0.0342, -0.0329,  0.0348, -0.0054,  0.0355,\n",
      "        -0.0345, -0.0157, -0.0351,  0.0003, -0.0013, -0.0114, -0.0334,  0.0311,\n",
      "         0.0006,  0.0117, -0.0012,  0.0121, -0.0189, -0.0017, -0.0261,  0.0358,\n",
      "         0.0352,  0.0252, -0.0066, -0.0247,  0.0195, -0.0141,  0.0248,  0.0262,\n",
      "         0.0385,  0.0357, -0.0074,  0.0392, -0.0313, -0.0128, -0.0031, -0.0368,\n",
      "         0.0235, -0.0041, -0.0088,  0.0259,  0.0172, -0.0170, -0.0005,  0.0044,\n",
      "        -0.0293, -0.0404,  0.0014, -0.0012, -0.0323,  0.0228, -0.0366,  0.0365,\n",
      "        -0.0099, -0.0155, -0.0296, -0.0171, -0.0069, -0.0365, -0.0342, -0.0012,\n",
      "        -0.0315,  0.0014, -0.0376,  0.0025, -0.0321, -0.0375,  0.0022, -0.0017,\n",
      "         0.0009, -0.0217, -0.0410,  0.0334,  0.0297, -0.0343, -0.0132,  0.0367,\n",
      "        -0.0088,  0.0281, -0.0176,  0.0065, -0.0032,  0.0343,  0.0178,  0.0382,\n",
      "        -0.0045, -0.0141, -0.0391,  0.0102, -0.0361,  0.0077, -0.0316,  0.0100,\n",
      "        -0.0038,  0.0324, -0.0030, -0.0350,  0.0358, -0.0336,  0.0320, -0.0119,\n",
      "        -0.0140,  0.0296,  0.0270, -0.0368, -0.0345, -0.0232,  0.0020,  0.0260,\n",
      "        -0.0359,  0.0110,  0.0221, -0.0114, -0.0197, -0.0033,  0.0058,  0.0280,\n",
      "        -0.0266,  0.0329,  0.0325, -0.0375,  0.0001, -0.0012, -0.0110, -0.0008,\n",
      "         0.0162,  0.0053,  0.0048, -0.0185, -0.0305, -0.0354, -0.0030,  0.0389,\n",
      "         0.0169, -0.0214, -0.0106, -0.0412,  0.0331, -0.0388, -0.0304,  0.0028,\n",
      "        -0.0394, -0.0263,  0.0267, -0.0141,  0.0139,  0.0338, -0.0167,  0.0322,\n",
      "        -0.0198,  0.0007, -0.0137,  0.0212, -0.0193, -0.0280, -0.0291, -0.0015],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for x, y in model.named_parameters():\n",
    "    print(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4705e-06, -1.5091e-06,  2.6124e-06,  1.1937e-06, -9.5364e-07,\n",
      "          4.8526e-09,  2.9668e-06,  2.0681e-06,  3.0560e-06, -8.6873e-07,\n",
      "          2.4488e-06, -2.7257e-06, -2.9203e-06, -2.5097e-06,  2.0711e-06,\n",
      "         -1.0097e-06, -3.0860e-06,  2.6070e-06, -1.7544e-06, -2.1002e-06,\n",
      "          2.0549e-06, -1.7169e-06, -8.0820e-07, -2.8750e-06,  1.7985e-06,\n",
      "          1.3931e-06,  1.1287e-06, -2.0333e-06,  1.6162e-06, -1.7779e-06,\n",
      "         -2.3550e-06, -1.8789e-06],\n",
      "        [-1.4705e-06, -1.5091e-06,  2.6124e-06,  1.1937e-06, -9.5364e-07,\n",
      "          4.8526e-09,  2.9668e-06,  2.0681e-06,  3.0560e-06, -8.6873e-07,\n",
      "          2.4488e-06, -2.7257e-06, -2.9203e-06, -2.5097e-06,  2.0711e-06,\n",
      "         -1.0097e-06, -3.0860e-06,  2.6070e-06, -1.7544e-06, -2.1002e-06,\n",
      "          2.0549e-06, -1.7169e-06, -8.0820e-07, -2.8750e-06,  1.7985e-06,\n",
      "          1.3931e-06,  1.1287e-06, -2.0333e-06,  1.6162e-06, -1.7779e-06,\n",
      "         -2.3550e-06, -1.8789e-06],\n",
      "        [-1.4705e-06, -1.5091e-06,  2.6124e-06,  1.1937e-06, -9.5364e-07,\n",
      "          4.8526e-09,  2.9668e-06,  2.0681e-06,  3.0560e-06, -8.6873e-07,\n",
      "          2.4488e-06, -2.7257e-06, -2.9203e-06, -2.5097e-06,  2.0711e-06,\n",
      "         -1.0097e-06, -3.0860e-06,  2.6070e-06, -1.7544e-06, -2.1002e-06,\n",
      "          2.0549e-06, -1.7169e-06, -8.0820e-07, -2.8750e-06,  1.7985e-06,\n",
      "          1.3931e-06,  1.1287e-06, -2.0333e-06,  1.6162e-06, -1.7779e-06,\n",
      "         -2.3550e-06, -1.8789e-06],\n",
      "        [-1.4705e-06, -1.5091e-06,  2.6124e-06,  1.1937e-06, -9.5364e-07,\n",
      "          4.8526e-09,  2.9668e-06,  2.0681e-06,  3.0560e-06, -8.6873e-07,\n",
      "          2.4488e-06, -2.7257e-06, -2.9203e-06, -2.5097e-06,  2.0711e-06,\n",
      "         -1.0097e-06, -3.0860e-06,  2.6070e-06, -1.7544e-06, -2.1002e-06,\n",
      "          2.0549e-06, -1.7169e-06, -8.0820e-07, -2.8750e-06,  1.7985e-06,\n",
      "          1.3931e-06,  1.1287e-06, -2.0333e-06,  1.6162e-06, -1.7779e-06,\n",
      "         -2.3550e-06, -1.8789e-06],\n",
      "        [-1.4705e-06, -1.5091e-06,  2.6124e-06,  1.1937e-06, -9.5364e-07,\n",
      "          4.8526e-09,  2.9668e-06,  2.0681e-06,  3.0560e-06, -8.6873e-07,\n",
      "          2.4488e-06, -2.7257e-06, -2.9203e-06, -2.5097e-06,  2.0711e-06,\n",
      "         -1.0097e-06, -3.0860e-06,  2.6070e-06, -1.7544e-06, -2.1002e-06,\n",
      "          2.0549e-06, -1.7169e-06, -8.0820e-07, -2.8750e-06,  1.7985e-06,\n",
      "          1.3931e-06,  1.1287e-06, -2.0333e-06,  1.6162e-06, -1.7779e-06,\n",
      "         -2.3550e-06, -1.8789e-06]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  print(model.forward_once(dataset_val.rows[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
